{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"/mnt/efs/shared/meg_shared_scripts/meg-kb\"\n",
    "data_ac=\"indeeda-meg-ac\"\n",
    "data_pt=\"indeeda-meg-pt\"\n",
    "yutong_base_dir=\"/home/ubuntu/users/yutong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr, entropy, gmean\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import importlib\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "\n",
    "import logging\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from annoy import AnnoyIndex\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "from glob import glob\n",
    "import ahocorasick\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "nlp_full = spacy.load('en_core_web_sm')\n",
    "\n",
    "from compute_concept_clusters import knn\n",
    "from compute_keyphrase_embeddings import ensure_tensor_on_device, mean_pooling\n",
    "\n",
    "from compute_multi_view_embeddings import get_lm_probe_concept_embeddings\n",
    "\n",
    "from lm_probes import LMProbe, LMProbe_GPT2, LMProbe_Joint, LMProbe_PMI, LMProbe_PMI_greedy\n",
    "from utils import load_seed_aligned_concepts, load_seed_aligned_relations, load_benchmark\n",
    "from utils import load_embeddings, load_embeddings_dict, load_EE_labels\n",
    "from utils import load_EE_labels\n",
    "from utils import get_masked_contexts, bert_untokenize\n",
    "from utils import learn_patterns\n",
    "\n",
    "from roberta_ses.interface import Roberta_SES_Entailment\n",
    "\n",
    "from multiview_EE_datasets import Wiki_EE_Dataset, Wiki_EE_Dataset_2, Indeed_EE_Dataset, Indeed_EE_Dataset_2\n",
    "from multiview_EE_models import EE_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import load_seed_aligned_concepts, load_seed_aligned_relations, load_benchmark\n",
    "from utils import load_embeddings, load_embeddings_dict, load_EE_labels\n",
    "from utils import load_EE_labels\n",
    "from utils import get_masked_contexts, bert_untokenize\n",
    "from utils import learn_patterns\n",
    "\n",
    "import lm_probes\n",
    "importlib.reload(lm_probes)\n",
    "from lm_probes import LMProbe, LMProbe_GPT2, LMProbe_Joint, LMProbe_PMI, LMProbe_PMI_greedy\n",
    "\n",
    "import compute_multi_view_embeddings\n",
    "importlib.reload(compute_multi_view_embeddings)\n",
    "from compute_multi_view_embeddings import get_lm_probe_concept_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dir = os.path.join(base_dir, f'data/indeed-benchmark')\n",
    "\n",
    "seed_aligned_concepts_path = os.path.join(benchmark_dir, f'seed_aligned_concepts.csv')\n",
    "seed_aux_concepts_path = os.path.join(benchmark_dir, f'seed_aligned_concepts_aux.csv')\n",
    "seed_aligned_relations_path = os.path.join(benchmark_dir, f'seed_aligned_relations_nodup.csv')\n",
    "seed_aux_relations_path = os.path.join(benchmark_dir, f'seed_aligned_relations_aux.csv')\n",
    "benchmark_path = os.path.join(benchmark_dir, f'benchmark_evidence_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_dir = os.path.join(base_dir, f'data/wiki-meg-ac/intermediate')\n",
    "wiki_gt_dir = os.path.join(base_dir, f'data/wiki-meg-ac/gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/efs/shared/meg_shared_scripts/meg-kb/data/wiki-meg-ac/intermediate'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: text corpus\n",
    "# step 1: extract key phrases (autophrase)\n",
    "# step 2: generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/keyword_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n"
     ]
    }
   ],
   "source": [
    "#change to keyword extractor directory\n",
    "%cd $base_dir/src/keyword_extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./corpusProcess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset and thread no\n",
    "data_ac = 'indeeda-meg-ac'\n",
    "data_pt = 'indeeda-meg-pt'\n",
    "thread = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process corpus and generate key prhases\n",
    "!./corpusProcess.sh $data_ac $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy these results to sample-meg-pt\n",
    "!cp -r $base_dir/data/$data_ac $base_dir/data/$data_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus with company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/question_answers.csv'\n",
    "company_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/fccid-companyName.csv'\n",
    "entity_emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')\n",
    "out_corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences_with_company.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|████████████████| 307122/307122 [11:31<00:00, 444.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "!python build_corpus_with_companies.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-rd /home/ubuntu/users/nikita/data/indeed/indeedQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $base_dir/data/$data_ac/intermediate/sentences_with_company.json $base_dir/data/$data_pt/intermediate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|███████████████| 901796/901796 [00:03<00:00, 274660.12it/s]\n",
      "computing entity-wise embedding: 100%|██████| 8028/8028 [04:55<00:00, 27.21it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python compute_keyphrase_embeddings.py \\\n",
    "-m roberta-base \\\n",
    "-et ac \\\n",
    "-ename RoBERTa \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3 python compute_keyphrase_embeddings.py -m bert-base-uncased -et pt -d $base_dir/data/$data_pt/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to autophrase\n",
    "%cd $base_dir/src/tools/AutoPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corel = 'sample-indeeda-corel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extractBertEmbedding.py ../../../data/$data_corel/intermediate/ $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add embeddings for seed instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using script\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2 python add_seed_instances_embeddings.py \\\n",
    "-m roberta-base \\\n",
    "-et ac \\\n",
    "-ename RoBERTa \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-c 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed.txt')\n",
    "\n",
    "embeddings = load_embeddings(bert_emb_path, 768)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Seed Entities (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd ../../concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn sentence-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 100\n",
    "output = '../../data/'+data_ac+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 177/177 [00:00<00:00, 5435.26it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 177/177 [00:00<00:00, 2001.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_ac/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 177/177 [00:00<00:00, 3661.18it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 177/177 [00:00<00:00, 4052.00it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_pt/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_corel/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed instances clustering (EE-emb)\n",
    "(using all seed instances of a concept to find neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|██████████████| 8064/8064 [00:01<00:00, 6444.07it/s]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:00<00:00, 16.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "!python compute_concept_seeds_knn.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-e $base_dir/data/$data_ac/intermediate/BERTembed+seeds.txt \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_concept_knn_k=None-2.csv \\\n",
    "-kdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check results \n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_knn_k=None.csv')\n",
    "\n",
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'company'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'pay_schedule'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-LM-probe (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_probe = LMProbe(model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 14/14 [00:58<00:00,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use scripts \n",
    "!python compute_EE_LM_probe.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-lm mlm \\\n",
    "-lm_model roberta-base \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_roberta_k=None.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:54<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "!python compute_EE_LM_probe.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-lm bert \\\n",
    "-lm_model /home/ubuntu/users/nikita/models/bert_finetuned_lm/indeed_reviews_ques_ans \\\n",
    "-agg avg \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_bert_DA_avg_k=None.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████| 14/14 [00:52<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "!python compute_EE_LM_probe.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-lm mlm \\\n",
    "-lm_model bert-base-uncased \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_bert_k=None-2.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class name prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lm_probe = LMProbe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cand': 'qualifications', 'score': 0.010107847861945629},\n",
       " {'cand': 'benefits', 'score': 1.0626394214341422e-05},\n",
       " {'cand': 'roles', 'score': 1.079390585800865e-06},\n",
       " {'cand': 'checks', 'score': 5.771184419245406e-07},\n",
       " {'cand': 'persons', 'score': 3.987514958225805e-07},\n",
       " {'cand': 'departments', 'score': 2.8143759323029387e-07},\n",
       " {'cand': 'schedules', 'score': 2.6243087702937383e-08},\n",
       " {'cand': 'dresses', 'score': 1.1577242275961901e-08}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_txt = '[MASK] such as bachelor degree is required to work at Walmart.'\n",
    "lm_probe.score_candidates(_input_txt, ['benefits', 'roles', 'persons', 'schedules', 'dresses', 'departments', 'checks', 'qualifications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cand': 'qualifications', 'score': 0.014812425710260875},\n",
       " {'cand': 'benefits', 'score': 9.158307875622997e-06},\n",
       " {'cand': 'persons', 'score': 9.146327784037573e-07},\n",
       " {'cand': 'roles', 'score': 6.875428653074773e-07},\n",
       " {'cand': 'checks', 'score': 2.984555749208085e-07},\n",
       " {'cand': 'departments', 'score': 1.602442836201591e-07},\n",
       " {'cand': 'dresses', 'score': 8.730975054049843e-09},\n",
       " {'cand': 'schedules', 'score': 7.1630932296784555e-09}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_txt = '[MASK] such as bachelor degree is required to work at the company.'\n",
    "lm_probe.score_candidates(_input_txt, ['benefits', 'roles', 'persons', 'schedules', 'dresses', 'departments', 'checks', 'qualifications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cand': 'qualifications', 'score': 0.06600436568260193},\n",
       " {'cand': 'benefits', 'score': 0.0003170075651723892},\n",
       " {'cand': 'departments', 'score': 0.00019293477816972865},\n",
       " {'cand': 'roles', 'score': 0.00011820000509032974},\n",
       " {'cand': 'persons', 'score': 9.001246326079126e-06},\n",
       " {'cand': 'checks', 'score': 8.256243745563545e-06},\n",
       " {'cand': 'schedules', 'score': 3.3704397992551064e-06},\n",
       " {'cand': 'dresses', 'score': 8.162938911482339e-07}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_txt = 'They have [MASK] such as bachelor degree.'\n",
    "lm_probe.score_candidates(_input_txt, ['benefits', 'roles', 'persons', 'schedules', 'dresses', 'departments', 'checks', 'qualifications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8064, ['multiple times', 'upper', 'management'])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')\n",
    "with open(emb_num_path, 'r') as f:\n",
    "    all_entities = [l.rsplit(' ', 1)[0] for l in f]\n",
    "len(all_entities), all_entities[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2618ef0796e46d7948041c5ca11c237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8064.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cn_scores_dict = dict()  # Dict[cn, Dict[e, score]]\n",
    "cn_scores_dict_qual = dict()\n",
    "\n",
    "for _e in tqdm(all_entities):\n",
    "    _input_txt = f'[MASK] such as {_e} is required to work at the company.'\n",
    "    _d = lm_probe.score_candidates(_input_txt, ['qualifications'])\n",
    "    cn_scores_dict_qual[_e] = _d[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('national background', 0.18993115425109863),\n",
       " ('excellent service', 0.17657598853111267),\n",
       " ('excellent training', 0.14337441325187683),\n",
       " ('election', 0.13679932057857513),\n",
       " ('civil service examination', 0.13190346956253055),\n",
       " ('latitude', 0.117889866232872),\n",
       " ('grade point average', 0.11734356731176378),\n",
       " ('473 exam', 0.11213560402393344),\n",
       " ('higher position', 0.11185302585363385),\n",
       " ('skill level', 0.10884014517068863),\n",
       " ('citizenship', 0.10140147805213928),\n",
       " ('advancement opportunity', 0.09752645343542099),\n",
       " ('career opportunity', 0.09613030403852464),\n",
       " ('great place', 0.09332876652479172),\n",
       " ('entry level', 0.09109833091497423),\n",
       " ('great opportunity', 0.08629722893238066),\n",
       " ('teaching experience', 0.08455906808376311),\n",
       " ('drivers license', 0.0844525247812271),\n",
       " ('excellent customer', 0.07997493445873259),\n",
       " ('perfect attendance', 0.07865294814109802),\n",
       " ('pay grade', 0.07696734368801117),\n",
       " ('pse position', 0.0760236829519272),\n",
       " ('highschool diploma', 0.07536540925502776),\n",
       " ('diploma', 0.07507280260324477),\n",
       " ('high school diploma', 0.07400100678205489),\n",
       " ('good standings', 0.07332821190357208),\n",
       " ('managers discretion', 0.07293488085269928),\n",
       " ('driver license', 0.07279372960329057),\n",
       " ('degree', 0.0696198120713234),\n",
       " ('high turn over', 0.06801415234804152),\n",
       " ('inclination', 0.0674176439642906),\n",
       " ('hs diploma', 0.06688763946294786),\n",
       " ('main focus', 0.06600431352853775),\n",
       " ('highly recommend', 0.06525976955890656),\n",
       " ('career path', 0.06435005366802217),\n",
       " ('ph', 0.06399897485971452),\n",
       " ('job title', 0.06349819153547287),\n",
       " ('skill set', 0.06346611678600313),\n",
       " ('test status', 0.062011212110519416),\n",
       " ('basic knowledge', 0.061216793954372406),\n",
       " ('require drug testing', 0.060669101774692535),\n",
       " ('constitution', 0.05975136905908584),\n",
       " ('academic', 0.05952610820531845),\n",
       " ('competitive pay', 0.05929696187376975),\n",
       " ('legal age', 0.05846517905592918),\n",
       " ('social status', 0.0580885335803032),\n",
       " ('gain experience', 0.0568312332034111),\n",
       " ('credit rating', 0.05572803691029547),\n",
       " (\"driver 's license\", 0.05565151944756508),\n",
       " ('first advantage', 0.05463823676109313),\n",
       " ('charter', 0.05447678640484809),\n",
       " ('prior experience', 0.05433794483542442),\n",
       " ('knowledge', 0.05327026918530464),\n",
       " ('driving record', 0.05317889899015426),\n",
       " ('bare minimum', 0.05310521274805068),\n",
       " ('secondary education', 0.05294932425022125),\n",
       " ('exceptional customer service', 0.05274960026144981),\n",
       " ('pilot', 0.05211394652724267),\n",
       " ('minimum age', 0.05208748206496239),\n",
       " ('education', 0.051332868635654456),\n",
       " ('highest paid', 0.05129551887512207),\n",
       " ('initial interview', 0.05128328874707221),\n",
       " ('excellent customer service', 0.049366328865289695),\n",
       " ('meritocracy', 0.04863769933581352),\n",
       " ('higher education', 0.048277437686920166),\n",
       " ('base level', 0.04702895879745484),\n",
       " ('food handlers card', 0.047008544206619256),\n",
       " ('great commission', 0.04681324586272239),\n",
       " ('enterprise rent a car', 0.0461384579539299),\n",
       " ('frequency', 0.046073310077190406),\n",
       " ('non career', 0.0455450415611267),\n",
       " ('great working environment', 0.04538623616099357),\n",
       " ('country', 0.045023821294307716),\n",
       " ('valid license', 0.04455062374472619),\n",
       " ('legal working age', 0.04407989233732223),\n",
       " ('bsc', 0.043301697820425034),\n",
       " ('higher level', 0.043088141828775406),\n",
       " ('equal opportunity', 0.04300498217344285),\n",
       " ('registered nurse', 0.0428810603916645),\n",
       " ('employment', 0.04243453592061996),\n",
       " ('liquor license', 0.04240747168660164),\n",
       " ('permanent position', 0.04238251224160195),\n",
       " ('french quarter', 0.042186260223388665),\n",
       " ('a level', 0.04172641783952714),\n",
       " ('sat', 0.041142795234918594),\n",
       " ('extensive training', 0.04061353951692582),\n",
       " ('proper training', 0.04051414132118225),\n",
       " ('hiring packet', 0.03991136699914932),\n",
       " ('great customer service', 0.03967151045799256),\n",
       " ('driving test', 0.039657227694988244),\n",
       " ('meritocratic', 0.039619464427232735),\n",
       " ('ma', 0.03957046940922737),\n",
       " ('regular associate', 0.03907345607876777),\n",
       " ('current position', 0.03898751363158227),\n",
       " ('yearly review', 0.037705745548009865),\n",
       " ('formal interview', 0.0376182496547699),\n",
       " ('excellent benefits', 0.03737684339284896),\n",
       " ('training', 0.03726719319820404),\n",
       " ('cca position', 0.03706249594688415),\n",
       " ('dress professional', 0.037038281559944146)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_l = sorted(cn_scores_dict_qual.items(), key=lambda p: p[1], reverse=True)\n",
    "_l[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6d32bad584422fb030165fa17156e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8064.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cn_scores_dict_qual_2 = dict()\n",
    "\n",
    "for _e in tqdm(all_entities):\n",
    "    _input_txt = f'[MASK] such as {_e}.'\n",
    "    _d = lm_probe.score_candidates(_input_txt, ['qualifications'])\n",
    "    cn_scores_dict_qual_2[_e] = _d[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_l = sorted(cn_scores_dict_qual_2.items(), key=lambda p: p[1], reverse=True)\n",
    "_l[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postproc contrastive EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postproc_contrastive_EE(ee_in_path, ee_contr_out_path, score_col, seed_concepts_path,\n",
    "                            seed_score=0.0, keep_all_entities=False):\n",
    "    \n",
    "    # score_col: column name of score to use for ranking \n",
    "    # seed_score: default score for seeds, since their scores are not computed in EE methods\n",
    "    # keep_all_entities: if False, only assign entities to best concepts (contr);\n",
    "    #   if True, keep all entities for each concept (acontr)\n",
    "    \n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    seed_instances_dict = dict(zip(\n",
    "        seed_concepts_df['alignedCategoryName'].tolist(),\n",
    "        seed_concepts_df['seedInstances'].tolist()\n",
    "    ))\n",
    "    all_seed_instances = set([_e for _seeds in seed_instances_dict.values() for _e in _seeds])\n",
    "    print(sorted(list(all_seed_instances)))\n",
    "    ee_in_df = pd.read_csv(ee_in_path)\n",
    "    \n",
    "    cc_list = list(set(ee_in_df['concept'].tolist()))\n",
    "    all_ent_list = list(set(ee_in_df['neighbor'].tolist()))\n",
    "    \n",
    "    cc_scores_dict = dict()\n",
    "    for cc in cc_list:\n",
    "        cc_df = ee_in_df[ee_in_df['concept'] == cc]\n",
    "        cc_scores_dict[cc] = dict(zip(\n",
    "            cc_df['neighbor'].tolist(),\n",
    "            cc_df[score_col].tolist()\n",
    "        ))\n",
    "    \n",
    "    cands_for_concepts = [[] for _ in range(len(cc_list))]\n",
    "    for _e in all_ent_list:\n",
    "#         if _e in all_seed_instances:\n",
    "#             continue\n",
    "        _scores = [cc_scores_dict[cc].get(_e, seed_score) for cc in cc_list]\n",
    "        _cc_ranking = np.argsort(_scores).tolist()[::-1]\n",
    "        _max_cc_id = _cc_ranking[0]\n",
    "        _second_cc_id = _cc_ranking[1]\n",
    "#         _score = _scores[_max_cc_id]\n",
    "#         _2nd_score = _scores[_second_cc_id]\n",
    "#         _margin = _score - _2nd_score\n",
    "        \n",
    "        for cc_id, cc in enumerate(cc_list):\n",
    "            if _e in seed_instances_dict[cc]:\n",
    "                continue\n",
    "            if not keep_all_entities and cc_id != _max_cc_id:\n",
    "                continue\n",
    "            obest_cc_id = _second_cc_id if cc_id == _max_cc_id else _max_cc_id\n",
    "            _score = _scores[cc_id]\n",
    "            _obest_score = _scores[obest_cc_id]\n",
    "            _margin = _score - _obest_score\n",
    "            cands_for_concepts[cc_id].append({\n",
    "                'concept': cc,\n",
    "                'obest_concept': cc_list[obest_cc_id],\n",
    "                'neighbor': _e,\n",
    "                score_col: _score,\n",
    "                f'obest_{score_col}': _obest_score,\n",
    "                'margin': _margin,\n",
    "                f'{score_col}+margin': _score + _margin\n",
    "            })\n",
    "\n",
    "    out_records = []\n",
    "    for cc_id, cands in enumerate(cands_for_concepts):\n",
    "        cands_sorted = sorted(cands, key=lambda d: d[f'{score_col}+margin'], reverse=True)\n",
    "        out_records.extend(cands_sorted)\n",
    "    \n",
    "    pd.DataFrame(out_records).to_csv(ee_contr_out_path, index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['401k', '7 days', '8 hour shift', 'amazon', 'bachelors degree', 'barista', 'base pay', 'biweekly', 'bonus', 'business casual', 'cashier', 'checks', 'christmas eve', 'criminal background check', 'criminals', 'delivery driver', 'dinner shift', 'direct deposit', 'disabled', 'dishwasher', 'drug addicts', 'drug test', 'early morning', 'early morning shift', 'employment verification', 'facial hair', 'felons', 'flexible schedule', 'friday', 'full time', 'hair color', 'health insurance', 'heavy lifting', 'high schoolers', 'hiring age', 'hoilday', 'introduction', 'microsoft', 'misdemeanor', 'night shift', 'orientation', 'overtime pay', 'package handler', 'paid vacation', 'part time', 'piercings', 'pregnant', 'prepaid card', 'prior experience', 'sales associate', 'saturday', 'seasonal', 'seniors', 'shoes', 'sick leave', 'stock options', 'store manager', 'students', 'subway', 'sunday', 'target', 'tattoos', 'team lunch', 'training', 'uniform', 'vision insurance', 'walmart', 'weekend', 'weekly', 'working permit', 'workstation']\n"
     ]
    }
   ],
   "source": [
    "# ee_in_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_roberta_k=None.csv')\n",
    "# ee_contr_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_roberta_contr_k=None.csv')\n",
    "ee_in_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_knn_k=None-2.csv')\n",
    "ee_contr_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None-2.csv')\n",
    "# ee_in_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')\n",
    "# ee_contr_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_acontr_k=None.csv')\n",
    "# seed_concepts_aux_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts_aux.csv')\n",
    "\n",
    "postproc_contrastive_EE(ee_in_path=ee_in_path,\n",
    "                        ee_contr_out_path=ee_contr_out_path,\n",
    "                        score_col='sim',\n",
    "                        seed_concepts_path=seed_aligned_concepts_path,\n",
    "                        keep_all_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeEntityExpander:\n",
    "    def __init__(self,\n",
    "                 step_K,\n",
    "                 total_steps,\n",
    "                 out_dir,\n",
    "                 bert_emb_path,\n",
    "                 bert_emb_dim,\n",
    "                 benchmark_dir,\n",
    "                 entity_emb_dict=None):\n",
    "        \n",
    "        self.step_K = step_K\n",
    "        self.total_steps = total_steps\n",
    "        self.out_dir = out_dir\n",
    "        self.bert_emb_path = bert_emb_path\n",
    "        self.bert_emb_dim = bert_emb_dim\n",
    "        \n",
    "        if entity_emb_dict is None:\n",
    "            _entity_embeddings = load_embeddings(bert_emb_path, bert_emb_dim)\n",
    "            entity_emb_dict = dict(zip(_entity_embeddings['entity'].tolist(),\n",
    "                                       _entity_embeddings['embedding'].tolist()))\n",
    "        self.entity_emb_dict = entity_emb_dict\n",
    "        \n",
    "        self.benchmark_dir = benchmark_dir\n",
    "        _seed_concepts_path = os.path.join(benchmark_dir, 'seed_aligned_concepts.csv')\n",
    "        _concepts_df = load_seed_aligned_concepts(_seed_concepts_path)\n",
    "        self.init_seeds_dict = dict(zip(_concepts_df['alignedCategoryName'].tolist(),\n",
    "                                        _concepts_df['seedInstances'].tolist()))\n",
    "        self.seeds_dict = dict(self.init_seeds_dict)\n",
    "    \n",
    "    def expand(self, start_step=0, end_step=None):\n",
    "        if end_step is None:\n",
    "            end_step = self.total_steps\n",
    "        for step_id in tqdm(range(start_step, end_step)):\n",
    "            self.expand_step(step_id)\n",
    "            \n",
    "    def expand_step(self, step_id):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeEE_Emb_Contrastive(IterativeEntityExpander):        \n",
    "    def expand_step(self, step_id):\n",
    "        entities, embeddings = zip(*self.entity_emb_dict.items())\n",
    "        \n",
    "        neighbors = []\n",
    "        \n",
    "        concept_emb_dict = dict()\n",
    "        for a_concept, seed_instances in self.seeds_dict.items():\n",
    "            embs = []\n",
    "            for inst in seed_instances:\n",
    "                try:\n",
    "                    embs.append(self.entity_emb_dict[inst])\n",
    "                except KeyError:\n",
    "                    print(f\"{inst} not found in entity_emb_dict??\")\n",
    "                    continue\n",
    "            if len(embs) == 0:\n",
    "                continue\n",
    "            concept_emb = np.mean(embs, axis=0)\n",
    "            concept_emb_dict[a_concept] = concept_emb\n",
    "\n",
    "        concepts = list(concept_emb_dict.keys())\n",
    "        concept_embs = list(concept_emb_dict.values())\n",
    "\n",
    "        # (n_concepts, n_entities)\n",
    "        cos_matrix = cosine_similarity(concept_embs, embeddings)\n",
    "\n",
    "        cands_for_concepts = [[] for _ in range(len(concepts))]\n",
    "        for e_id, e in enumerate(entities):\n",
    "            _scores = cos_matrix[:, e_id]\n",
    "            _cc_ranking = np.argsort(-_scores)\n",
    "            _max_cc_id = _cc_ranking[0]\n",
    "            _second_cc_id = _cc_ranking[1]\n",
    "            _score = _scores[_max_cc_id]\n",
    "            _2nd_score = _scores[_second_cc_id]\n",
    "            _margin = _score - _2nd_score\n",
    "            \n",
    "            if e in self.seeds_dict[concepts[_max_cc_id]]:\n",
    "                continue\n",
    "            \n",
    "            cands_for_concepts[_max_cc_id].append({\n",
    "                'concept': concepts[_max_cc_id],\n",
    "                '2nd_concept': concepts[_second_cc_id],\n",
    "                'neighbor': e,\n",
    "                'sim': _score,\n",
    "                '2nd_sim': _2nd_score,\n",
    "                'margin': _margin,\n",
    "                'sim+margin': _score + _margin\n",
    "            })\n",
    "\n",
    "        for cc_id, cands in enumerate(cands_for_concepts):\n",
    "            cands_sorted = sorted(cands, key=lambda d: d['sim+margin'], reverse=True)\n",
    "            neighbors.extend(cands_sorted)\n",
    "            \n",
    "            _cc = concepts[cc_id]\n",
    "            _orig_seeds = self.seeds_dict[_cc]\n",
    "            # _ranked_cands = [d['neighbor'] for d in cands_sorted if d['neighbor'] not in _orig_seeds]\n",
    "            _ranked_cands = [d['neighbor'] for d in cands_sorted]\n",
    "            _add_seeds = _ranked_cands[:self.step_K]\n",
    "            _new_seeds = list(set(_orig_seeds + _add_seeds))\n",
    "            self.seeds_dict[_cc] = _new_seeds\n",
    "    \n",
    "        _rankings_dest = os.path.join(self.out_dir, f'rankings-step={step_id}.csv')\n",
    "        c_df = pd.DataFrame(neighbors)\n",
    "        c_df.to_csv(_rankings_dest, index=None)\n",
    "        \n",
    "        _EE_dest = os.path.join(self.out_dir, f'expanded-step={step_id}.csv')\n",
    "        e_list = []\n",
    "        for _cc, _e_list in self.seeds_dict.items():\n",
    "            for _e in _e_list:\n",
    "                e_list.append((_cc, _e))\n",
    "        e_df = pd.DataFrame(e_list, columns=['concept', 'neighbor'])\n",
    "        e_df.to_csv(_EE_dest, index=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeEE_Emb(IterativeEntityExpander):        \n",
    "    def expand_step(self, step_id):\n",
    "        entities, embeddings = zip(*self.entity_emb_dict.items())\n",
    "        \n",
    "        neighbors = []\n",
    "        \n",
    "        concept_emb_dict = dict()\n",
    "        for a_concept, seed_instances in self.seeds_dict.items():\n",
    "            embs = []\n",
    "            for inst in seed_instances:\n",
    "                try:\n",
    "                    embs.append(self.entity_emb_dict[inst])\n",
    "                except KeyError:\n",
    "                    print(f\"{inst} not found in entity_emb_dict??\")\n",
    "                    continue\n",
    "            if len(embs) == 0:\n",
    "                continue\n",
    "            concept_emb = np.mean(embs, axis=0)\n",
    "            concept_emb_dict[a_concept] = concept_emb\n",
    "\n",
    "        concepts = list(concept_emb_dict.keys())\n",
    "        concept_embs = list(concept_emb_dict.values())\n",
    "\n",
    "        # (n_concepts, n_entities)\n",
    "        cos_matrix = cosine_similarity(concept_embs, embeddings)\n",
    "\n",
    "        cands_for_concepts = [[] for _ in range(len(concepts))]\n",
    "        for e_id, e in enumerate(entities):\n",
    "            _scores = cos_matrix[:, e_id]\n",
    "            _max_cc_id = np.argmax(_scores)\n",
    "            _score = _scores[_max_cc_id]\n",
    "            \n",
    "            if e in self.seeds_dict[concepts[_max_cc_id]]:\n",
    "                continue\n",
    "            \n",
    "            cands_for_concepts[_max_cc_id].append({\n",
    "                'concept': concepts[_max_cc_id],\n",
    "                'neighbor': e,\n",
    "                'sim': _score\n",
    "            })\n",
    "\n",
    "        for cc_id, cands in enumerate(cands_for_concepts):\n",
    "            cands_sorted = sorted(cands, key=lambda d: d['sim'], reverse=True)\n",
    "            neighbors.extend(cands_sorted)\n",
    "            \n",
    "            _cc = concepts[cc_id]\n",
    "            _orig_seeds = self.seeds_dict[_cc]\n",
    "            # _ranked_cands = [d['neighbor'] for d in cands_sorted if d['neighbor'] not in _orig_seeds]\n",
    "            _ranked_cands = [d['neighbor'] for d in cands_sorted]\n",
    "            _add_seeds = _ranked_cands[:self.step_K]\n",
    "            _new_seeds = list(set(_orig_seeds + _add_seeds))\n",
    "            self.seeds_dict[_cc] = _new_seeds\n",
    "    \n",
    "        _rankings_dest = os.path.join(self.out_dir, f'rankings-step={step_id}.csv')\n",
    "        c_df = pd.DataFrame(neighbors)\n",
    "        c_df.to_csv(_rankings_dest, index=None)\n",
    "        \n",
    "        _EE_dest = os.path.join(self.out_dir, f'expanded-step={step_id}.csv')\n",
    "        e_list = []\n",
    "        for _cc, _e_list in self.seeds_dict.items():\n",
    "            for _e in _e_list:\n",
    "                e_list.append((_cc, _e))\n",
    "        e_df = pd.DataFrame(e_list, columns=['concept', 'neighbor'])\n",
    "        e_df.to_csv(_EE_dest, index=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "bert_emb_dim = 768\n",
    "benchmark_dir = os.path.join(base_dir, f'data/indeed-benchmark')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8064"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_entity_embeddings = load_embeddings(bert_emb_path, bert_emb_dim)\n",
    "entity_emb_dict = dict(zip(_entity_embeddings['entity'].tolist(),\n",
    "                           _entity_embeddings['embedding'].tolist()))\n",
    "len(entity_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_contr_out_dir = os.path.join(base_dir, f'data/{data_ac}/intermediate/EE_emb_contr-K=3')\n",
    "os.makedirs(emb_contr_out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_contr_expander = IterativeEE_Emb_Contrastive(\n",
    "    step_K=3,\n",
    "    total_steps=20,\n",
    "    out_dir=emb_contr_out_dir,\n",
    "    bert_emb_path=bert_emb_path,\n",
    "    bert_emb_dim=bert_emb_dim,\n",
    "    benchmark_dir=benchmark_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4065e4398924edfa1bdffedec4c8f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_contr_expander.expand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_out_dir = os.path.join(base_dir, f'data/{data_ac}/intermediate/EE_emb-K=3')\n",
    "os.makedirs(emb_out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_expander = IterativeEE_Emb(\n",
    "    step_K=3,\n",
    "    total_steps=20,\n",
    "    out_dir=emb_out_dir,\n",
    "    bert_emb_path=bert_emb_path,\n",
    "    bert_emb_dim=bert_emb_dim,\n",
    "    benchmark_dir=benchmark_dir,\n",
    "    entity_emb_dict=entity_emb_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff61dac13ee247e89292fa4dbcb73bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_expander.expand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different methods overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_knn_k=None.csv')\n",
    "# ee_contr_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None.csv')\n",
    "# ee_emb_aux_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_acontr_knn-aux-k=None.csv')\n",
    "# ee_LM_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')\n",
    "# ee_LM_contr_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_contr_k=None.csv')\n",
    "# ee_LM_aux_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_acontr-aux-k=None.csv')\n",
    "\n",
    "# ee_labels_path = os.path.join(base_dir, f'data/indeed-benchmark/ee-labels-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_contr_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None-2.csv')\n",
    "ee_LM_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None-2.csv')\n",
    "\n",
    "ee_labels_path = os.path.join(base_dir, f'data/indeed-benchmark/ee-labels-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_labels_dict = load_EE_labels(ee_labels_path)\n",
    "concept_list = sorted(list(ee_labels_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background_screening 104\n",
      "person 67\n",
      "schedule 69\n",
      "company 155\n",
      "employee_type 30\n",
      "job_position 152\n",
      "dress_code 268\n",
      "payment_option 30\n",
      "compensation 47\n",
      "shifts 87\n",
      "onboarding_steps 30\n",
      "benefits 116\n",
      "pay_schedule 37\n",
      "hire_prerequisite 137\n"
     ]
    }
   ],
   "source": [
    "for _cc, _e_list in ee_labels_dict.items():\n",
    "    print(_cc, len(_e_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['leggings', 'logos', 'flip flops', 'black jeans', 'jeans'],\n",
       " ['hair', 'body piercings', 'lipstick', 'ear piercings', 'face piercings'])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_concept = 'dress_code'\n",
    "ee_contr_emb_df = pd.read_csv(ee_contr_emb_path)\n",
    "# ee_LM_df = pd.read_csv(ee_LM_path)\n",
    "ee_LM_df = pd.read_csv(ee_LM_path)\n",
    "\n",
    "ee_contr_emb_list = ee_contr_emb_df[ee_contr_emb_df['concept'] == _concept]['neighbor'].tolist()\n",
    "ee_LM_list = ee_LM_df[ee_LM_df['concept'] == _concept]['neighbor'].tolist()\n",
    "ee_contr_emb_list[:5], ee_LM_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 5    Emb = 5(5)    LM = 5(5)    Intersect = 0(0)    Union = 10(10)\n",
      "K = 10   Emb = 10(10)    LM = 10(10)    Intersect = 2(2)    Union = 18(18)\n",
      "K = 20   Emb = 20(20)    LM = 20(20)    Intersect = 5(5)    Union = 35(35)\n",
      "K = 50   Emb = 50(49)    LM = 50(48)    Intersect = 12(12)    Union = 88(85)\n",
      "K = 100  Emb = 100(94)    LM = 100(87)    Intersect = 31(31)    Union = 169(150)\n",
      "K = 300  Emb = 300(228)    LM = 298(151)    Intersect = 135(121)    Union = 463(258)\n",
      "K = 1000 Emb = 387(239)    LM = 996(204)    Intersect = 237(183)    Union = 1146(260)\n"
     ]
    }
   ],
   "source": [
    "for K in [5, 10, 20, 50, 100, 300, 1000]:\n",
    "    _s1 = set(ee_contr_emb_list[:K])\n",
    "    _s2 = set(ee_LM_list[:K])\n",
    "    \n",
    "    _s1_corr = [_e for _e in _s1 if _e in ee_labels_dict[_concept]]\n",
    "    _s2_corr = [_e for _e in _s2 if _e in ee_labels_dict[_concept]]\n",
    "\n",
    "    _ints = _s1 & _s2\n",
    "    _ints_corr = [_e for _e in _ints if _e in ee_labels_dict[_concept]]\n",
    "    \n",
    "    _union = _s1 | _s2\n",
    "    _union_corr = [_e for _e in _union if _e in ee_labels_dict[_concept]]\n",
    "    \n",
    "    print(f\"K = {K:<4d} Emb = {len(_s1)}({len(_s1_corr)})\\\n",
    "    LM = {len(_s2)}({len(_s2_corr)})\\\n",
    "    Intersect = {len(_ints)}({len(_ints_corr)})\\\n",
    "    Union = {len(_union)}({len(_union_corr)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: background_screening(104)\n",
      "K = 5    Emb = 5(5)    LM = 5(5)    Intersect = 1(1)    Union = 9(9)\n",
      "K = 10   Emb = 10(9)    LM = 10(10)    Intersect = 2(2)    Union = 18(17)\n",
      "K = 20   Emb = 20(11)    LM = 20(16)    Intersect = 3(2)    Union = 37(25)\n",
      "K = 50   Emb = 50(24)    LM = 50(33)    Intersect = 8(6)    Union = 92(51)\n",
      "K = 100  Emb = 100(41)    LM = 100(56)    Intersect = 14(10)    Union = 186(87)\n",
      "K = 300  Emb = 300(54)    LM = 300(75)    Intersect = 52(27)    Union = 548(102)\n",
      "K = 1000 Emb = 932(61)    LM = 999(87)    Intersect = 194(46)    Union = 1737(102)\n",
      "\n",
      "Concept: benefits(116)\n",
      "K = 5    Emb = 5(5)    LM = 5(5)    Intersect = 0(0)    Union = 10(10)\n",
      "K = 10   Emb = 10(10)    LM = 10(10)    Intersect = 4(4)    Union = 16(16)\n",
      "K = 20   Emb = 20(15)    LM = 20(20)    Intersect = 7(7)    Union = 33(28)\n",
      "K = 50   Emb = 50(31)    LM = 50(42)    Intersect = 17(15)    Union = 83(58)\n",
      "K = 100  Emb = 100(42)    LM = 100(66)    Intersect = 29(26)    Union = 171(82)\n",
      "K = 300  Emb = 107(45)    LM = 300(92)    Intersect = 38(30)    Union = 369(107)\n",
      "K = 1000 Emb = 107(45)    LM = 999(101)    Intersect = 57(37)    Union = 1049(109)\n",
      "\n",
      "Concept: company(155)\n",
      "K = 5    Emb = 5(5)    LM = 5(5)    Intersect = 1(1)    Union = 9(9)\n",
      "K = 10   Emb = 10(10)    LM = 10(10)    Intersect = 2(2)    Union = 18(18)\n",
      "K = 20   Emb = 20(20)    LM = 20(20)    Intersect = 2(2)    Union = 38(38)\n",
      "K = 50   Emb = 50(49)    LM = 50(41)    Intersect = 4(4)    Union = 96(86)\n",
      "K = 100  Emb = 100(63)    LM = 100(59)    Intersect = 9(6)    Union = 191(116)\n",
      "K = 300  Emb = 300(71)    LM = 298(86)    Intersect = 23(14)    Union = 575(143)\n",
      "K = 1000 Emb = 1000(87)    LM = 996(107)    Intersect = 173(44)    Union = 1823(150)\n",
      "\n",
      "Concept: compensation(47)\n",
      "K = 5    Emb = 5(3)    LM = 5(2)    Intersect = 0(0)    Union = 10(5)\n",
      "K = 10   Emb = 10(5)    LM = 10(5)    Intersect = 0(0)    Union = 20(10)\n",
      "K = 20   Emb = 20(8)    LM = 20(10)    Intersect = 1(1)    Union = 39(17)\n",
      "K = 50   Emb = 50(17)    LM = 50(20)    Intersect = 11(7)    Union = 89(30)\n",
      "K = 100  Emb = 100(24)    LM = 100(30)    Intersect = 21(14)    Union = 179(40)\n",
      "K = 300  Emb = 116(25)    LM = 300(40)    Intersect = 44(22)    Union = 372(43)\n",
      "K = 1000 Emb = 116(25)    LM = 997(42)    Intersect = 69(24)    Union = 1044(43)\n",
      "\n",
      "Concept: dress_code(268)\n",
      "K = 5    Emb = 5(5)    LM = 5(5)    Intersect = 0(0)    Union = 10(10)\n",
      "K = 10   Emb = 10(10)    LM = 10(10)    Intersect = 2(2)    Union = 18(18)\n",
      "K = 20   Emb = 20(20)    LM = 20(20)    Intersect = 5(5)    Union = 35(35)\n",
      "K = 50   Emb = 50(49)    LM = 50(48)    Intersect = 12(12)    Union = 88(85)\n",
      "K = 100  Emb = 100(94)    LM = 100(87)    Intersect = 31(31)    Union = 169(150)\n",
      "K = 300  Emb = 300(228)    LM = 298(151)    Intersect = 135(121)    Union = 463(258)\n",
      "K = 1000 Emb = 387(239)    LM = 996(204)    Intersect = 237(183)    Union = 1146(260)\n",
      "\n",
      "Concept: employee_type(30)\n",
      "K = 5    Emb = 5(2)    LM = 5(4)    Intersect = 0(0)    Union = 10(6)\n",
      "K = 10   Emb = 10(2)    LM = 10(8)    Intersect = 0(0)    Union = 20(10)\n",
      "K = 20   Emb = 20(5)    LM = 20(12)    Intersect = 2(2)    Union = 38(15)\n",
      "K = 50   Emb = 50(7)    LM = 50(15)    Intersect = 4(2)    Union = 96(20)\n",
      "K = 100  Emb = 100(8)    LM = 100(18)    Intersect = 9(2)    Union = 191(24)\n",
      "K = 300  Emb = 135(11)    LM = 299(20)    Intersect = 26(4)    Union = 408(27)\n",
      "K = 1000 Emb = 135(11)    LM = 994(25)    Intersect = 66(9)    Union = 1063(27)\n",
      "\n",
      "Concept: hire_prerequisite(137)\n",
      "K = 5    Emb = 5(5)    LM = 5(3)    Intersect = 1(1)    Union = 9(7)\n",
      "K = 10   Emb = 10(9)    LM = 10(5)    Intersect = 1(1)    Union = 19(13)\n",
      "K = 20   Emb = 20(15)    LM = 20(14)    Intersect = 1(1)    Union = 39(28)\n",
      "K = 50   Emb = 50(26)    LM = 50(33)    Intersect = 2(2)    Union = 98(57)\n",
      "K = 100  Emb = 100(48)    LM = 100(58)    Intersect = 9(8)    Union = 191(98)\n",
      "K = 300  Emb = 217(58)    LM = 300(94)    Intersect = 33(27)    Union = 484(125)\n",
      "K = 1000 Emb = 217(58)    LM = 997(113)    Intersect = 73(41)    Union = 1141(130)\n",
      "\n",
      "Concept: job_position(152)\n",
      "K = 5    Emb = 5(5)    LM = 5(4)    Intersect = 0(0)    Union = 10(9)\n",
      "K = 10   Emb = 10(9)    LM = 10(9)    Intersect = 1(1)    Union = 19(17)\n",
      "K = 20   Emb = 20(17)    LM = 20(18)    Intersect = 1(1)    Union = 39(34)\n",
      "K = 50   Emb = 50(31)    LM = 50(46)    Intersect = 7(7)    Union = 93(70)\n",
      "K = 100  Emb = 100(44)    LM = 100(86)    Intersect = 23(18)    Union = 177(112)\n",
      "K = 300  Emb = 300(83)    LM = 300(124)    Intersect = 87(62)    Union = 513(145)\n",
      "K = 1000 Emb = 321(84)    LM = 999(137)    Intersect = 144(76)    Union = 1176(145)\n",
      "\n",
      "Concept: onboarding_steps(30)\n",
      "K = 5    Emb = 5(2)    LM = 5(3)    Intersect = 0(0)    Union = 10(5)\n",
      "K = 10   Emb = 10(2)    LM = 10(7)    Intersect = 0(0)    Union = 20(9)\n",
      "K = 20   Emb = 20(3)    LM = 20(9)    Intersect = 2(2)    Union = 38(10)\n",
      "K = 50   Emb = 50(5)    LM = 50(15)    Intersect = 5(4)    Union = 95(16)\n",
      "K = 100  Emb = 100(9)    LM = 100(19)    Intersect = 12(6)    Union = 188(22)\n",
      "K = 300  Emb = 300(12)    LM = 299(26)    Intersect = 32(10)    Union = 567(28)\n",
      "K = 1000 Emb = 1000(15)    LM = 999(28)    Intersect = 155(15)    Union = 1844(28)\n",
      "\n",
      "Concept: pay_schedule(37)\n",
      "K = 5    Emb = 5(3)    LM = 4(4)    Intersect = 1(1)    Union = 8(6)\n",
      "K = 10   Emb = 10(4)    LM = 9(9)    Intersect = 1(1)    Union = 18(12)\n",
      "K = 20   Emb = 20(4)    LM = 17(17)    Intersect = 2(2)    Union = 35(19)\n",
      "K = 50   Emb = 24(4)    LM = 47(29)    Intersect = 6(3)    Union = 65(30)\n",
      "K = 100  Emb = 24(4)    LM = 97(32)    Intersect = 6(3)    Union = 115(33)\n",
      "K = 300  Emb = 24(4)    LM = 295(34)    Intersect = 10(4)    Union = 309(34)\n",
      "K = 1000 Emb = 24(4)    LM = 990(34)    Intersect = 12(4)    Union = 1002(34)\n",
      "\n",
      "Concept: payment_option(30)\n",
      "K = 5    Emb = 5(4)    LM = 5(3)    Intersect = 1(1)    Union = 9(6)\n",
      "K = 10   Emb = 10(9)    LM = 10(5)    Intersect = 2(2)    Union = 18(12)\n",
      "K = 20   Emb = 20(16)    LM = 20(10)    Intersect = 5(4)    Union = 35(22)\n",
      "K = 50   Emb = 50(17)    LM = 50(18)    Intersect = 11(9)    Union = 89(26)\n",
      "K = 100  Emb = 100(19)    LM = 100(19)    Intersect = 19(11)    Union = 181(27)\n",
      "K = 300  Emb = 159(19)    LM = 300(19)    Intersect = 36(11)    Union = 423(27)\n",
      "K = 1000 Emb = 159(19)    LM = 995(20)    Intersect = 53(12)    Union = 1101(27)\n",
      "\n",
      "Concept: person(67)\n",
      "K = 5    Emb = 5(4)    LM = 5(4)    Intersect = 0(0)    Union = 10(8)\n",
      "K = 10   Emb = 10(8)    LM = 10(9)    Intersect = 2(2)    Union = 18(15)\n",
      "K = 20   Emb = 20(8)    LM = 20(15)    Intersect = 4(4)    Union = 36(19)\n",
      "K = 50   Emb = 50(11)    LM = 50(22)    Intersect = 7(5)    Union = 93(28)\n",
      "K = 100  Emb = 100(15)    LM = 100(28)    Intersect = 11(6)    Union = 189(37)\n",
      "K = 300  Emb = 300(33)    LM = 299(39)    Intersect = 35(15)    Union = 564(57)\n",
      "K = 1000 Emb = 497(37)    LM = 999(47)    Intersect = 110(26)    Union = 1386(58)\n",
      "\n",
      "Concept: schedule(69)\n",
      "K = 5    Emb = 5(1)    LM = 4(4)    Intersect = 1(1)    Union = 8(4)\n",
      "K = 10   Emb = 10(3)    LM = 8(8)    Intersect = 1(1)    Union = 17(10)\n",
      "K = 20   Emb = 20(6)    LM = 18(18)    Intersect = 2(2)    Union = 36(22)\n",
      "K = 50   Emb = 50(12)    LM = 47(35)    Intersect = 4(4)    Union = 93(43)\n",
      "K = 100  Emb = 100(26)    LM = 97(51)    Intersect = 21(18)    Union = 176(59)\n",
      "K = 300  Emb = 181(33)    LM = 295(55)    Intersect = 64(26)    Union = 412(62)\n",
      "K = 1000 Emb = 181(33)    LM = 991(59)    Intersect = 105(30)    Union = 1067(62)\n",
      "\n",
      "Concept: shifts(87)\n",
      "K = 5    Emb = 5(1)    LM = 5(5)    Intersect = 0(0)    Union = 10(6)\n",
      "K = 10   Emb = 10(1)    LM = 10(9)    Intersect = 0(0)    Union = 20(10)\n",
      "K = 20   Emb = 20(2)    LM = 20(19)    Intersect = 0(0)    Union = 40(21)\n",
      "K = 50   Emb = 50(6)    LM = 50(42)    Intersect = 1(0)    Union = 99(48)\n",
      "K = 100  Emb = 100(11)    LM = 97(63)    Intersect = 5(4)    Union = 192(70)\n",
      "K = 300  Emb = 300(21)    LM = 295(82)    Intersect = 21(17)    Union = 574(86)\n",
      "K = 1000 Emb = 675(21)    LM = 993(82)    Intersect = 56(17)    Union = 1612(86)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _cc in sorted(concept_list):\n",
    "    print(f'Concept: {_cc}({len(ee_labels_dict[_cc])})')\n",
    "    _ee_contr_emb_list = ee_contr_emb_df[ee_contr_emb_df['concept'] == _cc]['neighbor'].tolist()\n",
    "    _ee_LM_list = ee_LM_df[ee_LM_df['concept'] == _cc]['neighbor'].tolist()\n",
    "    \n",
    "    for K in [5, 10, 20, 50, 100, 300, 1000]:\n",
    "        _s1 = set(_ee_contr_emb_list[:K])\n",
    "        _s2 = set(_ee_LM_list[:K])\n",
    "\n",
    "        _s1_corr = [_e for _e in _s1 if _e in ee_labels_dict[_cc]]\n",
    "        _s2_corr = [_e for _e in _s2 if _e in ee_labels_dict[_cc]]\n",
    "\n",
    "        _ints = _s1 & _s2\n",
    "        _ints_corr = [_e for _e in _ints if _e in ee_labels_dict[_cc]]\n",
    "\n",
    "        _union = _s1 | _s2\n",
    "        _union_corr = [_e for _e in _union if _e in ee_labels_dict[_cc]]\n",
    "\n",
    "        print(f\"K = {K:<4d} Emb = {len(_s1)}({len(_s1_corr)})    \" + \\\n",
    "            f\"LM = {len(_s2)}({len(_s2_corr)})    \" + \\\n",
    "            f\"Intersect = {len(_ints)}({len(_ints_corr)})    \" + \\\n",
    "            f\"Union = {len(_union)}({len(_union_corr)})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRR combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112260"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using MRR to combine ranking \n",
    "\n",
    "ee_mrr_combine_list = []\n",
    "\n",
    "for _cc in sorted(concept_list):\n",
    "#     print(f'Concept: {_cc}({len(ee_labels_dict[_cc])})')\n",
    "    _ce_df = ee_contr_emb_df[ee_contr_emb_df['concept'] == _cc].sort_values(by='sim+margin', ascending=False)\n",
    "    _ee_contr_emb_list = _ce_df['neighbor'].tolist()\n",
    "    \n",
    "    _ee_LM_list = ee_LM_df[ee_LM_df['concept'] == _cc]['neighbor'].tolist()\n",
    "    \n",
    "#     _all_entities = set(_ee_contr_emb_list) & set(_ee_LM_list)\n",
    "    \n",
    "    _all_entities_mrr = defaultdict(float)\n",
    "    for i, _e in enumerate(_ee_contr_emb_list):\n",
    "        _all_entities_mrr[_e] += 1.0 / (i+1)\n",
    "    for i, _e in enumerate(_ee_LM_list):\n",
    "        _all_entities_mrr[_e] += 1.0 / (i+1)\n",
    "\n",
    "    _all_entities_mrr_list = sorted(list(_all_entities_mrr.items()), key=lambda p: p[-1], reverse=True)\n",
    "    \n",
    "    for _e, _mrr in _all_entities_mrr_list:\n",
    "        ee_mrr_combine_list.append((_cc, _e, _mrr))\n",
    "\n",
    "len(ee_mrr_combine_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_mrr_combine_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_mrr_combine_k=None.csv')\n",
    "pd.DataFrame(ee_mrr_combine_list, columns=['concept', 'neighbor', 'MRR']).to_csv(ee_mrr_combine_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112260"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Weighted MRR \n",
    "\n",
    "LM_weight = 3.0\n",
    "ee_mrr_combine_list = []\n",
    "\n",
    "for _cc in sorted(concept_list):\n",
    "#     print(f'Concept: {_cc}({len(ee_labels_dict[_cc])})')\n",
    "    _ce_df = ee_contr_emb_df[ee_contr_emb_df['concept'] == _cc].sort_values(by='sim+margin', ascending=False)\n",
    "    _ee_contr_emb_list = _ce_df['neighbor'].tolist()\n",
    "    \n",
    "    _ee_LM_list = ee_LM_df[ee_LM_df['concept'] == _cc]['neighbor'].tolist()\n",
    "    \n",
    "#     _all_entities = set(_ee_contr_emb_list) & set(_ee_LM_list)\n",
    "    \n",
    "    _all_entities_mrr = defaultdict(float)\n",
    "    for i, _e in enumerate(_ee_contr_emb_list):\n",
    "        _all_entities_mrr[_e] += 1.0 / (i+1)\n",
    "    for i, _e in enumerate(_ee_LM_list):\n",
    "        _all_entities_mrr[_e] += LM_weight / (i+1)\n",
    "\n",
    "    _all_entities_mrr_list = sorted(list(_all_entities_mrr.items()), key=lambda p: p[-1], reverse=True)\n",
    "    \n",
    "    for _e, _mrr in _all_entities_mrr_list:\n",
    "        ee_mrr_combine_list.append((_cc, _e, _mrr))\n",
    "\n",
    "len(ee_mrr_combine_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_mrr_combine_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_mrr_combine-lm*3-k=None.csv')\n",
    "pd.DataFrame(ee_mrr_combine_list, columns=['concept', 'neighbor', 'MRR']).to_csv(ee_mrr_combine_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods difference - rank difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None.csv')\n",
    "ee_LM_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 8111)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = pd.read_csv(ee_emb_path)\n",
    "lm_df = pd.read_csv(ee_LM_path)\n",
    "\n",
    "concept_list = lm_df['concept'].drop_duplicates().to_list()\n",
    "entity_list = list(set(lm_df['neighbor'].to_list() + emb_df['neighbor'].to_list()))\n",
    "len(concept_list), len(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_instances_dict = dict(zip(\n",
    "    seed_concepts_df['alignedCategoryName'].tolist(),\n",
    "    seed_concepts_df['seedInstances'].tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_labels_path = os.path.join(base_dir, f'data/indeed-benchmark/ee-labels-2.csv')\n",
    "ee_labels_dict = load_EE_labels(ee_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only consider entities in top-K of at least 1 method \n",
    "K = 200\n",
    "\n",
    "## Diff: emb_rank - lm_rank, higher = more favored by lm, vice versa \n",
    "## diff = 9999: entity not assigned to this concept in emb \n",
    "\n",
    "rank_records = []\n",
    "\n",
    "for _cc in concept_list:\n",
    "    _emb_preds = emb_df[emb_df['concept'] == _cc]['neighbor'].to_list()\n",
    "    _lm_preds = lm_df[lm_df['concept'] == _cc]['neighbor'].to_list()\n",
    "    \n",
    "    _e2ranks = dict([(_e, [-1, -1]) for _e in entity_list])\n",
    "    for i, _e in enumerate(_emb_preds):\n",
    "        _e2ranks[_e][0] = i + 1\n",
    "    for i, _e in enumerate(_lm_preds):\n",
    "        _e2ranks[_e][1] = i + 1\n",
    "    \n",
    "    for _e in seed_instances_dict[_cc]:\n",
    "        if _e in _e2ranks:\n",
    "            del _e2ranks[_e]\n",
    "    \n",
    "    _records = []\n",
    "    for _e, (_r1, _r2) in _e2ranks.items():\n",
    "        # emb, lm \n",
    "        if _r1 not in range(K) and _r2 not in range(K):\n",
    "            continue\n",
    "        if _r1 == -1:\n",
    "            _diff = 9999\n",
    "        else:\n",
    "            _diff = _r1 - _r2\n",
    "        _records.append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'emb_rank': _r1,\n",
    "            'lm_rank': _r2,\n",
    "            'diff': _diff,\n",
    "            'human_label': int(_e in ee_labels_dict[_cc])\n",
    "        })\n",
    "    \n",
    "    _records.sort(key=lambda d: (d['diff'], d['lm_rank']))\n",
    "    rank_records.extend(_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df = pd.DataFrame(rank_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rank_diff.csv')\n",
    "rank_df.to_csv(rank_out_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scalar_features(X, normalize=False):\n",
    "#     f1 = np.array(X)\n",
    "#     f2 = np.log(f1)\n",
    "#     f3 = np.log(1-f1)\n",
    "#     if normalize:\n",
    "#         f1 = (f1 - f1.min()) / (f1.max() - f1.min())\n",
    "#         f2 = (f2 - f2.min()) / (f2.max() - f2.min())\n",
    "#         f3 = (f3 - f3.min()) / (f3.max() - f3.min())\n",
    "#     return np.stack([f1, f2, f3], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar_features([1e-5, 1e-4, 0.1, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_labels_path = os.path.join(base_dir, f'data/indeed-benchmark/ee-labels-2.csv')\n",
    "ee_labels_dict = load_EE_labels(ee_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None.csv')\n",
    "ee_LM_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_instances_dict = dict(zip(\n",
    "    seed_concepts_df['alignedCategoryName'].tolist(),\n",
    "    seed_concepts_df['seedInstances'].tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1: feats, neg sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 76)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_concept = 'job_position'\n",
    "\n",
    "_df = pd.read_csv(ee_emb_path)\n",
    "_emb_preds = _df[_df['concept'] == _concept].to_dict('records')[:100]\n",
    "\n",
    "_df = pd.read_csv(ee_LM_path)\n",
    "_lm_preds = _df[_df['concept'] == _concept].to_dict('records')[:100]\n",
    "\n",
    "_ints = set([d['neighbor'] for d in _emb_preds]) & set([d['neighbor'] for d in _lm_preds])\n",
    "\n",
    "_emb_pos_list = [d for d in _emb_preds if d['neighbor'] in _ints]\n",
    "_emb_neg_list = [d for d in _emb_preds if d['neighbor'] not in _ints]\n",
    "\n",
    "len(_emb_pos_list), len(_emb_neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assistant manager',\n",
       " 'assistant store manager',\n",
       " 'bakery',\n",
       " 'bartender',\n",
       " 'courtesy clerk',\n",
       " 'crew member',\n",
       " 'district manager',\n",
       " 'door greeter',\n",
       " 'driver helper',\n",
       " 'family member',\n",
       " 'general manager',\n",
       " 'grocery store',\n",
       " 'line cook',\n",
       " 'mail carrier',\n",
       " 'manager',\n",
       " 'night stocker',\n",
       " 'packer',\n",
       " 'pharmacy',\n",
       " 'sales rep',\n",
       " 'sales representative',\n",
       " 'salon manager',\n",
       " 'seasonal worker',\n",
       " 'shift supervisor',\n",
       " 'store manger'}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_emb_pos_list, _emb_neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 6), (100,))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N: pos | neg (total = 2N)\n",
    "train_N = 20\n",
    "test_N = 4\n",
    "N = train_N + test_N\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "# pos_samples = random.sample(_emb_pos_list[:N], k=N)\n",
    "# neg_samples = random.sample(_emb_neg_list[:N], k=N)\n",
    "# samples = pos_samples + neg_samples\n",
    "\n",
    "n_pos = len(_emb_pos_list)\n",
    "n_neg = len(_emb_neg_list)\n",
    "\n",
    "all_samples = _emb_pos_list + _emb_neg_list\n",
    "all_labels = [1] * n_pos + [0] * n_neg\n",
    "\n",
    "pos_ids = random.sample(range(n_pos), k=N)\n",
    "neg_ids = random.sample(range(n_pos, n_pos+n_neg), k=N)\n",
    "pos_train_ids = pos_ids[:train_N]\n",
    "neg_train_ids = neg_ids[:train_N]\n",
    "pos_test_ids = pos_ids[train_N:]\n",
    "neg_test_ids = neg_ids[train_N:]\n",
    "\n",
    "sim_feats = np.array([d['sim'] for d in all_samples])\n",
    "sim2_feats = np.array([d['2nd_sim'] for d in all_samples])\n",
    "margin_feats = np.array([d['margin'] for d in all_samples])\n",
    "feats_X = np.stack([\n",
    "    sim_feats,\n",
    "    np.log(1 - sim_feats),\n",
    "    sim2_feats,\n",
    "    np.log(1 - sim2_feats),\n",
    "    margin_feats,\n",
    "    np.log(margin_feats)\n",
    "], axis=-1)\n",
    "\n",
    "labels_Y = np.array(all_labels)\n",
    "\n",
    "feats_X.shape, labels_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 6), (8,))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = feats_X[pos_train_ids + neg_train_ids]\n",
    "train_Y = labels_Y[pos_train_ids + neg_train_ids]\n",
    "test_X = feats_X[pos_test_ids + neg_test_ids]\n",
    "test_Y = labels_Y[pos_test_ids + neg_test_ids]\n",
    "train_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-37.98891136,   2.49464195, -26.57782132,  -3.99934953,\n",
       "        -11.41109004,   1.60053595]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(tol=1e-4, C=1e4)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "# coef for [sim, log(1-sim), 2nd_sim, log(1-2nd_sim), margin, log(margin)]\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67116313, 0.62589717, 0.5465702 , 0.45832449, 0.53638333,\n",
       "       0.30278473, 0.67782842, 0.13473516])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pred_proba = clf.predict_proba(test_X)[:, 1]\n",
    "_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>2nd_concept</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>sim</th>\n",
       "      <th>2nd_sim</th>\n",
       "      <th>margin</th>\n",
       "      <th>sim+margin</th>\n",
       "      <th>pred_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job_position</td>\n",
       "      <td>employee_type</td>\n",
       "      <td>assistant store manager</td>\n",
       "      <td>0.984076</td>\n",
       "      <td>0.978733</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.989419</td>\n",
       "      <td>0.671163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job_position</td>\n",
       "      <td>employee_type</td>\n",
       "      <td>shift supervisor</td>\n",
       "      <td>0.990097</td>\n",
       "      <td>0.982083</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.998112</td>\n",
       "      <td>0.625897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>job_position</td>\n",
       "      <td>company</td>\n",
       "      <td>family member</td>\n",
       "      <td>0.985568</td>\n",
       "      <td>0.982632</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.988504</td>\n",
       "      <td>0.546570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>job_position</td>\n",
       "      <td>company</td>\n",
       "      <td>district manager</td>\n",
       "      <td>0.987350</td>\n",
       "      <td>0.985278</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.989421</td>\n",
       "      <td>0.458324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        concept    2nd_concept                 neighbor       sim   2nd_sim  \\\n",
       "0  job_position  employee_type  assistant store manager  0.984076  0.978733   \n",
       "1  job_position  employee_type         shift supervisor  0.990097  0.982083   \n",
       "2  job_position        company            family member  0.985568  0.982632   \n",
       "3  job_position        company         district manager  0.987350  0.985278   \n",
       "\n",
       "     margin  sim+margin  pred_proba  \n",
       "0  0.005343    0.989419    0.671163  \n",
       "1  0.008014    0.998112    0.625897  \n",
       "2  0.002936    0.988504    0.546570  \n",
       "3  0.002072    0.989421    0.458324  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame(pos_samples[train_N:])\n",
    "_df['pred_proba'] = _pred_proba[:test_N]\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>2nd_concept</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>sim</th>\n",
       "      <th>2nd_sim</th>\n",
       "      <th>margin</th>\n",
       "      <th>sim+margin</th>\n",
       "      <th>pred_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job_position</td>\n",
       "      <td>employee_type</td>\n",
       "      <td>barista</td>\n",
       "      <td>0.993921</td>\n",
       "      <td>0.986723</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>1.001119</td>\n",
       "      <td>0.536383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job_position</td>\n",
       "      <td>employee_type</td>\n",
       "      <td>student</td>\n",
       "      <td>0.991679</td>\n",
       "      <td>0.987705</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.302785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>job_position</td>\n",
       "      <td>employee_type</td>\n",
       "      <td>lead</td>\n",
       "      <td>0.991533</td>\n",
       "      <td>0.986845</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.996222</td>\n",
       "      <td>0.677828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>job_position</td>\n",
       "      <td>employee_type</td>\n",
       "      <td>package handler</td>\n",
       "      <td>0.994812</td>\n",
       "      <td>0.987993</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>1.001630</td>\n",
       "      <td>0.134735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        concept    2nd_concept         neighbor       sim   2nd_sim    margin  \\\n",
       "0  job_position  employee_type          barista  0.993921  0.986723  0.007198   \n",
       "1  job_position  employee_type          student  0.991679  0.987705  0.003974   \n",
       "2  job_position  employee_type             lead  0.991533  0.986845  0.004689   \n",
       "3  job_position  employee_type  package handler  0.994812  0.987993  0.006818   \n",
       "\n",
       "   sim+margin  pred_proba  \n",
       "0    1.001119    0.536383  \n",
       "1    0.995652    0.302785  \n",
       "2    0.996222    0.677828  \n",
       "3    1.001630    0.134735  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame(neg_samples[train_N:])\n",
    "_df['pred_proba'] = _pred_proba[test_N:]\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_proba_all = clf.predict_proba(feats_X)[:, 1]\n",
    "_preds_all = clf.predict(feats_X)\n",
    "assert np.allclose(_preds_all, (_pred_proba_all > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(all_samples)\n",
    "_df['pred_proba'] = _pred_proba_all\n",
    "_df['pred'] = _preds_all\n",
    "_df['label'] = labels_Y\n",
    "_df['human_label'] = _df.apply(lambda d : d[2] in ee_labels_dict['job_position'], axis=1).astype(int)\n",
    "\n",
    "_split = ['Test'] * len(all_samples)\n",
    "for i in pos_train_ids + neg_train_ids:\n",
    "    _split[i] = 'Train'\n",
    "# for i in pos_test_ids + neg_test_ids:\n",
    "#     _split[i] = 'Test'\n",
    "_df['split'] = _split\n",
    "\n",
    "_df.sort_values(by='sim+margin', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "_df[['neighbor', 'sim', 'margin', 'sim+margin', 'pred_proba', 'split', 'pred', 'label', 'human_label']].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.sort_values(by='pred_proba', ascending=False, inplace=False)[['neighbor', 'sim', 'margin', 'sim+margin', 'pred_proba', 'split', 'pred', 'label', 'human_label']].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 62)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(_df['pred'] == _df['human_label']), sum(_df['label'] == _df['human_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 24)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TP, TN \n",
    "sum(_df['pred'] * _df['human_label']), sum((1 - _df['pred']) * (1 - _df['human_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5250346692711781, 2.0483942338052392e-08)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['sim+margin'], _df['human_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34894826201089824, 0.0003734901873738247)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['pred_proba'], _df['human_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2: scalar, pos/neg intersection, all concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1:K1] = pos; [K1:K2] or missing_one = neg \n",
    "K1 = 200\n",
    "K2 = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.read_csv(ee_emb_path)\n",
    "lm_df = pd.read_csv(ee_LM_path)\n",
    "\n",
    "concept_list = emb_df['concept'].drop_duplicates().to_list()\n",
    "\n",
    "cc_pos_ents = dict()\n",
    "cc_neg_ents = dict()\n",
    "\n",
    "for _cc in concept_list:\n",
    "    _emb_df_cc = emb_df[emb_df['concept'] == _cc]\n",
    "    _lm_df_cc = lm_df[lm_df['concept'] == _cc]\n",
    "    \n",
    "    _emb_preds = _emb_df_cc['neighbor'].to_list()\n",
    "    _lm_preds = _lm_df_cc['neighbor'].to_list()\n",
    "    \n",
    "    _pos_ents = []\n",
    "    for _e in _lm_preds[:K1]:\n",
    "        if _e in seed_instances_dict[_cc]:\n",
    "            continue\n",
    "        if _e in _emb_preds[:K1]:\n",
    "            _pos_ents.append(_e)\n",
    "            \n",
    "    _neg_ents = []\n",
    "    for _e in _lm_preds[K1:K2]:\n",
    "        if _e in seed_instances_dict[_cc]:\n",
    "            continue\n",
    "        if _e not in _emb_preds[:K1]:\n",
    "            # emb_contr prediction is not complete \n",
    "            _neg_ents.append(_e)\n",
    "    \n",
    "    cc_pos_ents[_cc] = _pos_ents\n",
    "    cc_neg_ents[_cc] = _neg_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company 16 279\n",
      "dress_code 78 265\n",
      "job_position 60 270\n",
      "pay_schedule 7 296\n",
      "benefits 35 292\n",
      "compensation 36 287\n",
      "payment_option 32 291\n",
      "background_screening 31 268\n",
      "person 24 284\n",
      "hire_prerequisite 25 279\n",
      "shifts 18 296\n",
      "schedule 47 268\n",
      "employee_type 20 282\n",
      "onboarding_steps 19 285\n"
     ]
    }
   ],
   "source": [
    "for _cc in concept_list:\n",
    "    print(_cc, len(cc_pos_ents[_cc]), len(cc_neg_ents[_cc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Emb-contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((681, 1), (681,))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_pos = sum([len(cc_pos_samples[_cc]) for _cc in concept_list])\n",
    "# n_neg = sum([len(cc_neg_samples[_cc]) for _cc in concept_list])\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "all_samples = emb_df.to_dict('record')\n",
    "\n",
    "pos_ids = []\n",
    "neg_ids = []\n",
    "for i, d in enumerate(all_samples):\n",
    "    _e = d['neighbor']\n",
    "    _cc = d['concept']\n",
    "    if _e in seed_instances_dict[_cc]:\n",
    "        continue\n",
    "    if _e in cc_pos_ents[_cc]:\n",
    "        pos_ids.append(i)\n",
    "    elif _e in cc_neg_ents[_cc]:\n",
    "        neg_ids.append(i)\n",
    "\n",
    "n_pos = len(pos_ids)\n",
    "n_neg = len(neg_ids)\n",
    "train_samples = [all_samples[i] for i in pos_ids + neg_ids]\n",
    "train_labels = [1] * n_pos + [0] * n_neg\n",
    "\n",
    "all_score_feats = np.array([d['sim+margin'] for d in all_samples])\n",
    "all_X = np.expand_dims(np.array(all_score_feats), axis=-1)\n",
    "\n",
    "train_X = all_X[pos_ids + neg_ids]\n",
    "train_Y = np.array(train_labels)\n",
    "\n",
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 238)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pos, n_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[24.76929231]]), array([-22.69912982]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(tol=1e-4, C=1e4)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_proba_all = clf.predict_proba(all_X)[:, 1]\n",
    "_preds_all = clf.predict(all_X)\n",
    "assert np.allclose(_preds_all, (_pred_proba_all > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(all_samples)\n",
    "_df['pred_proba'] = _pred_proba_all\n",
    "_df['pred'] = _preds_all\n",
    "_labels = np.ones(len(all_samples)) * float('nan')\n",
    "_labels[pos_ids] = 1\n",
    "_labels[neg_ids] = 0\n",
    "_df['label'] = _labels\n",
    "_df['human_label'] = _df.apply(lambda d : d[2] in ee_labels_dict[d[0]], axis=1).astype(int)\n",
    "\n",
    "_df.sort_values(by='sim+margin', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5968390040264266, 5.96968367874835e-67)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(train_X[:, 0], train_Y[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24228485244317408, 1.3310348799954575e-05)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['sim+margin'][_df['concept'] == 'job_position'],\n",
    "         _df['label'][_df['concept'] == 'job_position'].fillna(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23925277614705454, 1.7173308981842377e-05)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['sim+margin'][_df['concept'] == 'job_position'],\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4830851384005777, 6.991380128318044e-20)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['label'][_df['concept'] == 'job_position'].fillna(0.5),\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2526710578373066, 5.421364437118311e-06)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['pred_proba'][_df['concept'] == 'job_position'],\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_calib-emb_contr-k=None.csv')\n",
    "_df.to_csv(emb_calib_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For LM-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4390, 1), (4390,))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_pos = sum([len(cc_pos_samples[_cc]) for _cc in concept_list])\n",
    "# n_neg = sum([len(cc_neg_samples[_cc]) for _cc in concept_list])\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "all_samples = lm_df.to_dict('record')\n",
    "\n",
    "pos_ids = []\n",
    "neg_ids = []\n",
    "for i, d in enumerate(all_samples):\n",
    "    _e = d['neighbor']\n",
    "    _cc = d['concept']\n",
    "    if _e in seed_instances_dict[_cc]:\n",
    "        continue\n",
    "    if _e in cc_pos_ents[_cc]:\n",
    "        pos_ids.append(i)\n",
    "    elif _e in cc_neg_ents[_cc]:\n",
    "        neg_ids.append(i)\n",
    "\n",
    "n_pos = len(pos_ids)\n",
    "n_neg = len(neg_ids)\n",
    "train_samples = [all_samples[i] for i in pos_ids + neg_ids]\n",
    "train_labels = [1] * n_pos + [0] * n_neg\n",
    "\n",
    "all_score_feats = np.array([d['lm_score'] for d in all_samples])\n",
    "all_X = np.expand_dims(np.array(all_score_feats), axis=-1)\n",
    "\n",
    "train_X = all_X[pos_ids + neg_ids]\n",
    "train_Y = np.array(train_labels)\n",
    "\n",
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 3942)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pos, n_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1493.69497205]]), array([-3.51509008]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(tol=1e-4, C=1e4)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_proba_all = clf.predict_proba(all_X)[:, 1]\n",
    "_preds_all = clf.predict(all_X)\n",
    "assert np.allclose(_preds_all, (_pred_proba_all > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(all_samples)\n",
    "_df['pred_proba'] = _pred_proba_all\n",
    "_df['pred'] = _preds_all\n",
    "_labels = np.ones(len(all_samples)) * float('nan')\n",
    "_labels[pos_ids] = 1\n",
    "_labels[neg_ids] = 0\n",
    "_df['label'] = _labels\n",
    "_df['human_label'] = _df.apply(lambda d : d[1] in ee_labels_dict[d[0]], axis=1).astype(int)\n",
    "\n",
    "_df.sort_values(by='lm_score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3241716503231114, 5.970425324525125e-108)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(train_X[:, 0], train_Y[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1086978156747781, 1.5008556345975551e-22)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['lm_score'][_df['concept'] == 'job_position'],\n",
    "         _df['label'][_df['concept'] == 'job_position'].fillna(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3542178935505932, 3.4462133219041016e-236)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['lm_score'][_df['concept'] == 'job_position'],\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22232557138073736, 1.5034103372699886e-90)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['label'][_df['concept'] == 'job_position'].fillna(0.5),\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6764257188525772, 0.0)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['pred_proba'][_df['concept'] == 'job_position'],\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert-calib-k=None.csv')\n",
    "_df.to_csv(lm_calib_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine: mean calibrated prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_calib-emb_contr-k=None.csv')\n",
    "lm_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_calib-LM_bert-k=None.csv')\n",
    "\n",
    "merge_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_calib-merged-k=None.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8064, 112166)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_calib_df = pd.read_csv(emb_calib_path)\n",
    "lm_calib_df = pd.read_csv(lm_calib_path)\n",
    "\n",
    "emb_calib_scores = dict()\n",
    "for d in emb_calib_df.to_dict('record'):\n",
    "    _k = (d['concept'], d['neighbor'])\n",
    "    _v = d['pred_proba']\n",
    "    emb_calib_scores[_k] = _v\n",
    "\n",
    "lm_calib_scores = dict()\n",
    "for d in lm_calib_df.to_dict('record'):\n",
    "    _k = (d['concept'], d['neighbor'])\n",
    "    _v = d['pred_proba']\n",
    "    lm_calib_scores[_k] = _v\n",
    "\n",
    "len(emb_calib_scores), len(lm_calib_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112324"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_records = []\n",
    "\n",
    "_keys = list(set(emb_calib_scores.keys()) | set(lm_calib_scores.keys()))\n",
    "for _k in _keys:\n",
    "    _cc, _e = _k\n",
    "    _emb_proba = emb_calib_scores.get(_k, 0)\n",
    "    _lm_proba = lm_calib_scores.get(_k, 0)\n",
    "    merged_records.append({\n",
    "        'concept': _cc,\n",
    "        'neighbor': _e,\n",
    "        'emb_p': _emb_proba,\n",
    "        'lm_p': _lm_proba,\n",
    "        'avg': (_emb_proba + _lm_proba) / 2,\n",
    "        'lm*3_avg': (_emb_proba + 3 * _lm_proba) / 4,\n",
    "        'harmonic': (2 * _emb_proba * _lm_proba) / (_emb_proba + _lm_proba),\n",
    "        'min': min(_emb_proba, _lm_proba),\n",
    "        'human_label': int(_e in ee_labels_dict[_cc])\n",
    "    })\n",
    "\n",
    "merged_records.sort(key=lambda d: (d['concept'], -d['avg']))\n",
    "len(merged_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(merged_records).to_csv(merge_calib_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3: joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1:K1] = pos; [K1:K2] or missing_one = neg \n",
    "K1 = 200\n",
    "K2 = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.read_csv(ee_emb_path)\n",
    "lm_df = pd.read_csv(ee_LM_path)\n",
    "\n",
    "concept_list = emb_df['concept'].drop_duplicates().to_list()\n",
    "\n",
    "cc_pos_ents = dict()\n",
    "cc_neg_ents = dict()\n",
    "all_samples = []\n",
    "\n",
    "for _cc in concept_list:\n",
    "    _emb_df_cc = emb_df[emb_df['concept'] == _cc]\n",
    "    _lm_df_cc = lm_df[lm_df['concept'] == _cc]\n",
    "    \n",
    "    _emb_preds = _emb_df_cc['neighbor'].to_list()\n",
    "    _lm_preds = _lm_df_cc['neighbor'].to_list()\n",
    "    \n",
    "    _e2emb_record = dict([(d['neighbor'], d) for d in _emb_df_cc.to_dict('record')])\n",
    "    _e2lm_record = dict([(d['neighbor'], d) for d in _lm_df_cc.to_dict('record')])\n",
    "    for _e in (set(_emb_preds) | set(_lm_preds)):\n",
    "        _emb_d = _e2emb_record.get(_e, None)\n",
    "        _lm_d = _e2lm_record.get(_e, None)\n",
    "        \n",
    "        _emb_sim = 0 if _emb_d is None else _emb_d['sim']\n",
    "        _emb_sim2 = 0 if _emb_d is None else _emb_d['2nd_sim']\n",
    "        _lm_score = 0 if _lm_d is None else _lm_d['lm_score']\n",
    "        all_samples.append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'emb_sim': _emb_sim,\n",
    "            'emb_sim2': _emb_sim2,\n",
    "            'lm_score': _lm_score\n",
    "        })\n",
    "    \n",
    "    _pos_ents = []\n",
    "    for _e in _lm_preds[:K1]:\n",
    "        if _e in seed_instances_dict[_cc]:\n",
    "            continue\n",
    "        if _e in _emb_preds[:K1]:\n",
    "            _pos_ents.append(_e)\n",
    "            \n",
    "    _neg_ents = []\n",
    "    for _e in _lm_preds[K1:K2]:\n",
    "        if _e in seed_instances_dict[_cc]:\n",
    "            continue\n",
    "        if _e not in _emb_preds[:K1]:\n",
    "            # emb_contr prediction is not complete \n",
    "            _neg_ents.append(_e)\n",
    "    \n",
    "    cc_pos_ents[_cc] = _pos_ents\n",
    "    cc_neg_ents[_cc] = _neg_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4377, 3), (4377,))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_pos = sum([len(cc_pos_samples[_cc]) for _cc in concept_list])\n",
    "# n_neg = sum([len(cc_neg_samples[_cc]) for _cc in concept_list])\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "pos_ids = []\n",
    "neg_ids = []\n",
    "for i, d in enumerate(all_samples):\n",
    "    _e = d['neighbor']\n",
    "    _cc = d['concept']\n",
    "    if _e in seed_instances_dict[_cc]:\n",
    "        continue\n",
    "    if _e in cc_pos_ents[_cc]:\n",
    "        pos_ids.append(i)\n",
    "    elif _e in cc_neg_ents[_cc]:\n",
    "        neg_ids.append(i)\n",
    "\n",
    "n_pos = len(pos_ids)\n",
    "n_neg = len(neg_ids)\n",
    "train_samples = [all_samples[i] for i in pos_ids + neg_ids]\n",
    "train_labels = [1] * n_pos + [0] * n_neg\n",
    "\n",
    "all_sim_feats = np.array([d['emb_sim'] for d in all_samples])\n",
    "all_sim2_feats = np.array([d['emb_sim2'] for d in all_samples])\n",
    "all_lm_score_feats = np.array([d['lm_score'] for d in all_samples])\n",
    "\n",
    "all_X = np.stack([\n",
    "    all_sim_feats,\n",
    "    all_sim2_feats,\n",
    "    all_lm_score_feats,\n",
    "], axis=-1)\n",
    "\n",
    "train_X = all_X[pos_ids + neg_ids]\n",
    "train_Y = np.array(train_labels)\n",
    "\n",
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 3934)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pos, n_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 26.78777647, -10.75430862, 685.03393977]]), array([-13.31463886]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(tol=1e-4, C=1e4, class_weight='balanced')\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "# coef_: sim, sim2, lm_score \n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_proba_all = clf.predict_proba(all_X)[:, 1]\n",
    "_preds_all = clf.predict(all_X)\n",
    "assert np.allclose(_preds_all, (_pred_proba_all > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(all_samples)\n",
    "_df['pred_proba'] = _pred_proba_all\n",
    "_df['pred'] = _preds_all\n",
    "_labels = np.ones(len(all_samples)) * float('nan')\n",
    "_labels[pos_ids] = 1\n",
    "_labels[neg_ids] = 0\n",
    "_df['label'] = _labels\n",
    "_df['human_label'] = _df.apply(lambda d : d[1] in ee_labels_dict[d[0]], axis=1).astype(int)\n",
    "\n",
    "_df.sort_values(by=['concept', 'pred_proba'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21812334653501303, 5.8811988683958415e-87)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['label'][_df['concept'] == 'job_position'].fillna(0.5),\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4322842625273697, 0.0)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(_df['pred_proba'][_df['concept'] == 'job_position'],\n",
    "         _df['human_label'][_df['concept'] == 'job_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_calib-joint-k=None.csv')\n",
    "_df.to_csv(joint_calib_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.read_csv(ee_emb_path)\n",
    "lm_df = pd.read_csv(ee_LM_path)\n",
    "\n",
    "concept_list = emb_df['concept'].drop_duplicates().to_list()\n",
    "\n",
    "cc_all_samples = defaultdict(list)\n",
    "\n",
    "for _cc in concept_list:\n",
    "    _emb_df_cc = emb_df[emb_df['concept'] == _cc]\n",
    "    _lm_df_cc = lm_df[lm_df['concept'] == _cc]\n",
    "    \n",
    "    _emb_preds = _emb_df_cc['neighbor'].to_list()\n",
    "    _lm_preds = _lm_df_cc['neighbor'].to_list()\n",
    "    \n",
    "    _e2emb_record = dict([(d['neighbor'], (d, i+1)) for i, d in enumerate(_emb_df_cc.to_dict('record'))])\n",
    "    _e2lm_record = dict([(d['neighbor'], (d, i+1)) for i, d in enumerate(_lm_df_cc.to_dict('record'))])\n",
    "    for _e in (set(_emb_preds) | set(_lm_preds)):\n",
    "        _emb_d, _emb_r = _e2emb_record.get(_e, (None, np.inf))\n",
    "        _lm_d, _lm_r = _e2lm_record.get(_e, (None, np.inf))\n",
    "        \n",
    "        _emb_sim = 0 if _emb_d is None else _emb_d['sim']\n",
    "        _emb_sim2 = 0 if _emb_d is None else _emb_d['2nd_sim']\n",
    "        _lm_score = 0 if _lm_d is None else _lm_d['lm_score']\n",
    "        cc_all_samples[_cc].append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'emb_sim': _emb_sim,\n",
    "            'emb_sim2': _emb_sim2,\n",
    "            'lm_score': _lm_score,\n",
    "            'emb_rank': _emb_r,\n",
    "            'lm_rank': _lm_r\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company 150 7873\n",
      "dress_code 261 7751\n",
      "job_position 145 7865\n",
      "pay_schedule 34 7979\n",
      "benefits 110 7902\n",
      "compensation 43 7970\n",
      "payment_option 27 7988\n",
      "background_screening 102 7927\n",
      "person 58 7954\n",
      "hire_prerequisite 132 7884\n",
      "shifts 86 7943\n",
      "schedule 62 7949\n",
      "employee_type 27 7987\n",
      "onboarding_steps 28 8023\n"
     ]
    }
   ],
   "source": [
    "# n_pos = sum([len(cc_pos_samples[_cc]) for _cc in concept_list])\n",
    "# n_neg = sum([len(cc_neg_samples[_cc]) for _cc in concept_list])\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "cc_df = None\n",
    "\n",
    "for _cc, _all_samples in cc_all_samples.items():\n",
    "    pos_ids = []\n",
    "    neg_ids = []\n",
    "    for i, d in enumerate(_all_samples):\n",
    "        _e = d['neighbor']\n",
    "        _cc = d['concept']\n",
    "        if _e in seed_instances_dict[_cc]:\n",
    "            continue\n",
    "        if _e in ee_labels_dict[_cc]:\n",
    "            pos_ids.append(i)\n",
    "        else:\n",
    "            neg_ids.append(i)\n",
    "\n",
    "    n_pos = len(pos_ids)\n",
    "    n_neg = len(neg_ids)\n",
    "    train_samples = [_all_samples[i] for i in pos_ids + neg_ids]\n",
    "    train_labels = [1] * n_pos + [0] * n_neg\n",
    "\n",
    "    _all_sim_feats = np.array([d['emb_sim'] for d in _all_samples])\n",
    "    _all_sim2_feats = np.array([d['emb_sim2'] for d in _all_samples])\n",
    "    _all_lm_score_feats = np.array([d['lm_score'] for d in _all_samples])\n",
    "    _all_emb_ranks = np.array([d['emb_rank'] for d in _all_samples])\n",
    "    _all_lm_ranks = np.array([d['lm_rank'] for d in _all_samples])\n",
    "\n",
    "    _all_X = np.stack([\n",
    "        _all_sim_feats,\n",
    "        np.log(1 - _all_sim_feats),\n",
    "        _all_sim2_feats,\n",
    "        np.log(1 - _all_sim2_feats),\n",
    "        _all_lm_score_feats,\n",
    "        np.log(_all_lm_score_feats + 1e-9),\n",
    "        1 / _all_emb_ranks,\n",
    "        1 / _all_lm_ranks\n",
    "    ], axis=-1)\n",
    "\n",
    "    train_X = _all_X[pos_ids + neg_ids]\n",
    "    train_Y = np.array(train_labels)\n",
    "\n",
    "    print(_cc, n_pos, n_neg)\n",
    "#     print(train_X.shape, train_Y.shape)\n",
    "    \n",
    "#     clf = make_pipeline(\n",
    "#         StandardScaler(),\n",
    "#         LogisticRegression(tol=1e-12, C=1e12, max_iter=1000)\n",
    "#     )\n",
    "    clf = LogisticRegression(tol=1e-8, C=1e4, max_iter=1000)\n",
    "    clf.fit(train_X, train_Y)\n",
    "\n",
    "    # coef_: sim, sim2, lm_score \n",
    "#     _lr = clf.steps[1][1]\n",
    "#     print(_lr.coef_, _lr.intercept_)\n",
    "    \n",
    "    _pred_proba_all = clf.predict_proba(_all_X)[:, 1]\n",
    "    _preds_all = clf.predict(_all_X)\n",
    "    assert np.allclose(_preds_all, (_pred_proba_all > 0.5).astype(int))\n",
    "    \n",
    "    _df = pd.DataFrame(_all_samples)\n",
    "    _df['pred_proba'] = _pred_proba_all\n",
    "    _df['pred'] = _preds_all\n",
    "    _labels = np.ones(len(_all_samples)) * float('nan')\n",
    "    _labels[pos_ids] = 1\n",
    "    _labels[neg_ids] = 0\n",
    "    _df['human_label'] = _labels\n",
    "\n",
    "    _df.sort_values(by='pred_proba', ascending=False, inplace=True)\n",
    "    \n",
    "    if cc_df is None:\n",
    "        cc_df = _df\n",
    "    else:\n",
    "        cc_df = cc_df.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(725, 1265)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((cc_df['pred'] == 1) & (cc_df['human_label'] == 1)), sum(cc_df['human_label'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 540)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((cc_df['pred'] == 1) & (cc_df['human_label'] == 0)), sum((cc_df['pred'] == 0) & (cc_df['human_label'] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub_calib_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_calib-ub-k=None-2.csv')\n",
    "cc_df.to_csv(ub_calib_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compute_multi_view_embeddings\n",
    "importlib.reload(compute_multi_view_embeddings)\n",
    "from compute_multi_view_embeddings import get_lm_probe_concept_embeddings, get_lm_probe_entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res = get_lm_probe_concept_embeddings(model_path='bert-base-uncased',\n",
    "                                       seed_concepts_path=seed_aligned_concepts_path)\n",
    "_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_ccs = ['benefits', 'schedule', 'pay_schedule']\n",
    "\n",
    "cosine_similarity([_res[_cc] for _cc in _tmp_ccs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ent_res = get_lm_probe_entity_embeddings(\n",
    "    model_path='bert-base-uncased',\n",
    "    embed_num_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt'))\n",
    "len(_ent_res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_ents = ['starbucks', '401k', 'sales associate']\n",
    "_tmp_ccs = ['company', 'benefits', 'job_position']\n",
    "\n",
    "cosine_similarity([_res[_cc] for _cc in _tmp_ccs],\n",
    "                  [_ent_res[_e] for _e in _tmp_ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script: getting vector embeddings : indeedQA \n",
    "\n",
    "!python compute_multi_view_embeddings.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-scp $seed_aligned_concepts_path \\\n",
    "-enp $base_dir/data/$data_ac/intermediate/BERTembednum+seeds.txt \\\n",
    "-es $base_dir/data/$data_ac/intermediate/BERTembed+seeds.txt \\\n",
    "-m bert-base-uncased \\\n",
    "--lm_ent_hearst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n",
      "lm_probe entity embeddings: 100%|███████████| 2694/2694 [20:05<00:00,  2.24it/s]\n",
      "Saving lm entity embeddings\n"
     ]
    }
   ],
   "source": [
    "# Wiki \n",
    "!CUDA_VISIBLE_DEVICES=2 python compute_multi_view_embeddings.py \\\n",
    "-d $wiki_data_dir \\\n",
    "-scp $wiki_gt_dir/seed_aligned_concepts.csv \\\n",
    "-enp $wiki_data_dir/BERTembednum_gt.txt \\\n",
    "-es $wiki_data_dir/BERTembed_gt.txt \\\n",
    "-m bert-base-uncased \\\n",
    "-ename gt \\\n",
    "--lm_ent_hearst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(wiki_data_dir, 'BERTembednum_gt.txt'), 'r') as f:\n",
    "    _entities = [l.strip().rsplit(' ', 1)[0] for l in f]\n",
    "len(_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_seeds = load_seed_aligned_concepts(os.path.join(wiki_gt_dir, 'seed_aligned_concepts.csv'))\n",
    "_seed_insts = [_e for _l in _seeds['seedInstances'].tolist() for _e in _l]\n",
    "len(_seed_insts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(_seed_insts) - set(_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2776"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gt_entities = set()\n",
    "for f_path in glob(f'{wiki_gt_dir}/*.txt'):\n",
    "    with open(f_path, 'r') as f:\n",
    "        _gt_entities.update([l.lower().strip().split('\\t')[1] for l in f])\n",
    "len(_gt_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## gt entities having no context \n",
    "len(set(_gt_entities) - set(_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiview_EE_datasets, multiview_EE_models\n",
    "importlib.reload(multiview_EE_datasets) \n",
    "importlib.reload(multiview_EE_models)\n",
    "from multiview_EE_datasets import Wiki_EE_Dataset, Wiki_EE_Dataset_2, Indeed_EE_Dataset, Indeed_EE_Dataset_2\n",
    "from multiview_EE_models import EE_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_wiki_data(wiki_gt_dir, wiki_embednum_path, wiki_seeds_path=None, dest=None):\n",
    "    if wiki_seeds_path is None:\n",
    "        wiki_seeds_path = os.path.join(wiki_gt_dir, 'seed_aligned_concepts.csv')\n",
    "\n",
    "    wiki_seeds_df = pd.read_csv(wiki_seeds_path)\n",
    "    concept_list = wiki_seeds_df['alignedCategoryName'].tolist()\n",
    "    \n",
    "    with open(wiki_embednum_path, 'r') as f:\n",
    "        valid_ents = set([l.strip().rsplit(' ', 1)[0] for l in f])\n",
    "\n",
    "    cc2ent_records = dict()\n",
    "    for cc in concept_list:\n",
    "        _cc_filepath = os.path.join(wiki_gt_dir, f'{cc}.txt')\n",
    "        _ent_records = []\n",
    "        with open(_cc_filepath, 'r') as f:\n",
    "            for l in f:\n",
    "                _id, _e, _label = l.strip().split('\\t')\n",
    "                if _e.lower() not in valid_ents:\n",
    "                    continue\n",
    "                _ent_records.append((_e.lower(), int(_label)))\n",
    "        cc2ent_records[cc] = _ent_records\n",
    "\n",
    "    all_samples = []\n",
    "    for cc, _ent_records in cc2ent_records.items():\n",
    "        for _e, _label in _ent_records:\n",
    "            all_samples.append({\n",
    "                'concept': cc,\n",
    "                'neighbor': _e,\n",
    "                'label': _label\n",
    "            })\n",
    "    \n",
    "    if dest is not None:\n",
    "        pd.DataFrame(all_samples).to_csv(dest, index=None)\n",
    "        \n",
    "    return all_samples\n",
    "\n",
    "def split_wiki_data(all_path, train_path, dev_path, test_path):\n",
    "    all_samples_df = pd.read_csv(all_path)\n",
    "    concept_list = all_samples_df['concept'].drop_duplicates().to_list()\n",
    "    dev_C = test_C = max(int(len(concept_list) / 4), 2)\n",
    "    train_C = len(concept_list) - dev_C - test_C\n",
    "    \n",
    "#     random.seed(123)\n",
    "#     random.shuffle(concept_list)\n",
    "#     train_concepts = concept_list[:train_C]\n",
    "#     dev_concepts = concept_list[train_C:train_C+dev_C]\n",
    "#     test_concepts = concept_list[train_C+dev_C:]\n",
    "\n",
    "    ## Manually balanced split \n",
    "    train_concepts = ['companies', 'countries', 'diseases', 'parties']\n",
    "    dev_concepts = ['china_provinces', 'sportsleagues']\n",
    "    test_concepts = ['tv_channels' ,'us_states']\n",
    "    print(train_concepts)\n",
    "    print(dev_concepts)\n",
    "    print(test_concepts)\n",
    "    \n",
    "    all_samples = all_samples_df.to_dict('records')\n",
    "    train_concepts = [d for d in all_samples if d['concept'] in train_concepts]\n",
    "    dev_concepts = [d for d in all_samples if d['concept'] in dev_concepts]\n",
    "    test_concepts = [d for d in all_samples if d['concept'] in test_concepts]\n",
    "    \n",
    "    pd.DataFrame(train_concepts).to_csv(train_path, index=None)\n",
    "    pd.DataFrame(dev_concepts).to_csv(dev_path, index=None)\n",
    "    pd.DataFrame(test_concepts).to_csv(test_path, index=None)\n",
    "    \n",
    "# def load_wiki_data_tensors(ds_paths,\n",
    "#                            emb_ent_path, \n",
    "#                            emb_cc_path, \n",
    "#                            lm_ent_path, \n",
    "#                            lm_cc_path,\n",
    "#                            embeddings_dim):\n",
    "#     # ds_paths = Dict[ds, path]\n",
    "#     ## Test todo \n",
    "    \n",
    "#     emb_ent_dict = load_embeddings_dict(emb_ent_path, embeddings_dim)\n",
    "#     emb_cc_dict = load_embeddings_dict(emb_cc_path, embeddings_dim)\n",
    "#     lm_ent_dict = load_embeddings_dict(lm_ent_path, embeddings_dim)\n",
    "#     lm_cc_dict = load_embeddings_dict(lm_cc_path, embeddings_dim)\n",
    "    \n",
    "#     ds_tensors = dict()  # Dict[ds, Dict[tensor_name, tensor]]\n",
    "#     for ds, path in ds_paths.items():\n",
    "#         samples = pd.read_csv(path).to_dict('record')\n",
    "#         emb_ent = []\n",
    "#         emb_cc = []\n",
    "#         lm_ent = []\n",
    "#         lm_cc = []\n",
    "#         labels = []\n",
    "#         for d in samples:\n",
    "#             _e, _cc, _lbl = d['neighbor'], d['concept'], d['label']\n",
    "#             emb_ent.append(emb_ent_dict[_e])\n",
    "#             emb_cc.append(emb_cc_dict[_cc])\n",
    "#             lm_ent.append(lm_ent_dict[_e])\n",
    "#             lm_cc.append(lm_cc_dict[_cc])\n",
    "#             labels.append(_lbl)\n",
    "#         ds_tensors[ds] = {\n",
    "#             'emb_ent': torch.tensor(emb_ent, dtype=torch.float32),\n",
    "#             'emb_cc': torch.tensor(emb_cc, dtype=torch.float32),\n",
    "#             'lm_ent': torch.tensor(lm_ent, dtype=torch.float32),\n",
    "#             'lm_cc': torch.tensor(lm_cc, dtype=torch.float32),\n",
    "#             'label': torch.tensor(label, dtype=torch.int),\n",
    "#         }\n",
    "    \n",
    "#     return ds_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = preproc_wiki_data(wiki_gt_dir=wiki_gt_dir,\n",
    "                  wiki_embednum_path=os.path.join(wiki_data_dir, 'BERTembednum_gt.txt'),\n",
    "                  dest=os.path.join(wiki_data_dir, 'wiki_ee_all.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['companies', 'countries', 'diseases', 'parties']\n",
      "['china_provinces', 'sportsleagues']\n",
      "['tv_channels', 'us_states']\n"
     ]
    }
   ],
   "source": [
    "split_wiki_data(all_path=os.path.join(wiki_data_dir, 'wiki_ee_all.csv'),\n",
    "                train_path=os.path.join(wiki_data_dir, 'wiki_ee_train.csv'),\n",
    "                dev_path=os.path.join(wiki_data_dir, 'wiki_ee_dev.csv'),\n",
    "                test_path=os.path.join(wiki_data_dir, 'wiki_ee_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: preproc of indeed\n",
    "\n",
    "training: seeds as positive, seeds-mismatch as negative (using 3.15.3 Concepts overlapping to avoid wrong negatives)\n",
    "\n",
    "valid/test: labed positive & negative (sample negatives to have the same amount)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_instances_dict = dict(zip(seed_concepts_df['alignedCategoryName'].tolist(),\n",
    "                               seed_concepts_df['seedInstances'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_training_records = [] ## concept, neighbor, label \n",
    "\n",
    "random.seed(123)\n",
    "_neg_cnt = Counter()  # do not use one entity >2 times as negative sample\n",
    "for _cc, _seeds in seed_instances_dict.items():\n",
    "    _non_overlap_ccs = [c for c in seed_instances_dict.keys() if c != _cc and c not in cc_overlap_dict[_cc]]\n",
    "    _neg_cands = [e for c in _non_overlap_ccs for e in seed_instances_dict[c] if _neg_cnt[e] < 2]\n",
    "    _neg_samples = random.sample(_neg_cands, k=len(_seeds))\n",
    "    \n",
    "    for _e in _seeds:\n",
    "        indeed_training_records.append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'label': 1\n",
    "        })\n",
    "        \n",
    "    for _e in _neg_samples:\n",
    "        indeed_training_records.append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'label': 0\n",
    "        })\n",
    "        _neg_cnt[_e] += 1\n",
    "        \n",
    "indeed_training_records.sort(key=lambda d: (d['concept'], -d['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_train_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/indeed_ee_seed_train.csv')\n",
    "pd.DataFrame(indeed_training_records).to_csv(indeed_train_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company 150 124 124\n",
      "dress_code 261 56 56\n",
      "job_position 145 108 108\n",
      "pay_schedule 34 87 34\n",
      "benefits 110 97 97\n",
      "compensation 43 164 43\n",
      "payment_option 27 197 27\n",
      "background_screening 102 123 102\n",
      "person 58 198 58\n",
      "hire_prerequisite 133 157 133\n",
      "shifts 88 170 88\n",
      "schedule 62 179 62\n",
      "employee_type 27 200 27\n",
      "onboarding_steps 28 232 28\n"
     ]
    }
   ],
   "source": [
    "raw_ee_label_file = os.path.join(base_dir, f'data/indeed-benchmark/ee-increment-labels-2.csv')\n",
    "\n",
    "raw_ee_df = pd.read_csv(raw_ee_label_file).dropna()\n",
    "\n",
    "indeed_test_records = []\n",
    "random.seed(123)\n",
    "for _cc, _seeds in seed_instances_dict.items():\n",
    "    _cc_df = raw_ee_df[raw_ee_df['concept'] == _cc]\n",
    "    _pos_df = _cc_df[_cc_df['Majority'] == 1]\n",
    "    _neg_df = _cc_df[_cc_df['Majority'] == 0]\n",
    "    _pos_list = [e for e in _pos_df['neighbor'].to_list() if e not in _seeds]\n",
    "    _neg_list = [e for e in _neg_df['neighbor'].to_list() if e not in _seeds]\n",
    "    \n",
    "    _n = min(len(_pos_list), len(_neg_list))\n",
    "    print(_cc, len(_pos_list), len(_neg_list), _n)\n",
    "    \n",
    "    _pos_samples = random.sample(_pos_list, k=_n)\n",
    "    _neg_samples = random.sample(_neg_list, k=_n)\n",
    "    \n",
    "    for e in _pos_samples:\n",
    "        indeed_test_records.append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'label': 1\n",
    "        })\n",
    "    for e in _neg_samples:\n",
    "        indeed_test_records.append({\n",
    "            'concept': _cc,\n",
    "            'neighbor': _e,\n",
    "            'label': 0\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_test_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/indeed_ee_test.csv')\n",
    "pd.DataFrame(indeed_test_records).to_csv(indeed_test_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_set = Wiki_EE_Dataset(\n",
    "    ds_path=os.path.join(wiki_data_dir, 'wiki_ee_train.csv'), \n",
    "    emb_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt.txt'), \n",
    "    emb_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_entities.txt'), \n",
    "    lm_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_concepts.txt'),\n",
    ")\n",
    "wiki_train_loader = DataLoader(wiki_train_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dev_set = Wiki_EE_Dataset(\n",
    "    ds_path=os.path.join(wiki_data_dir, 'wiki_ee_dev.csv'), \n",
    "    emb_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt.txt'), \n",
    "    emb_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_entities.txt'), \n",
    "    lm_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_concepts.txt'),\n",
    ")\n",
    "wiki_dev_loader = DataLoader(wiki_dev_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_set = Wiki_EE_Dataset(\n",
    "    ds_path=os.path.join(wiki_data_dir, 'wiki_ee_test.csv'), \n",
    "    emb_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt.txt'), \n",
    "    emb_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_entities.txt'), \n",
    "    lm_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_concepts.txt'),\n",
    ")\n",
    "wiki_test_loader = DataLoader(wiki_test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_set_2 = Wiki_EE_Dataset_2(\n",
    "    ds_path=os.path.join(wiki_data_dir, 'wiki_ee_test.csv'), \n",
    "    emb_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt.txt'), \n",
    "    emb_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_entities_hearst.csv'), \n",
    "    lm_cc_path=os.path.join(wiki_data_dir, 'BERTembed_gt_lm_concepts.txt'),\n",
    ")\n",
    "wiki_test_loader_2 = DataLoader(wiki_test_set_2, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_test_set = Indeed_EE_Dataset(\n",
    "    ds_path=os.path.join(benchmark_dir, 'ee-labels-2.csv'), \n",
    "    emb_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt'), \n",
    "    emb_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_entities.txt'), \n",
    "    lm_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_concepts.txt'),\n",
    ")\n",
    "indeed_test_loader = DataLoader(indeed_test_set, batch_size=1, shuffle=False)\n",
    "len(indeed_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112896"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed_test_set_2 = Indeed_EE_Dataset_2(\n",
    "    ds_path=os.path.join(benchmark_dir, 'ee-labels-2.csv'), \n",
    "    emb_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt'), \n",
    "    emb_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_entities_hearst.csv'), \n",
    "    lm_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_concepts.txt'),\n",
    "    emb_ent_dict=indeed_ft_train_set.emb_ent_dict,\n",
    "    emb_cc_dict=indeed_ft_train_set.emb_cc_dict,\n",
    "    lm_ent_dict=indeed_ft_train_set.lm_ent_dict,\n",
    "    lm_cc_dict=indeed_ft_train_set.lm_cc_dict,\n",
    ")\n",
    "indeed_test_loader_2 = DataLoader(indeed_test_set_2, batch_size=1, shuffle=False)\n",
    "len(indeed_test_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept': 'us_states', 'neighbor': 'iowa', 'label': 1}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test_set.sample_records[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept': 'us_states', 'neighbor': 'indiana', 'label': 1}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test_set_2.sample_records[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_set_2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 $wiki_data_dir/BERTembed_gt_lm_entities_hearst.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_clf = EE_Classifier(\n",
    "    embeddings_dim=768,\n",
    "    ff_dims=[8, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1295: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=None, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(ee_clf,\n",
    "            train_dataloaders=wiki_train_loader,\n",
    "            val_dataloaders=wiki_dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept': 'companies', 'neighbor': 'siemens', 'label': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dev_set.sample_records[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(ee_clf, dataloaders=wiki_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.test.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emb_logits': tensor(0.6159, grad_fn=<SqueezeBackward1>),\n",
       " 'lm_logits': tensor(0.5063, grad_fn=<SqueezeBackward1>),\n",
       " 'joint_logits': tensor(0.5611, grad_fn=<MeanBackward1>)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee_clf(wiki_dev_set[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _emb_ent = torch.randn(4, 16)\n",
    "# _emb_cc = torch.randn(4, 16)\n",
    "# _lm_ent = torch.randn(4, 16)\n",
    "# _lm_cc = torch.randn(4, 16)\n",
    "# _labels = torch.randint(high=2, size=(4,)).to(torch.float32)\n",
    "\n",
    "# _in_batch = {\n",
    "#     'emb_ent': _emb_ent,\n",
    "#     'emb_cc': _emb_cc,\n",
    "#     'lm_ent': _lm_ent,\n",
    "#     'lm_cc': _lm_cc,\n",
    "#     'labels': _labels,\n",
    "# }\n",
    "\n",
    "# ee_clf.forward(_in_batch)\n",
    "\n",
    "# loss = ee_clf.training_step(_in_batch, 0)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Script \n",
    "!CUDA_VISIBLE_DEVICES=2 python multiview_EE_train.py \\\n",
    "-d $wiki_data_dir \\\n",
    "-o $wiki_data_dir/ee_clf_test.csv \\\n",
    "-ep 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should rename dataset classes...\n",
    "\n",
    "indeed_ft_train_set = Wiki_EE_Dataset_2(\n",
    "    ds_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/indeed_ee_seed_train.csv'), \n",
    "    emb_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt'), \n",
    "    emb_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_entities_hearst.csv'), \n",
    "    lm_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_concepts.txt'),\n",
    ")\n",
    "\n",
    "indeed_ft_train_loader = DataLoader(indeed_ft_train_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_ft_val_set = Wiki_EE_Dataset_2(\n",
    "    ds_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/indeed_ee_test.csv'), \n",
    "    emb_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt'), \n",
    "    emb_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_concepts.txt'), \n",
    "    lm_ent_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_entities_hearst.csv'), \n",
    "    lm_cc_path=os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed_lm_concepts.txt'),\n",
    "    emb_ent_dict=indeed_ft_train_set.emb_ent_dict,\n",
    "    emb_cc_dict=indeed_ft_train_set.emb_cc_dict,\n",
    "    lm_ent_dict=indeed_ft_train_set.lm_ent_dict,\n",
    "    lm_cc_dict=indeed_ft_train_set.lm_cc_dict,\n",
    ")\n",
    "\n",
    "indeed_ft_val_loader = DataLoader(indeed_ft_val_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1295: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=None, \n",
    "                     max_epochs=10,\n",
    "                     callbacks=ModelCheckpoint(save_top_k=-1),\n",
    "                     logger=TensorBoardLogger(save_dir='lightning_logs', version='14.2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'lightning_logs/default/version_14/checkpoints/epoch=999-step=385999.ckpt'\n",
    "# hparams_file = 'lightning_logs/version_8/hparams.yaml'\n",
    "ee_clf_pretrained = EE_Classifier.load_from_checkpoint(ckpt_path,\n",
    "                                                       embeddings_dim=768,\n",
    "                                                       optim_type='sgd',\n",
    "                                                       init_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type       | Params\n",
      "-----------------------------------------------\n",
      "0 | views_clf_heads | ModuleDict | 1.6 M \n",
      "-----------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.426     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/transformers_nikita/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef259e1ea7164893816042cf5922dfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(ee_clf_pretrained,\n",
    "            train_dataloaders=indeed_ft_train_loader,\n",
    "            val_dataloaders=indeed_ft_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: evaluate each finetuned ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pretrained_ckpt(ckpt_path, test_loader, ee_pred_out_path=None, trainer=None):\n",
    "    ee_clf_trained = EE_Classifier.load_from_checkpoint(ckpt_path, embeddings_dim=768)\n",
    "    test_set = test_loader.dataset\n",
    "    if trainer is None:\n",
    "        trainer = pl.Trainer(gpus=None)\n",
    "    \n",
    "    preds = trainer.predict(ee_clf_trained, dataloaders=test_loader)\n",
    "    \n",
    "    pred_records = []\n",
    "    for i in range(len(test_set)):\n",
    "        _raw_d = test_set.sample_records[i]\n",
    "        _pred_d = preds[i]\n",
    "\n",
    "        pred_records.append({\n",
    "            'concept': _raw_d['concept'],\n",
    "            'neighbor': _raw_d['neighbor'],\n",
    "            'label': _raw_d['label'],\n",
    "            'emb_logits': _pred_d['emb_logits'][0],\n",
    "            'lm_logits': _pred_d['lm_logits'][0],\n",
    "            'joint_logits': _pred_d['joint_logits'][0],\n",
    "            'emb_pred': _pred_d['emb_pred'][0],\n",
    "            'lm_pred': _pred_d['lm_pred'][0],\n",
    "            'joint_pred': _pred_d['joint_pred'][0]\n",
    "        })\n",
    "\n",
    "    if ee_pred_out_path is not None:\n",
    "        pd.DataFrame(pred_records).to_csv(ee_pred_out_path, index=None)\n",
    "    \n",
    "#     print('Emb acc:', sum([d['emb_pred'][0] == d['label'][0] for d in preds]) / len(preds))\n",
    "#     print('LM acc:', sum([d['lm_pred'][0] == d['label'][0] for d in preds]) / len(preds))\n",
    "#     print('Joint acc:', sum([d['joint_pred'][0] == d['label'][0] for d in preds]) / len(preds))\n",
    "    \n",
    "    print_results(preds)\n",
    "    \n",
    "    print('Joint stats:')\n",
    "    _labels = [d['label'][0] for d in preds]\n",
    "    _preds = [d['joint_pred'][0] for d in preds]\n",
    "    print('correct pos', sum([(l == 1) & (p == 1) for l, p in zip(_labels, _preds)]))\n",
    "    print('correct neg', sum([(l == 0) & (p == 0) for l, p in zip(_labels, _preds)]))\n",
    "    print('correct all', sum([l == p for l, p in zip(_labels, _preds)]))\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pretrained_ckpt(ckpt_path='lightning_logs/default/version_14/checkpoints/epoch=999-step=385999.ckpt',\n",
    "                     test_loader=indeed_ft_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(preds):\n",
    "    _labels = [d['label'][0] for d in preds]\n",
    "    \n",
    "    for m in ['emb', 'lm', 'joint']:\n",
    "        _preds = [d[f'{m}_pred'][0] for d in preds]\n",
    "        \n",
    "        TP = sum([(l == 1) & (p == 1) for l, p in zip(_labels, _preds)])\n",
    "        FP = sum([(l == 0) & (p == 1) for l, p in zip(_labels, _preds)])\n",
    "        FN = sum([(l == 1) & (p == 0) for l, p in zip(_labels, _preds)])\n",
    "        TN = sum([(l == 0) & (p == 0) for l, p in zip(_labels, _preds)])\n",
    "        acc = (TP + TN) / len(_preds)\n",
    "        prec = TP / (TP + FP)\n",
    "        rec = TP / (TP + FN)\n",
    "        f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "        \n",
    "        print(m)\n",
    "        print(f'Acc: {acc:.4f}')\n",
    "        print(f'Prec: {prec:.4f}')\n",
    "        print(f'Recall: {rec:.4f}')\n",
    "        print(f'F1: {f1:.4f}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch = 0 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d71e0f487d3472d99b4bfcbcf69801c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.2988\n",
      "Prec: 0.0142\n",
      "Recall: 0.8678\n",
      "F1: 0.0279\n",
      "\n",
      "lm\n",
      "Acc: 0.6393\n",
      "Prec: 0.0189\n",
      "Recall: 0.5928\n",
      "F1: 0.0367\n",
      "\n",
      "joint\n",
      "Acc: 0.6505\n",
      "Prec: 0.0214\n",
      "Recall: 0.6501\n",
      "F1: 0.0413\n",
      "\n",
      "Joint stats:\n",
      "correct pos 851\n",
      "correct neg 72583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 73434\n",
      "\n",
      "=== epoch = 1 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560012b95a764ad3838bc72eb6702552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3102\n",
      "Prec: 0.0143\n",
      "Recall: 0.8587\n",
      "F1: 0.0281\n",
      "\n",
      "lm\n",
      "Acc: 0.6701\n",
      "Prec: 0.0194\n",
      "Recall: 0.5539\n",
      "F1: 0.0375\n",
      "\n",
      "joint\n",
      "Acc: 0.6753\n",
      "Prec: 0.0223\n",
      "Recall: 0.6310\n",
      "F1: 0.0431\n",
      "\n",
      "Joint stats:\n",
      "correct pos 826\n",
      "correct neg 75411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 76237\n",
      "\n",
      "=== epoch = 2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d12c02ceefc4a9f99bcc6be4002ee56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3110\n",
      "Prec: 0.0143\n",
      "Recall: 0.8571\n",
      "F1: 0.0280\n",
      "\n",
      "lm\n",
      "Acc: 0.7186\n",
      "Prec: 0.0205\n",
      "Recall: 0.4973\n",
      "F1: 0.0394\n",
      "\n",
      "joint\n",
      "Acc: 0.7111\n",
      "Prec: 0.0234\n",
      "Recall: 0.5882\n",
      "F1: 0.0451\n",
      "\n",
      "Joint stats:\n",
      "correct pos 770\n",
      "correct neg 79510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 80280\n",
      "\n",
      "=== epoch = 3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee94554ef9c4d8eaff900f55da86b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3114\n",
      "Prec: 0.0143\n",
      "Recall: 0.8571\n",
      "F1: 0.0281\n",
      "\n",
      "lm\n",
      "Acc: 0.7296\n",
      "Prec: 0.0208\n",
      "Recall: 0.4851\n",
      "F1: 0.0399\n",
      "\n",
      "joint\n",
      "Acc: 0.7192\n",
      "Prec: 0.0237\n",
      "Recall: 0.5775\n",
      "F1: 0.0455\n",
      "\n",
      "Joint stats:\n",
      "correct pos 756\n",
      "correct neg 80442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 81198\n",
      "\n",
      "=== epoch = 4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58143dbf7cd1479baf823b7827ce494f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3118\n",
      "Prec: 0.0143\n",
      "Recall: 0.8571\n",
      "F1: 0.0281\n",
      "\n",
      "lm\n",
      "Acc: 0.7246\n",
      "Prec: 0.0208\n",
      "Recall: 0.4927\n",
      "F1: 0.0398\n",
      "\n",
      "joint\n",
      "Acc: 0.7152\n",
      "Prec: 0.0235\n",
      "Recall: 0.5821\n",
      "F1: 0.0452\n",
      "\n",
      "Joint stats:\n",
      "correct pos 762\n",
      "correct neg 79978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 80740\n",
      "\n",
      "=== epoch = 5 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58313da772c4525a50b8cd95c153af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3121\n",
      "Prec: 0.0143\n",
      "Recall: 0.8571\n",
      "F1: 0.0281\n",
      "\n",
      "lm\n",
      "Acc: 0.7793\n",
      "Prec: 0.0216\n",
      "Recall: 0.4072\n",
      "F1: 0.0410\n",
      "\n",
      "joint\n",
      "Acc: 0.7588\n",
      "Prec: 0.0250\n",
      "Recall: 0.5202\n",
      "F1: 0.0476\n",
      "\n",
      "Joint stats:\n",
      "correct pos 681\n",
      "correct neg 84980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 85661\n",
      "\n",
      "=== epoch = 6 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9329d5cc99e045e2a23c9f80fcde3a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3128\n",
      "Prec: 0.0143\n",
      "Recall: 0.8571\n",
      "F1: 0.0281\n",
      "\n",
      "lm\n",
      "Acc: 0.8176\n",
      "Prec: 0.0216\n",
      "Recall: 0.3331\n",
      "F1: 0.0406\n",
      "\n",
      "joint\n",
      "Acc: 0.7925\n",
      "Prec: 0.0259\n",
      "Recall: 0.4622\n",
      "F1: 0.0491\n",
      "\n",
      "Joint stats:\n",
      "correct pos 605\n",
      "correct neg 88870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 89475\n",
      "\n",
      "=== epoch = 8 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5e7cbfc2c1471784e50ad08efe3866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb\n",
      "Acc: 0.3135\n",
      "Prec: 0.0143\n",
      "Recall: 0.8571\n",
      "F1: 0.0281\n",
      "\n",
      "lm\n",
      "Acc: 0.8212\n",
      "Prec: 0.0218\n",
      "Recall: 0.3285\n",
      "F1: 0.0409\n",
      "\n",
      "joint\n",
      "Acc: 0.7954\n",
      "Prec: 0.0261\n",
      "Recall: 0.4591\n",
      "F1: 0.0495\n",
      "\n",
      "Joint stats:\n",
      "correct pos 601\n",
      "correct neg 89197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct all 89798\n",
      "\n",
      "=== epoch = 9 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52feff58f3a540788a8bb70b25b42e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Predicting', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ep in range(10):\n",
    "    print(f'=== epoch = {_ep} ===')\n",
    "    _ckpt_path = glob(f'lightning_logs/default/14.2/checkpoints/epoch={_ep}-*')[0]\n",
    "    _ee_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_cotraining_14.2_ep={_ep}.csv')\n",
    "\n",
    "    eval_pretrained_ckpt(ckpt_path=_ckpt_path,\n",
    "                         test_loader=indeed_test_loader_2,\n",
    "                         ee_pred_out_path=_ee_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'lightning_logs/default/version_14/checkpoints/epoch=999-step=385999.ckpt'\n",
    "# hparams_file = 'lightning_logs/version_8/hparams.yaml'\n",
    "ee_clf_trained = EE_Classifier.load_from_checkpoint(ckpt_path, embeddings_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_clf_trained(wiki_test_set_2[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds = trainer.predict(ee_clf_trained, dataloaders=wiki_test_loader)\n",
    "preds = pd.read_csv(os.path.join(base_dir, f'{wiki_data_dir}/ee_clf_test_v=14.csv')).to_dict('record')\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6992481203007519"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([d['joint_pred'] == d['label'] for d in preds]) / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept': 'background_screening', 'neighbor': 'case report', 'label': 1}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on indeed \n",
    "indeed_test_set_2.sample_records[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emb_logits': tensor(0., grad_fn=<SqueezeBackward1>),\n",
       " 'lm_logits': tensor(1.5716e-11, grad_fn=<SqueezeBackward1>),\n",
       " 'joint_logits': tensor(7.8581e-12, grad_fn=<MeanBackward1>)}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee_clf_trained(indeed_test_set_2[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_preds = trainer.predict(ee_clf_trained, dataloaders=indeed_test_loader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_pred_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_cotraining_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_pred_records = []\n",
    "for i in range(len(indeed_test_set)):\n",
    "    _raw_d = indeed_test_set.sample_records[i]\n",
    "    _pred_d = indeed_preds[i]\n",
    "    \n",
    "    indeed_pred_records.append({\n",
    "        'concept': _raw_d['concept'],\n",
    "        'neighbor': _raw_d['neighbor'],\n",
    "        'label': _raw_d['label'],\n",
    "        'emb_logits': _pred_d['emb_logits'][0],\n",
    "        'lm_logits': _pred_d['lm_logits'][0],\n",
    "        'joint_logits': _pred_d['joint_logits'][0],\n",
    "        'emb_pred': _pred_d['emb_pred'][0],\n",
    "        'lm_pred': _pred_d['lm_pred'][0],\n",
    "        'joint_pred': _pred_d['joint_pred'][0]\n",
    "    })\n",
    "pd.DataFrame(indeed_pred_records).to_csv(indeed_pred_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28762755102040816\n",
      "0.5712691326530612\n",
      "0.5988786139455783\n"
     ]
    }
   ],
   "source": [
    "print(sum([d['emb_pred'][0] == d['label'][0] for d in indeed_preds]) / len(indeed_preds))\n",
    "print(sum([d['lm_pred'][0] == d['label'][0] for d in indeed_preds]) / len(indeed_preds))\n",
    "print(sum([d['joint_pred'][0] == d['label'][0] for d in indeed_preds]) / len(indeed_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32472"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([d['emb_pred'][0] == d['label'][0] for d in indeed_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n",
      "66697\n",
      "67611\n"
     ]
    }
   ],
   "source": [
    "_labels = [d['label'][0] for d in indeed_preds]\n",
    "_preds = [d['joint_pred'][0] for d in indeed_preds]\n",
    "print(sum([(l == 1) & (p == 1) for l, p in zip(_labels, _preds)]))\n",
    "print(sum([(l == 0) & (p == 0) for l, p in zip(_labels, _preds)]))\n",
    "print(sum([l == p for l, p in zip(_labels, _preds)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_clf_trained.views_clf_heads['lm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_set.sample_records[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s = wiki_test_set[400]\n",
    "_ev = _s['lm_ent']\n",
    "_cv = _s['lm_cc']\n",
    "_v = torch.cat([_ev, _cv, _ev * _cv, _ev - _cv]).unsqueeze(0)\n",
    "\n",
    "print(_v)\n",
    "for _layer in ee_clf_trained.views_clf_heads['lm']:\n",
    "    _v = _layer(_v)\n",
    "    print(_layer)\n",
    "    print(_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_set[20].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model.embeddings.word_embeddings.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_mlm_model.cls.predictions.decoder.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(bert_model.embeddings.word_embeddings.weight, bert_mlm_model.cls.predictions.decoder.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-clusters analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_LM_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8064"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "\n",
    "_entity_embeddings = load_embeddings(bert_emb_path, 768)\n",
    "_entity_emb_dict = dict(zip(_entity_embeddings['entity'].tolist(),\n",
    "                            _entity_embeddings['embedding'].tolist()))\n",
    "len(_entity_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8036"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_concept = 'benefits'\n",
    "ee_LM_df = pd.read_csv(ee_LM_path)\n",
    "ee_LM_list = ee_LM_df[ee_LM_df['concept'] == _concept]['neighbor'].tolist()\n",
    "len(ee_LM_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "_entities = ee_LM_list[:K]\n",
    "_feats_arr = [_entity_emb_dict[_e] for _e in _entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 20\n",
    "A_cls = AgglomerativeClustering(n_clusters=C, affinity='euclidean')\n",
    "_cls_res = A_cls.fit_predict(_feats_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(C):\n",
    "    print(f'CLUSTER {i}:')\n",
    "    print(' | '.join(_entities[j] for j in range(K) if _cls_res[j] == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bin_name(log_freq, base=2):\n",
    "    return f'{base**log_freq}~{base**(log_freq+1)-1}' if log_freq > 0 else '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeed entities in Wiki\n",
    "wiki_corpus_path = f'{wiki_data_dir}/sentences.json'\n",
    "indeed_corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences.json')\n",
    "indeed_embed_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8064"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(indeed_embed_num_path, 'r') as f:\n",
    "    indeed_ents = [l.strip().rsplit(' ', 1)[0] for l in f]\n",
    "len(indeed_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes_count': 51325,\n",
       " 'words_count': 8064,\n",
       " 'longest_word': 44,\n",
       " 'links_count': 51324,\n",
       " 'sizeof_node': 32,\n",
       " 'total_size': 2052992}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = ahocorasick.Automaton()\n",
    "for e in indeed_ents:\n",
    "    A.add_word(f' {e} ', e)\n",
    "A.make_automaton()\n",
    "A.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e03a6731a64bf89aea4c4180f46cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1180171.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ent_freqs_wiki = Counter()\n",
    "ent_sent_freqs_wiki = Counter()\n",
    "\n",
    "with open(wiki_corpus_path, 'r') as f:\n",
    "    for l in tqdm(f, total=1180171):\n",
    "        d = json.loads(l)\n",
    "        _l = ' ' + ' '.join(d['tokens']).lower() + ' '\n",
    "        A_matches = [e for _pos, e in A.iter(_l)]\n",
    "        ent_freqs_wiki.update(A_matches)\n",
    "        ent_sent_freqs_wiki.update(set(A_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6368, 6368)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_freqs_wiki), len(ent_sent_freqs_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_records_wiki = []\n",
    "for _e, _f in ent_freqs_wiki.most_common():\n",
    "    _sf = ent_sent_freqs_wiki[_e]\n",
    "    freq_records_wiki.append({\n",
    "        'entity': _e,\n",
    "        'freq': _f,\n",
    "        'sent_freq': _sf\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_records_wiki_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ent_freqs_in_wiki.csv')\n",
    "pd.DataFrame(freq_records_wiki).to_csv(freq_records_wiki_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bins_wiki = Counter()\n",
    "for _e, _f in ent_freqs_wiki.most_common():\n",
    "    _exp = np.floor(np.log2(_f)).astype(int)\n",
    "    _bin_name = f'{2**_exp}~{2**(_exp+1)-1}' if _exp > 0 else '1'\n",
    "    freq_bins_wiki[_bin_name] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'16384~32767': 8,\n",
       "         '8192~16383': 28,\n",
       "         '4096~8191': 70,\n",
       "         '2048~4095': 134,\n",
       "         '1024~2047': 217,\n",
       "         '512~1023': 279,\n",
       "         '256~511': 399,\n",
       "         '128~255': 490,\n",
       "         '64~127': 494,\n",
       "         '32~63': 490,\n",
       "         '16~31': 472,\n",
       "         '8~15': 448,\n",
       "         '4~7': 492,\n",
       "         '2~3': 414,\n",
       "         '1': 421})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on indeed corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc49c7144074d6bb3ba924a1fb90a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=901796.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ent_freqs_indeed = Counter()\n",
    "ent_sent_freqs_indeed = Counter()\n",
    "\n",
    "with open(indeed_corpus_path, 'r') as f:\n",
    "    for l in tqdm(f, total=901796):\n",
    "        d = json.loads(l)\n",
    "        _l = ' ' + ' '.join(d['tokens']).lower() + ' '\n",
    "        A_matches = [e for _pos, e in A.iter(_l)]\n",
    "        ent_freqs_indeed.update(A_matches)\n",
    "        ent_sent_freqs_indeed.update(set(A_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8064, 8064)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_freqs_indeed), len(ent_sent_freqs_indeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_records_indeed = []\n",
    "for _e, _f in ent_freqs_indeed.most_common():\n",
    "    _sf = ent_sent_freqs_indeed[_e]\n",
    "    freq_records_indeed.append({\n",
    "        'entity': _e,\n",
    "        'freq': _f,\n",
    "        'sent_freq': _sf\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_records_indeed_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ent_freqs_in_indeed.csv')\n",
    "pd.DataFrame(freq_records_indeed).to_csv(freq_records_indeed_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bins_indeed = Counter()\n",
    "for _e, _f in ent_freqs_indeed.most_common():\n",
    "    _exp = np.floor(np.log2(_f)).astype(int)\n",
    "    _bin_name = f'{2**_exp}~{2**(_exp+1)-1}' if _exp > 0 else '1'\n",
    "    freq_bins_indeed[_bin_name] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'32768~65535': 1,\n",
       "         '16384~32767': 3,\n",
       "         '8192~16383': 6,\n",
       "         '4096~8191': 16,\n",
       "         '2048~4095': 44,\n",
       "         '1024~2047': 67,\n",
       "         '512~1023': 118,\n",
       "         '256~511': 182,\n",
       "         '128~255': 326,\n",
       "         '64~127': 444,\n",
       "         '32~63': 787,\n",
       "         '16~31': 1172,\n",
       "         '8~15': 788,\n",
       "         '4~7': 1385,\n",
       "         '2~3': 2359,\n",
       "         '1': 366})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bins_indeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy on subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3423, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ee_label_file = os.path.join(base_dir, f'data/indeed-benchmark/ee-increment-labels-2.csv')\n",
    "\n",
    "raw_ee_df = pd.read_csv(raw_ee_label_file).dropna()\n",
    "raw_ee_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['concept', 'neighbor', 'label-Nikita', 'label-Sajjadur', 'label-Yutong',\n",
       "       'Disagree', 'Majority'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ee_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_instances_dict = dict(zip(seed_concepts_df['alignedCategoryName'],\n",
    "                               seed_concepts_df['seedInstances']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_k_dict = Counter()\n",
    "for d in raw_ee_df.to_dict('record'):\n",
    "    cc = d['concept']\n",
    "    if d['neighbor'] not in seed_instances_dict[cc] and d['Majority'] == 1:\n",
    "        gold_k_dict[cc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_pred_paths = {\n",
    "    'emb_contr': os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None-2.csv'),\n",
    "    'lm': os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None-2.csv'),\n",
    "    'mrr': os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_mrr_combine_k=None-2.csv'),\n",
    "}\n",
    "\n",
    "method_preds = dict()\n",
    "for m, p in method_pred_paths.items():\n",
    "    _df = pd.read_csv(p)\n",
    "    _cc_dict = dict()\n",
    "    for cc, _seeds in seed_instances_dict.items():\n",
    "        _sub_df = _df[_df['concept'] == cc]\n",
    "        _preds = [e for e in _sub_df['neighbor'] if e not in _seeds]\n",
    "        _preds = _preds[:gold_k_dict[cc]]\n",
    "        _cc_dict[cc] = _preds\n",
    "    method_preds[m] = _cc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to csv\n",
    "\n",
    "_output_csv = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee-dim-analysis.csv')\n",
    "\n",
    "_output_records = []\n",
    "for raw_d in raw_ee_df.to_dict('record'):\n",
    "    d = dict(raw_d)\n",
    "    cc = d['concept']\n",
    "    e = d['neighbor']\n",
    "    if e in seed_instances_dict[cc]:\n",
    "        continue\n",
    "    \n",
    "    d['votes'] = np.round(d['label-Nikita'] + d['label-Sajjadur'] + d['label-Yutong']).astype(int)\n",
    "    \n",
    "    _freq_wiki = ent_sent_freqs_wiki[e]\n",
    "    _bin_wiki = np.floor(np.log2(_freq_wiki + 0.9)).astype(int) + 1\n",
    "    _freq_indeed = ent_sent_freqs_indeed[e]\n",
    "    _bin_indeed = np.floor(np.log2(_freq_indeed + 0.9)).astype(int) + 1\n",
    "    ## these are log(freq) + 1\n",
    "    d['log_freq_wiki'] = _bin_wiki\n",
    "    d['log_freq_indeed'] = _bin_indeed\n",
    "\n",
    "    for _m, _cc_preds in method_preds.items():\n",
    "        d[f'{_m}_pred'] = int(e in _cc_preds[cc])\n",
    "    \n",
    "    _output_records.append(d)\n",
    "\n",
    "pd.DataFrame(_output_records).to_csv(_output_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['concept', 'neighbor', 'label-Nikita', 'label-Sajjadur', 'label-Yutong',\n",
       "       'Disagree', 'Majority', 'votes', 'log_freq_wiki', 'log_freq_indeed',\n",
       "       'emb_contr_pred', 'lm_pred', 'mrr_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_info_df = pd.read_csv(_output_csv)\n",
    "joint_info_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by agreement \n",
    "\n",
    "by_agreement_records = []\n",
    "\n",
    "for _agree in [0, 1]:\n",
    "    _agree_df = joint_info_df[joint_info_df['Disagree'] == 1 - _agree]\n",
    "    print(f'Agree = {_agree} samples:')\n",
    "    for m in method_pred_paths.keys():\n",
    "        print(f'{m}:')\n",
    "        for cc in seed_instances_dict.keys():\n",
    "            _cc_df = _agree_df[_agree_df['concept'] == cc]\n",
    "            _corr = (_cc_df['Majority'] * _cc_df[f'{m}_pred']).sum()\n",
    "            _all_pred = _cc_df[f'{m}_pred'].sum()\n",
    "            _all_corr = _cc_df['Majority'].sum()\n",
    "            _P = _corr / _all_pred\n",
    "            _R = _corr / _all_corr\n",
    "            _F1 = 2 * _P * _R / (_P + _R + 1e-9)\n",
    "            print(f'{cc:<20s} >> P = {_P:.4f} | R = {_R:.4f} | F1 = {_F1:.4f}')\n",
    "            print(f'{\"\":<20s} >> corr = {_corr} | all_p = {_all_pred} | all_r = {_all_corr}')\n",
    "            by_agreement_records.append({\n",
    "                'agreement': _agree,\n",
    "                'method': m,\n",
    "                'concept': cc,\n",
    "                'P': _P,\n",
    "                'R': _R,\n",
    "                'F1': _F1,\n",
    "                'corr': _corr,\n",
    "                'all_p': _all_pred,\n",
    "                'all_r': _all_corr,\n",
    "            })\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "_agreement_csv = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee-agreement-analysis.csv')\n",
    "pd.DataFrame(by_agreement_records).to_csv(_agreement_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by indeed frequency \n",
    "\n",
    "by_indeed_freq_records = []\n",
    "\n",
    "for _freq_bin in sorted(joint_info_df['log_freq_indeed'].drop_duplicates().to_list()):\n",
    "    _freq_df = joint_info_df[joint_info_df['log_freq_indeed'] == _freq_bin]\n",
    "    print(f'Freq bin {_freq_bin}:')\n",
    "    for m in method_pred_paths.keys():\n",
    "        print(f'{m}:')\n",
    "        for cc in seed_instances_dict.keys():\n",
    "            _cc_df = _freq_df[_freq_df['concept'] == cc]\n",
    "            _corr = (_cc_df['Majority'] * _cc_df[f'{m}_pred']).sum()\n",
    "            _all_pred = _cc_df[f'{m}_pred'].sum()\n",
    "            _all_corr = _cc_df['Majority'].sum()\n",
    "            _P = _corr / _all_pred\n",
    "            _R = _corr / _all_corr\n",
    "            _F1 = 2 * _P * _R / (_P + _R + 1e-9)\n",
    "            print(f'{cc:<20s} >> P = {_P:.4f} | R = {_R:.4f} | F1 = {_F1:.4f}')\n",
    "            print(f'{\"\":<20s} >> corr = {_corr} | all_p = {_all_pred} | all_r = {_all_corr}')\n",
    "            by_indeed_freq_records.append({\n",
    "                'freq_bin': _freq_bin,\n",
    "                'method': m,\n",
    "                'concept': cc,\n",
    "                'P': _P,\n",
    "                'R': _R,\n",
    "                'F1': _F1,\n",
    "                'corr': _corr,\n",
    "                'all_p': _all_pred,\n",
    "                'all_r': _all_corr,\n",
    "            })\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "_freq_indeed_csv = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee-freq-indeed-analysis.csv')\n",
    "pd.DataFrame(by_indeed_freq_records).to_csv(_freq_indeed_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by wiki frequency \n",
    "\n",
    "by_wiki_freq_records = []\n",
    "\n",
    "for _freq_bin in sorted(joint_info_df['log_freq_wiki'].drop_duplicates().to_list()):\n",
    "    _freq_df = joint_info_df[joint_info_df['log_freq_wiki'] == _freq_bin]\n",
    "    print(f'Freq bin {_freq_bin}:')\n",
    "    for m in method_pred_paths.keys():\n",
    "        print(f'{m}:')\n",
    "        for cc in seed_instances_dict.keys():\n",
    "            _cc_df = _freq_df[_freq_df['concept'] == cc]\n",
    "            _corr = (_cc_df['Majority'] * _cc_df[f'{m}_pred']).sum()\n",
    "            _all_pred = _cc_df[f'{m}_pred'].sum()\n",
    "            _all_corr = _cc_df['Majority'].sum()\n",
    "            _P = _corr / _all_pred\n",
    "            _R = _corr / _all_corr\n",
    "            _F1 = 2 * _P * _R / (_P + _R + 1e-9)\n",
    "            print(f'{cc:<20s} >> P = {_P:.4f} | R = {_R:.4f} | F1 = {_F1:.4f}')\n",
    "            print(f'{\"\":<20s} >> corr = {_corr} | all_p = {_all_pred} | all_r = {_all_corr}')\n",
    "            by_wiki_freq_records.append({\n",
    "                'freq_bin': _freq_bin,\n",
    "                'method': m,\n",
    "                'concept': cc,\n",
    "                'P': _P,\n",
    "                'R': _R,\n",
    "                'F1': _F1,\n",
    "                'corr': _corr,\n",
    "                'all_p': _all_pred,\n",
    "                'all_r': _all_corr,\n",
    "            })\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "_freq_wiki_csv = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee-freq-wiki-analysis.csv')\n",
    "pd.DataFrame(by_wiki_freq_records).to_csv(_freq_wiki_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair2votes = dict()\n",
    "\n",
    "# for cc in raw_ee_df['concept'].drop_duplicate().to_list():\n",
    "for cc in ['job_position']:\n",
    "    _sub_df = raw_ee_df[raw_ee_df['concept'] == cc]\n",
    "    for d in _sub_df.to_dict('record'):\n",
    "        e = d['neighbor']\n",
    "        maj = d['Majority']\n",
    "        votes = d['label-Nikita'] + d['label-Sajjadur'] + d['label-Yutong']\n",
    "        pair2votes[(cc, e)] = np.round(votes).astype(int)\n",
    "\n",
    "pair2votes[('job_position', 'field tech')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_freq_stats = defaultdict(Counter)\n",
    "indeed_freq_stats = defaultdict(Counter)\n",
    "freq_records = []\n",
    "\n",
    "for (cc, e), votes in pair2votes.items():\n",
    "    _freq_wiki = ent_sent_freqs_wiki[e]\n",
    "    if _freq_wiki == 0: continue\n",
    "    _bin_wiki = np.floor(np.log2(_freq_wiki)).astype(int)\n",
    "    _bin_name_wiki = _get_bin_name(_bin_wiki)\n",
    "    _freq_indeed = ent_sent_freqs_indeed[e]\n",
    "    _bin_indeed = np.floor(np.log2(_freq_indeed)).astype(int)\n",
    "    _bin_name_indeed = _get_bin_name(_bin_indeed)\n",
    "    # print(cc, e, (_freq_wiki, _bin_wiki, _bin_name_wiki), (_freq_indeed, _bin_indeed, _bin_name_indeed))\n",
    "    wiki_freq_stats[_bin_name_wiki][votes] += 1\n",
    "    indeed_freq_stats[_bin_name_indeed][votes] += 1\n",
    "    freq_records.append({\n",
    "        'concept': cc,\n",
    "        'neighbor': e,\n",
    "        # ?\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_freq_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_freq_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE labels preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract labeled entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_EE_label_file = os.path.join(base_dir, f'data/indeed-benchmark/ee-increment-labels-2.csv')\n",
    "out_EE_label_file = os.path.join(base_dir, f'data/indeed-benchmark/ee-labels-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ee_df = pd.read_csv(in_EE_label_file)\n",
    "\n",
    "gold_labels_dict = dict()\n",
    "for d in in_ee_df.to_dict('records'):\n",
    "    if d['Majority'] != 1:\n",
    "        continue\n",
    "    _cc = d['concept']\n",
    "    _e = d['neighbor']\n",
    "    gold_labels_dict[_cc] = gold_labels_dict.get(_cc, []) + [_e]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company: 155\n",
      "dress_code: 268\n",
      "job_position: 152\n",
      "pay_schedule: 37\n",
      "benefits: 116\n",
      "compensation: 47\n",
      "payment_option: 30\n",
      "background_screening: 104\n",
      "person: 67\n",
      "hire_prerequisite: 137\n",
      "shifts: 89\n",
      "schedule: 69\n",
      "employee_type: 30\n",
      "onboarding_steps: 30\n"
     ]
    }
   ],
   "source": [
    "for _cc, _e_list in gold_labels_dict.items():\n",
    "    print(f'{_cc}: {len(_e_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels_list = []\n",
    "for _cc, _e_list in gold_labels_dict.items():\n",
    "    for _e in _e_list:\n",
    "        gold_labels_list.append((_cc, _e))\n",
    "pd.DataFrame(gold_labels_list, columns=['concept', 'neighbor']).to_csv(out_EE_label_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increment label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_incre_label_path = os.path.join(base_dir, f'data/indeed-benchmark/ee-increment-labels-2.csv')\n",
    "ee_pred_paths = [\n",
    "    os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_knn_roberta_k=None.csv'),\n",
    "    os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_roberta_k=None.csv'),\n",
    "    os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_roberta_k=None.csv'),\n",
    "    os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_roberta_contr_k=None.csv'),\n",
    "]\n",
    "# ee_pred_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn-aux-k=None.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-check and dedup current label file \n",
    "\n",
    "ee_label_df = pd.read_csv(ee_incre_label_path)\n",
    "ee_label_dict = dict()  # Dict[(cc, e), maj]\n",
    "for _d in ee_label_df.to_dict('record'):\n",
    "    _key = (_d['concept'], _d['neighbor'])\n",
    "    _label = ee_label_dict.get(_key, None)\n",
    "    if _label is None:\n",
    "        ee_label_dict[_key] = _d['Majority']\n",
    "    elif _label == 'ERR':\n",
    "        pass\n",
    "    else:\n",
    "        if _label != _d['Majority']:\n",
    "            print(f\"Contradict: {_key}\")\n",
    "            ee_label_dict[_key] = 'ERR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3435"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ee_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 0,\n",
       " 'dress_code': 1,\n",
       " 'job_position': 2,\n",
       " 'pay_schedule': 3,\n",
       " 'benefits': 4,\n",
       " 'compensation': 5,\n",
       " 'payment_option': 6,\n",
       " 'background_screening': 7,\n",
       " 'person': 8,\n",
       " 'hire_prerequisite': 9,\n",
       " 'shifts': 10,\n",
       " 'schedule': 11,\n",
       " 'employee_type': 12,\n",
       " 'onboarding_steps': 13}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_concepts = ee_label_df['concept'].drop_duplicates(keep='first').tolist()\n",
    "_concepts_order_dict = dict([(_concepts[i], i) for i in range(len(_concepts))])\n",
    "_concepts_order_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5620"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 100\n",
    "\n",
    "for ee_pred_path in ee_pred_paths:\n",
    "    ee_new_df = pd.read_csv(ee_pred_path)\n",
    "\n",
    "    for _cc in _concepts:\n",
    "        _new_ents = ee_new_df[ee_new_df['concept'] == _cc]['neighbor'].tolist()[:K]\n",
    "        for _e in _new_ents:\n",
    "            _key = (_cc, _e)\n",
    "            if _key in ee_label_dict:\n",
    "                continue\n",
    "            ee_label_dict[_key] = None\n",
    "\n",
    "len(ee_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'concept': 'company',\n",
       "   'neighbor': 'wal mart',\n",
       "   'label-Nikita': 1,\n",
       "   'label-Sajjadur': 1.0,\n",
       "   'label-Yutong': 1,\n",
       "   'Disagree': 0,\n",
       "   'Majority': 1},\n",
       "  {'concept': 'company',\n",
       "   'neighbor': 'walmart',\n",
       "   'label-Nikita': 1,\n",
       "   'label-Sajjadur': 1.0,\n",
       "   'label-Yutong': 1,\n",
       "   'Disagree': 0,\n",
       "   'Majority': 1},\n",
       "  {'concept': 'company',\n",
       "   'neighbor': 'costco',\n",
       "   'label-Nikita': 1,\n",
       "   'label-Sajjadur': 1.0,\n",
       "   'label-Yutong': 1,\n",
       "   'Disagree': 0,\n",
       "   'Majority': 1}],\n",
       " [{'concept': 'onboarding_steps',\n",
       "   'neighbor': 'changeup',\n",
       "   'label-Nikita': nan,\n",
       "   'label-Sajjadur': nan,\n",
       "   'label-Yutong': nan,\n",
       "   'Disagree': nan,\n",
       "   'Majority': nan},\n",
       "  {'concept': 'onboarding_steps',\n",
       "   'neighbor': 'personal issues',\n",
       "   'label-Nikita': nan,\n",
       "   'label-Sajjadur': nan,\n",
       "   'label-Yutong': nan,\n",
       "   'Disagree': nan,\n",
       "   'Majority': nan},\n",
       "  {'concept': 'onboarding_steps',\n",
       "   'neighbor': 'basic information',\n",
       "   'label-Nikita': nan,\n",
       "   'label-Sajjadur': nan,\n",
       "   'label-Yutong': nan,\n",
       "   'Disagree': nan,\n",
       "   'Majority': nan}])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out_records = []\n",
    "_added_keys = set()\n",
    "\n",
    "# remove existing errors \n",
    "for _d in ee_label_df.to_dict('record'):\n",
    "    _cc = _d['concept']\n",
    "    _e = _d['neighbor']\n",
    "    if (_cc, _e) in _added_keys:\n",
    "        continue\n",
    "        \n",
    "    if ee_label_dict[(_cc, _e)] == 'ERR':\n",
    "        _d['label-Nikita'] = _d['label-Sajjadur'] = _d['label-Yutong'] = \\\n",
    "        _d['Disagree'] = _d['Majority'] = np.nan\n",
    "    _out_records.append(_d)   \n",
    "    _added_keys.add((_cc, _e))\n",
    "\n",
    "for (_cc, _e), _label in ee_label_dict.items():\n",
    "    if (_cc, _e) in _added_keys:\n",
    "        continue\n",
    "        \n",
    "    _out_records.append({\n",
    "        'concept': _cc,\n",
    "        'neighbor': _e,\n",
    "        'label-Nikita': np.nan,\n",
    "        'label-Sajjadur': np.nan,\n",
    "        'label-Yutong': np.nan,\n",
    "        'Disagree': np.nan,\n",
    "        'Majority': np.nan\n",
    "    })\n",
    "    _added_keys.add((_cc, _e))\n",
    "\n",
    "_out_records.sort(key=lambda d : (np.isnan(d['Majority']), _concepts_order_dict[d['concept']]))\n",
    "_out_records[:3], _out_records[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee_incre_new_path: the new unlabeled file \n",
    "ee_incre_new_path = os.path.join(base_dir, f'data/indeed-benchmark/ee-increment-labels-NEW3.csv')\n",
    "pd.DataFrame(_out_records).to_csv(ee_incre_new_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "EE_label_file = os.path.join(base_dir, f'data/indeed-benchmark/ee-labels-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv(EE_label_file)\n",
    "_e_dict = dict([(_cc, _df[_df['concept'] == _cc]['neighbor'].tolist())\n",
    "                    for _cc in set(_df['concept'].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 47, 2, {'stock options', 'unemployment compensation'})"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ints = set(_e_dict['benefits']) & set(_e_dict['compensation'])\n",
    "len(_e_dict['benefits']), len(_e_dict['compensation']), len(_ints), _ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_overlap_ccs = [\n",
    "    ('benefits', 'compensation'),\n",
    "    ('hire_prerequisite', 'background_screening'),\n",
    "    ('hire_prerequisite', 'person'),\n",
    "    ('person', 'background_screening'),\n",
    "    ('schedule', 'pay_schedule'),\n",
    "    ('schedule', 'shifts'),\n",
    "    ('shifts', 'pay_schedule'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _c1, _c2 in _overlap_ccs:\n",
    "    _ints = sorted(list(set(_e_dict[_c1]) & set(_e_dict[_c2])))\n",
    "    print(f'{_c1} -- {_c2}')\n",
    "    print(f'{_c1}: {len(_e_dict[_c1])} ')\n",
    "    print(f'{_c2}: {len(_e_dict[_c2])} ')\n",
    "    print(f'Intersection: {_ints} {len(_ints)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "_e2cc = dict([(_e, tuple(sorted(_df[_df['neighbor'] == _e]['concept'].tolist())))\n",
    "                  for _e in set(_df['neighbor'].tolist())])\n",
    "\n",
    "_ccs2e = dict([(_ccs, [_e for _e, _l in _e2cc.items() if _l == _ccs]) for _ccs in sorted(_e2cc.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts: background_screening | benefits\n",
      "Intersection: ['college education', 'medical diagnosis', 'unemployment', 'medical marijuana card', 'medical marijuana', 'health'] 6\n",
      "\n",
      "Concepts: background_screening | benefits | hire_prerequisite\n",
      "Intersection: ['education'] 1\n",
      "\n",
      "Concepts: background_screening | benefits | hire_prerequisite | payment_option\n",
      "Intersection: ['medical card'] 1\n",
      "\n",
      "Concepts: background_screening | hire_prerequisite\n",
      "Intersection: ['social security number', 'driver license', 'social security card', 'marriage license', 'employment contract', 'medical test', 'passport', 'citizenship', 'security card', 'physical exam', 'marriage certificate'] 11\n",
      "\n",
      "Concepts: benefits | compensation\n",
      "Intersection: ['unemployment compensation', 'stock options'] 2\n",
      "\n",
      "Concepts: benefits | dress_code\n",
      "Intersection: ['hair'] 1\n",
      "\n",
      "Concepts: benefits | hire_prerequisite\n",
      "Intersection: ['car insurance', 'high school education', 'health insurance', 'dental insurance', 'insurance', 'disability insurance', 'physical disability', 'vehicle insurance', 'property insurance', 'medical insurance', 'health care insurance'] 11\n",
      "\n",
      "Concepts: benefits | hire_prerequisite | onboarding_steps\n",
      "Intersection: ['cross training'] 1\n",
      "\n",
      "Concepts: benefits | job_position\n",
      "Intersection: ['dentist'] 1\n",
      "\n",
      "Concepts: benefits | person\n",
      "Intersection: ['autism'] 1\n",
      "\n",
      "Concepts: company | dress_code\n",
      "Intersection: ['nike'] 1\n",
      "\n",
      "Concepts: company | job_position\n",
      "Intersection: ['rca', 'gm'] 2\n",
      "\n",
      "Concepts: company | schedule\n",
      "Intersection: ['sun'] 1\n",
      "\n",
      "Concepts: compensation | payment_option\n",
      "Intersection: ['cash money'] 1\n",
      "\n",
      "Concepts: dress_code | hire_prerequisite\n",
      "Intersection: ['hygiene'] 1\n",
      "\n",
      "Concepts: employee_type | hire_prerequisite\n",
      "Intersection: ['apprenticeship', 'internship', 'contract'] 3\n",
      "\n",
      "Concepts: hire_prerequisite | job_position\n",
      "Intersection: ['engineer'] 1\n",
      "\n",
      "Concepts: hire_prerequisite | onboarding_steps\n",
      "Intersection: ['training class', 'formal training', 'training classes', 'computer based training', 'training program', 'classroom training'] 6\n",
      "\n",
      "Concepts: pay_schedule | schedule\n",
      "Intersection: ['saturday and sunday', 'bi / weekly', 'weekly', 'saturday / sunday', 'every other friday', 'calendar year'] 6\n",
      "\n",
      "Concepts: pay_schedule | schedule | shifts\n",
      "Intersection: ['thursday morning', 'monday sunday', 'monday friday', 'monday thursday', 'saturday sunday', 'monday morning', 'sunday thursday', 'saturday morning', 'thursday night', 'friday saturday', 'weekend', 'sunday saturday', 'friday / saturday', 'daily', 'friday', 'sunday', 'friday morning', 'monday mornings', 'monday saturday', 'saturday night', 'monday night'] 21\n",
      "\n",
      "Concepts: schedule | shifts\n",
      "Intersection: ['morning afternoon', 'late nights', 'early morning', 'late night', '7 am 3 pm', '8 am 2 pm', '7 am 5 pm', '10 pm 7 am', '5 am 2 pm', '3rd shift', 'ten hour shifts', 'early mornings', '6 am 2 pm', '3 pm 10 pm', 'saturday', '8 am to 4 pm'] 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ccs, _e_list in _ccs2e.items():\n",
    "    if len(_ccs) > 1:\n",
    "        print('Concepts:', ' | '.join(_ccs))\n",
    "        print(f'Intersection: {_e_list} {len(_e_list)}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_overlap_dict = defaultdict(set)\n",
    "for _ccs in _ccs2e.keys():\n",
    "    for i in range(len(_ccs)):\n",
    "        for j in range(i+1, len(_ccs)):\n",
    "            _c1 = _ccs[i]\n",
    "            _c2 = _ccs[j]\n",
    "            cc_overlap_dict[_c1].add(_c2)\n",
    "            cc_overlap_dict[_c2].add(_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_overlap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _e, _cc_list in _e2cc.items():\n",
    "    if tuple(sorted(_cc_list)) == ('shifts', 'shifts'):\n",
    "        print(_e, _cc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _e, _cc_list in _e2cc.items():\n",
    "    if len(_cc_list) >= 3:\n",
    "        print(_e, _cc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity expansion evaluation\n",
    "Now using benchmark entities, mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_concept_knn_k=None-2.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_concept_knn_k=None-2_eval.csv \\\n",
    "-rank sim \\\n",
    "-rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_LM_bert_k=None-2.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_bert_k=None-2_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_mrr_combine-lm*3-k=None.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_mrr_combine-lm*3-k=None_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for COMB in ['avg', 'lm*3_avg', 'harmonic', 'min']:\n",
    "    !python eval_entities.py \\\n",
    "    -b $benchmark_dir \\\n",
    "    -pred $base_dir/data/$data_ac/intermediate/ee_calib-merged-k=None.csv \\\n",
    "    -o $base_dir/data/$data_ac/intermediate/ee_calib-merge=$COMB-k=None_eval.csv \\\n",
    "    -rank $COMB \\\n",
    "    -rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5533  0.5533  1.0000  0.1333  0.8300  0.5533 \n",
      "dress_code                8057    261    0.5977  0.5977  1.0000  0.0766  1.0000  0.3831 \n",
      "job_position              8057    145    0.0414  0.0414  0.1500  0.0207  0.0400  0.0276 \n",
      "pay_schedule              8060     34    0.1765  0.1765  0.3000  0.1765  0.0600  0.1765 \n",
      "benefits                  8058    110    0.3727  0.3727  1.0000  0.1818  0.4100  0.3727 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.8519  0.8519  1.0000  0.7407  0.2300  0.8519 \n",
      "background_screening      8061    102    0.3333  0.3333  1.0000  0.1961  0.3400  0.3333 \n",
      "person                    8055     58    0.0690  0.0690  0.2000  0.0690  0.0800  0.1379 \n",
      "hire_prerequisite         8059    133    0.4812  0.4812  1.0000  0.1504  0.6400  0.4812 \n",
      "shifts                    8060     86    0.7558  0.7558  1.0000  0.2326  0.6500  0.7558 \n",
      "schedule                  8057     62    0.4839  0.4839  1.0000  0.3226  0.3000  0.4839 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.2143  0.2143  0.3000  0.2143  0.0600  0.2143 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_cotraining_14.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_cotraining_14_eval.csv \\\n",
    "-rank joint_logits \\\n",
    "-rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5467  0.5467  1.0000  0.1333  0.8200  0.5467 \n",
      "dress_code                8057    261    0.4444  0.4444  1.0000  0.0766  1.0000  0.3831 \n",
      "job_position              8057    145    0.0414  0.0414  0.1000  0.0138  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3364  0.3364  1.0000  0.1818  0.3700  0.3364 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.8148  0.8148  1.0000  0.7407  0.2200  0.8148 \n",
      "background_screening      8061    102    0.2745  0.2745  1.0000  0.1961  0.2800  0.2745 \n",
      "person                    8055     58    0.0690  0.0690  0.2000  0.0690  0.0800  0.1379 \n",
      "hire_prerequisite         8059    133    0.3459  0.3459  1.0000  0.1504  0.4600  0.3459 \n",
      "shifts                    8060     86    0.7209  0.7209  1.0000  0.2326  0.6200  0.7209 \n",
      "schedule                  8057     62    0.4194  0.4194  1.0000  0.3226  0.2600  0.4194 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.1786  0.1786  0.2500  0.1786  0.0500  0.1786 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5333  0.5333  1.0000  0.1333  0.8000  0.5333 \n",
      "dress_code                8057    261    0.3525  0.3525  1.0000  0.0766  0.9200  0.3525 \n",
      "job_position              8057    145    0.0414  0.0414  0.0500  0.0069  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3273  0.3273  1.0000  0.1818  0.3600  0.3273 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.7407  0.7407  1.0000  0.7407  0.2000  0.7407 \n",
      "background_screening      8061    102    0.2451  0.2451  1.0000  0.1961  0.2500  0.2451 \n",
      "person                    8055     58    0.0862  0.0862  0.2000  0.0690  0.0700  0.1207 \n",
      "hire_prerequisite         8059    133    0.3233  0.3233  1.0000  0.1504  0.4300  0.3233 \n",
      "shifts                    8060     86    0.6744  0.6744  1.0000  0.2326  0.5800  0.6744 \n",
      "schedule                  8057     62    0.3871  0.3871  1.0000  0.3226  0.2400  0.3871 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.1429  0.1429  0.2000  0.1429  0.0400  0.1429 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5267  0.5267  1.0000  0.1333  0.7900  0.5267 \n",
      "dress_code                8057    261    0.2107  0.2107  1.0000  0.0766  0.5500  0.2107 \n",
      "job_position              8057    145    0.0414  0.0414  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3182  0.3182  1.0000  0.1818  0.3500  0.3182 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.7037  0.7037  0.9500  0.7037  0.1900  0.7037 \n",
      "background_screening      8061    102    0.2157  0.2157  1.0000  0.1961  0.2200  0.2157 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.0900  0.1552 \n",
      "hire_prerequisite         8059    133    0.2782  0.2782  1.0000  0.1504  0.3700  0.2782 \n",
      "shifts                    8060     86    0.4651  0.4651  1.0000  0.2326  0.4000  0.4651 \n",
      "schedule                  8057     62    0.3226  0.3226  1.0000  0.3226  0.2000  0.3226 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.0714  0.0714  0.1000  0.0714  0.0200  0.0714 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5267  0.5267  1.0000  0.1333  0.7900  0.5267 \n",
      "dress_code                8057    261    0.1877  0.1877  1.0000  0.0766  0.4900  0.1877 \n",
      "job_position              8057    145    0.0414  0.0414  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3182  0.3182  1.0000  0.1818  0.3500  0.3182 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.7037  0.7037  0.9500  0.7037  0.1900  0.7037 \n",
      "background_screening      8061    102    0.1961  0.1961  1.0000  0.1961  0.2000  0.1961 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.0900  0.1552 \n",
      "hire_prerequisite         8059    133    0.2556  0.2556  1.0000  0.1504  0.3400  0.2556 \n",
      "shifts                    8060     86    0.4302  0.4302  1.0000  0.2326  0.3700  0.4302 \n",
      "schedule                  8057     62    0.3065  0.3065  0.9500  0.3065  0.1900  0.3065 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.0714  0.0714  0.1000  0.0714  0.0200  0.0714 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5267  0.5267  1.0000  0.1333  0.7900  0.5267 \n",
      "dress_code                8057    261    0.1954  0.1954  1.0000  0.0766  0.5100  0.1954 \n",
      "job_position              8057    145    0.0414  0.0414  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3182  0.3182  1.0000  0.1818  0.3500  0.3182 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.7037  0.7037  0.9500  0.7037  0.1900  0.7037 \n",
      "background_screening      8061    102    0.2059  0.2059  1.0000  0.1961  0.2100  0.2059 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.0900  0.1552 \n",
      "hire_prerequisite         8059    133    0.2632  0.2632  1.0000  0.1504  0.3500  0.2632 \n",
      "shifts                    8060     86    0.4302  0.4302  1.0000  0.2326  0.3700  0.4302 \n",
      "schedule                  8057     62    0.3226  0.3226  1.0000  0.3226  0.2000  0.3226 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.0714  0.0714  0.1000  0.0714  0.0200  0.0714 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5133  0.5133  1.0000  0.1333  0.7700  0.5133 \n",
      "dress_code                8057    261    0.1188  0.1188  1.0000  0.0766  0.3100  0.1188 \n",
      "job_position              8057    145    0.0483  0.0483  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3000  0.3000  1.0000  0.1818  0.3300  0.3000 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.5556  0.5556  0.7500  0.5556  0.1500  0.5556 \n",
      "background_screening      8061    102    0.1569  0.1569  0.8000  0.1569  0.1600  0.1569 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.1100  0.1897 \n",
      "hire_prerequisite         8059    133    0.2105  0.2105  1.0000  0.1504  0.2800  0.2105 \n",
      "shifts                    8060     86    0.2558  0.2558  1.0000  0.2326  0.2200  0.2558 \n",
      "schedule                  8057     62    0.3065  0.3065  0.9500  0.3065  0.1900  0.3065 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.0714  0.0714  0.1000  0.0714  0.0200  0.0714 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5133  0.5133  1.0000  0.1333  0.7700  0.5133 \n",
      "dress_code                8057    261    0.1264  0.1264  1.0000  0.0766  0.3300  0.1264 \n",
      "job_position              8057    145    0.0414  0.0414  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.3182  0.3182  1.0000  0.1818  0.3500  0.3182 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.5926  0.5926  0.8000  0.5926  0.1600  0.5926 \n",
      "background_screening      8061    102    0.1667  0.1667  0.8500  0.1667  0.1700  0.1667 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.1100  0.1897 \n",
      "hire_prerequisite         8059    133    0.2180  0.2180  1.0000  0.1504  0.2900  0.2180 \n",
      "shifts                    8060     86    0.2674  0.2674  1.0000  0.2326  0.2300  0.2674 \n",
      "schedule                  8057     62    0.3065  0.3065  0.9500  0.3065  0.1900  0.3065 \n",
      "employee_type             8061     27    0.2593  0.2593  0.3500  0.2593  0.0700  0.2593 \n",
      "onboarding_steps          8059     28    0.0714  0.0714  0.1000  0.0714  0.0200  0.0714 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5067  0.5067  1.0000  0.1333  0.7600  0.5067 \n",
      "dress_code                8057    261    0.0805  0.0805  1.0000  0.0766  0.2100  0.0805 \n",
      "job_position              8057    145    0.1241  0.1241  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.2909  0.2909  1.0000  0.1818  0.3200  0.2909 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.4444  0.4444  0.6000  0.4444  0.1200  0.4444 \n",
      "background_screening      8061    102    0.1275  0.1275  0.6500  0.1275  0.1300  0.1275 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.1300  0.2241 \n",
      "hire_prerequisite         8059    133    0.1429  0.1429  0.9500  0.1429  0.1900  0.1429 \n",
      "shifts                    8060     86    0.1395  0.1395  0.6000  0.1395  0.1200  0.1395 \n",
      "schedule                  8057     62    0.2903  0.2903  0.9000  0.2903  0.1800  0.2903 \n",
      "employee_type             8061     27    0.2222  0.2222  0.3000  0.2222  0.0600  0.2222 \n",
      "onboarding_steps          8059     28    0.0357  0.0357  0.0500  0.0357  0.0100  0.0357 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5067  0.5067  1.0000  0.1333  0.7600  0.5067 \n",
      "dress_code                8057    261    0.0728  0.0728  0.9500  0.0728  0.1900  0.0728 \n",
      "job_position              8057    145    0.1241  0.1241  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.2818  0.2818  1.0000  0.1818  0.3100  0.2818 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.4074  0.4074  0.5500  0.4074  0.1100  0.4074 \n",
      "background_screening      8061    102    0.1176  0.1176  0.6000  0.1176  0.1200  0.1176 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.1300  0.2241 \n",
      "hire_prerequisite         8059    133    0.1353  0.1353  0.9000  0.1353  0.1800  0.1353 \n",
      "shifts                    8060     86    0.1279  0.1279  0.5500  0.1279  0.1100  0.1279 \n",
      "schedule                  8057     62    0.2903  0.2903  0.9000  0.2903  0.1800  0.2903 \n",
      "employee_type             8061     27    0.2222  0.2222  0.3000  0.2222  0.0600  0.2222 \n",
      "onboarding_steps          8059     28    0.0357  0.0357  0.0500  0.0357  0.0100  0.0357 \n",
      "\n",
      "--- Summary ---\n",
      "Concept                  Max K   Gold K   P@K     R@K     P@20    R@20   P@100   R@100  \n",
      "company                   8059    150    0.5067  0.5067  1.0000  0.1333  0.7600  0.5067 \n",
      "dress_code                8057    261    0.0766  0.0766  1.0000  0.0766  0.2000  0.0766 \n",
      "job_position              8057    145    0.1103  0.1103  0.0000  0.0000  0.0500  0.0345 \n",
      "pay_schedule              8060     34    0.0294  0.0294  0.0500  0.0294  0.0100  0.0294 \n",
      "benefits                  8058    110    0.2727  0.2727  1.0000  0.1818  0.3000  0.2727 \n",
      "compensation              8060     43    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000 \n",
      "payment_option            8061     27    0.4444  0.4444  0.6000  0.4444  0.1200  0.4444 \n",
      "background_screening      8061    102    0.1176  0.1176  0.6000  0.1176  0.1200  0.1176 \n",
      "person                    8055     58    0.1034  0.1034  0.2000  0.0690  0.1300  0.2241 \n",
      "hire_prerequisite         8059    133    0.1353  0.1353  0.9000  0.1353  0.1800  0.1353 \n",
      "shifts                    8060     86    0.1395  0.1395  0.6000  0.1395  0.1200  0.1395 \n",
      "schedule                  8057     62    0.2903  0.2903  0.9000  0.2903  0.1800  0.2903 \n",
      "employee_type             8061     27    0.2222  0.2222  0.3000  0.2222  0.0600  0.2222 \n",
      "onboarding_steps          8059     28    0.0357  0.0357  0.0500  0.0357  0.0100  0.0357 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(10):\n",
    "    !python eval_entities.py \\\n",
    "    -b $benchmark_dir \\\n",
    "    -pred $base_dir/data/$data_ac/intermediate/ee_cotraining_14.2_ep={ep}.csv \\\n",
    "    -o $base_dir/data/$data_ac/intermediate/ee_cotraining_14.2_ep={ep}_eval.csv \\\n",
    "    -rank joint_logits \\\n",
    "    -rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tidy_data_gold_k(filepath_dict):\n",
    "    record_ls = []\n",
    "    \n",
    "    for method, f in filepath_dict.items():\n",
    "        res_df = pd.read_csv(f)\n",
    "        for d in res_df.to_dict('records'):\n",
    "            record_ls.append({\n",
    "                'method': method,\n",
    "                'concept': d['concept'],\n",
    "                'acc': d['R@K']\n",
    "            })\n",
    "    df = pd.DataFrame(record_ls)\n",
    "    return df\n",
    "\n",
    "def pivot_df(df_, measure_y, dimension_x, group):\n",
    "    df_pivot = pd.pivot_table(\n",
    "        df_,\n",
    "        values=measure_y,\n",
    "        index=dimension_x,\n",
    "        columns=group,\n",
    "        aggfunc=np.mean\n",
    "    )\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_eval_filepath_dicts = {\n",
    "    'emb': f'{base_dir}/data/{data_ac}/intermediate/ee_concept_knn_k=None_eval.csv',\n",
    "    'emb-contr': f'{base_dir}/data/{data_ac}/intermediate/ee_concept_contr_knn_k=None_eval.csv',\n",
    "    'emb-contr-aux': f'{base_dir}/data/{data_ac}/intermediate/ee_concept_contr_knn-aux-k=None_eval.csv',\n",
    "    'lm': f'{base_dir}/data/{data_ac}/intermediate/ee_LM_bert_k=None_eval.csv',\n",
    "    'lm-contr': f'{base_dir}/data/{data_ac}/intermediate/ee_LM_bert_contr_k=None_eval.csv',\n",
    "    'lm-contr-aux': f'{base_dir}/data/{data_ac}/intermediate/ee_LM_bert_contr-aux-k=None_eval.csv',\n",
    "    'mrr-combine': f'{base_dir}/data/{data_ac}/intermediate/ee_mrr_combine_k=None_eval.csv',\n",
    "    'mrr-combine-aux': f'{base_dir}/data/{data_ac}/intermediate/ee_mrr_combine_3-k=None_eval.csv',\n",
    "}\n",
    "\n",
    "_eval_df = create_tidy_data_gold_k(ee_eval_filepath_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>concept</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emb</td>\n",
       "      <td>company</td>\n",
       "      <td>0.413333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emb</td>\n",
       "      <td>dress_code</td>\n",
       "      <td>0.295019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emb</td>\n",
       "      <td>job_position</td>\n",
       "      <td>0.220690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method       concept       acc\n",
       "0    emb       company  0.413333\n",
       "1    emb    dress_code  0.295019\n",
       "2    emb  job_position  0.220690"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_eval_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy = ['company','dress_code','benefits','job_position','employee_type']\n",
    "# hard = ['background_screening', 'compensation', 'hire_prerequisite','onboarding_steps','payment_option','person','schedule','shifts']\n",
    "\n",
    "# cc_groups = {\n",
    "#     '1-easy': ['company', 'dress_code', 'job_position', 'benefits', 'payment_option', 'employee_type'],\n",
    "#     '2-medium': ['hire_prerequisite', 'background_screening', 'shifts', 'pay_schedule', 'schedule'],\n",
    "#     '3-hard': ['person', 'onboarding_steps', 'compensation']\n",
    "# }\n",
    "\n",
    "cc_groups = {\n",
    "    '1-non-overlap': ['company', 'dress_code', 'job_position', 'payment_option', 'employee_type', 'onboarding_steps'],\n",
    "    '2-overlap': ['hire_prerequisite', 'background_screening', 'person', 'benefits', 'compensation', 'shifts', 'pay_schedule', 'schedule']\n",
    "}\n",
    "\n",
    "cc2group = dict([(cc, g) for g, cc_list in cc_groups.items() for cc in cc_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': '1-non-overlap',\n",
       " 'dress_code': '1-non-overlap',\n",
       " 'job_position': '1-non-overlap',\n",
       " 'payment_option': '1-non-overlap',\n",
       " 'employee_type': '1-non-overlap',\n",
       " 'onboarding_steps': '1-non-overlap',\n",
       " 'hire_prerequisite': '2-overlap',\n",
       " 'background_screening': '2-overlap',\n",
       " 'person': '2-overlap',\n",
       " 'benefits': '2-overlap',\n",
       " 'compensation': '2-overlap',\n",
       " 'shifts': '2-overlap',\n",
       " 'pay_schedule': '2-overlap',\n",
       " 'schedule': '2-overlap'}"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc2group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>concept</th>\n",
       "      <th>acc</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emb</td>\n",
       "      <td>company</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0-all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emb</td>\n",
       "      <td>dress_code</td>\n",
       "      <td>0.295019</td>\n",
       "      <td>0-all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emb</td>\n",
       "      <td>job_position</td>\n",
       "      <td>0.220690</td>\n",
       "      <td>0-all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method       concept       acc  group\n",
       "0    emb       company  0.413333  0-all\n",
       "1    emb    dress_code  0.295019  0-all\n",
       "2    emb  job_position  0.220690  0-all"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_eval_df['group'] = _eval_df.apply(lambda row: cc2group[row['concept']], axis=1)\n",
    "_eval_df_2 = _eval_df.copy()\n",
    "_eval_df_2['group'] = '0-all'\n",
    "_eval_df_a = pd.concat([_eval_df_2, _eval_df])\n",
    "_eval_df_a.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df_plot = _eval_df_a[_eval_df_a.apply(lambda row: row['method'] != 'mrr-combine', axis=1)]\n",
    "_df_plot = _eval_df_a\n",
    "df_pivot_difficulty = pivot_df(_df_plot, \"acc\", \"group\", \"method\")\n",
    "df_pivot_difficulty.plot(kind=\"bar\", figsize=(8,6)).legend(loc='upper left',ncol=1,bbox_to_anchor=(1.05, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump (expanded) entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112322, 3)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee_pred_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_mrr_combine_k=None.csv')\n",
    "ee_df = pd.read_csv(ee_pred_path)\n",
    "ee_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "instances_dict = dict(zip(concepts_df['alignedCategoryName'].tolist(),\n",
    "                      concepts_df['seedInstances'].tolist()))\n",
    "instances_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('company', 23),\n",
       " ('dress_code', 25),\n",
       " ('job_position', 22),\n",
       " ('pay_schedule', 21),\n",
       " ('benefits', 23),\n",
       " ('compensation', 21),\n",
       " ('payment_option', 21),\n",
       " ('background_screening', 21),\n",
       " ('person', 25),\n",
       " ('hire_prerequisite', 22),\n",
       " ('shifts', 23),\n",
       " ('schedule', 25),\n",
       " ('employee_type', 20),\n",
       " ('onboarding_steps', 25)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 20\n",
    "for cc, e_list in instances_dict.items():\n",
    "    e_list = list(set(e_list + ee_df[ee_df['concept'] == cc]['neighbor'].tolist()[:K]))\n",
    "    instances_dict[cc] = e_list\n",
    "[(cc, len(e_list)) for cc, e_list in instances_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_dict_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/expanded_mrr_combine_k=20.json')\n",
    "with open(instances_dict_path, 'w') as f:\n",
    "    json.dump(instances_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction Baselines\n",
    "Currently only for single relation. TODO: include all relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null baseline - Cartesian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company \t pay_schedule\n",
      "company \t dress_code\n",
      "company \t background_screening\n",
      "company \t benefits\n",
      "company \t person\n",
      "company \t compensation\n",
      "company \t hire_prerequisite\n",
      "company \t schedule\n",
      "company \t employee_type\n",
      "company \t onboarding_steps\n",
      "company \t shifts\n",
      "company \t job_position\n",
      "company \t payment_option\n",
      "job_position \t background_screening\n",
      "job_position \t hire_prerequisite\n",
      "job_position \t employee_type\n",
      "job_position \t onboarding_steps\n"
     ]
    }
   ],
   "source": [
    "# Use script \n",
    "!python relation_extraction_cartesian.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-aux-EE=emb_contr-K=100-RE=Ct.csv \\\n",
    "-cknn $base_dir/data/$data_ac/intermediate/ee_concept_contr_knn_k=None.csv \\\n",
    "-topk 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company \t pay_schedule\n",
      "company \t dress_code\n",
      "company \t background_screening\n",
      "company \t benefits\n",
      "company \t person\n",
      "company \t compensation\n",
      "company \t hire_prerequisite\n",
      "company \t schedule\n",
      "company \t employee_type\n",
      "company \t onboarding_steps\n",
      "company \t shifts\n",
      "company \t job_position\n",
      "company \t payment_option\n",
      "job_position \t background_screening\n",
      "job_position \t hire_prerequisite\n",
      "job_position \t employee_type\n",
      "job_position \t onboarding_steps\n"
     ]
    }
   ],
   "source": [
    "# Use script \n",
    "!python relation_extraction_cartesian.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-aux-EE=mrr_combine-K=20-RE=Ct.csv \\\n",
    "-cknn $base_dir/data/$data_ac/intermediate/ee_mrr_combine_k=None.csv \\\n",
    "-topk 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 107\r\n",
      "Predicted relations: 10000\r\n",
      "Intersection: 6\r\n",
      "P = 0.0006, R = 0.0561, F1 = 0.0012\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('subway', 'has_dress_code', 'piercings')\r\n",
      "('walmart', 'has_dress_code', 'black jeans')\r\n",
      "('walmart', 'has_dress_code', 'blue collar')\r\n",
      "('walmart', 'has_dress_code', 'face tattoos')\r\n",
      "('walmart', 'has_dress_code', 'jeans')\r\n",
      "('walmart', 'has_dress_code', 'uniform')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_dress_code-EE=LM_bert-RE=Ct.csv \\\n",
    "-r has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 107\r\n",
      "Predicted relations: 203\r\n",
      "Intersection: 4\r\n",
      "P = 0.0197, R = 0.0374, F1 = 0.0258\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('subway', 'has_dress_code', 'piercings')\r\n",
      "('walmart', 'has_dress_code', 'black jeans')\r\n",
      "('walmart', 'has_dress_code', 'jeans')\r\n",
      "('walmart', 'has_dress_code', 'uniform')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_dress_code-EE=LM_bert-RE=Ct+KV=0.9.csv \\\n",
    "-r has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Verification baseline\n",
    "(finding co-occurrences of head / tail from corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Finding evidence for rels: 100%|████████████████| 68/68 [00:08<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script \n",
    "!python knowledge_verification_entail.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-in $base_dir/data/$data_ac/intermediate/rel_extraction-all-EE=mrr_combine-RE=Ct-MINI.csv \\\n",
    "-o_kv $base_dir/data/$data_ac/intermediate/kv_evidences-all-EE=mrr_combine-RE=Ct-MINI.json \\\n",
    "-o_re $base_dir/data/$data_ac/intermediate/rel_extraction-all-EE=mrr_combine-RE=Ct+KV=0.9-MINI.csv \\\n",
    "-r $yutong_base_dir/models/roberta-large \\\n",
    "-rs $yutong_base_dir/repos/Roberta_SES/checkpoints/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt \\\n",
    "-p_kv 0.7 \\\n",
    "-p_re 0.9 \\\n",
    "--fast_skip 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use script \n",
    "!python knowledge_verification_entail.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-in $base_dir/data/$data_ac/intermediate/rel_extraction-aux-EE=mrr_combine-K=100-RE=Ct.csv \\\n",
    "-o_kv $base_dir/data/$data_ac/intermediate/kv_evidences-aux-EE=mrr_combine-K=100-RE=Ct.json \\\n",
    "-o_re $base_dir/data/$data_ac/intermediate/rel_extraction-aux-EE=mrr_combine-K=100-RE=Ct+KV=0.9.csv \\\n",
    "-r $yutong_base_dir/models/roberta-large \\\n",
    "-rs $yutong_base_dir/repos/Roberta_SES/checkpoints/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt \\\n",
    "-p_kv 0.7 \\\n",
    "-p_re 0.9 \\\n",
    "--fast_skip 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!grep \"\\\"pharmacy\\\"\" $base_dir/data/$data_ac/intermediate/kv_evidences-aux-EE=mrr_combine-RE=Ct.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Prompt Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit here: /meg-kb/src/analysis/lm_probing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggest Quality Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussions:\n",
    "# coherence clustering / ensemble models?\n",
    "# trying for other relations or entities\n",
    "# using entities in sub-categories\n",
    "# fine-tuning\n",
    "# ambiguous samples (high for pos and neg)\n",
    "# quantitative-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs/version_8 --port 6008 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 76798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_transformers_nikita)",
   "language": "python",
   "name": "conda_transformers_nikita"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
