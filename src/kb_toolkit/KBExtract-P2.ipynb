{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change paths if needed. Shouldn't need to change yutong_base_dir (which points to roberta_ses ckpt)\n",
    "base_dir=\"/mnt/efs/shared/meg_shared_scripts/meg-kb\"\n",
    "data_ac=\"indeeda-meg-ac\"\n",
    "data_pt=\"indeeda-meg-pt\"\n",
    "yutong_base_dir=\"/home/ubuntu/users/yutong\"\n",
    "\n",
    "benchmark_dir = os.path.join(base_dir, 'data/indeed-benchmark')\n",
    "seed_aligned_concepts_path = os.path.join(benchmark_dir, f'seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(benchmark_dir, f'seed_aligned_relations_nodup.csv')\n",
    "benchmark_path = os.path.join(benchmark_dir, f'benchmark_evidence_clean.csv')\n",
    "\n",
    "# Seed files with auxiliary concepts / relations\n",
    "# seed_aux_concepts_path = os.path.join(benchmark_dir, f'seed_aligned_concepts_aux.csv')\n",
    "# seed_aux_relations_path = os.path.join(benchmark_dir, f'seed_aligned_relations_aux.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr, entropy, gmean\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import logging\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from annoy import AnnoyIndex\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "nlp_full = spacy.load('en_core_web_sm')\n",
    "\n",
    "from compute_concept_clusters import knn\n",
    "from compute_keyphrase_embeddings import ensure_tensor_on_device, mean_pooling\n",
    "\n",
    "from lm_probes import LMProbe, LMProbe_GPT2, LMProbe_Joint, LMProbe_PMI, LMProbe_PMI_greedy\n",
    "from utils import load_embeddings, load_seed_aligned_concepts, load_seed_aligned_relations, load_benchmark\n",
    "from utils import load_EE_labels\n",
    "from utils import get_masked_contexts, bert_untokenize\n",
    "from utils import learn_patterns\n",
    "\n",
    "from roberta_ses.interface import Roberta_SES_Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload modules if changed their code \n",
    "\n",
    "# import utils\n",
    "# importlib.reload(utils)\n",
    "# from utils import load_embeddings, load_seed_aligned_concepts, load_seed_aligned_relations, load_benchmark\n",
    "# from utils import load_EE_labels\n",
    "# from utils import get_masked_contexts, bert_untokenize\n",
    "# from utils import learn_patterns\n",
    "\n",
    "# import lm_probes\n",
    "# importlib.reload(lm_probes)\n",
    "# from lm_probes import LMProbe, LMProbe_GPT2, LMProbe_Joint, LMProbe_PMI, LMProbe_PMI_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: text corpus\n",
    "# step 1: extract key phrases (autophrase)\n",
    "# step 2: generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/keyword_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n"
     ]
    }
   ],
   "source": [
    "#change to keyword extractor directory\n",
    "%cd $base_dir/src/keyword_extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./corpusProcess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset and thread no\n",
    "# data_ac = 'indeeda-meg-ac'\n",
    "# data_pt = 'indeeda-meg-pt'\n",
    "thread = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process corpus and generate key prhases\n",
    "!./corpusProcess.sh $data_ac $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy these results to sample-meg-pt\n",
    "!cp -r $base_dir/data/$data_ac $base_dir/data/$data_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using BERT\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2 python compute_keyphrase_embeddings.py \\\n",
    "-m bert-base-uncased \\\n",
    "-et ac \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-c 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|███████████████| 901796/901796 [00:03<00:00, 274660.12it/s]\n",
      "computing entity-wise embedding: 100%|██████| 8028/8028 [04:55<00:00, 27.21it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "## Using RoBERTa \n",
    "\n",
    "# !CUDA_VISIBLE_DEVICES=2 python compute_keyphrase_embeddings.py \\\n",
    "# -m roberta-base \\\n",
    "# -et ac \\\n",
    "# -ename RoBERTa \\\n",
    "# -d $base_dir/data/$data_ac/intermediate \\\n",
    "# -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python compute_keyphrase_embeddings.py \\\n",
    "-m bert-base-uncased \\\n",
    "-et pt \\\n",
    "-d $base_dir/data/$data_pt/intermediate \\\n",
    "-c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding\n",
    "not used for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to autophrase\n",
    "# %cd $base_dir/src/tools/AutoPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_corel = 'sample-indeeda-corel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !CUDA_VISIBLE_DEVICES=0 python extractBertEmbedding.py ../../../data/$data_corel/intermediate/ $thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add embeddings for seed instances\n",
    "skipped - assume that seed entities are in AutoPhrase output. If not, need some more work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !CUDA_VISIBLE_DEVICES=2 python add_seed_instances_embeddings.py \\\n",
    "# -m roberta-base \\\n",
    "# -et ac \\\n",
    "# -ename RoBERTa \\\n",
    "# -d $base_dir/data/$data_ac/intermediate \\\n",
    "# -b $base_dir/data/indeed-benchmark \\\n",
    "# -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed.txt')\n",
    "\n",
    "# embeddings = load_embeddings(bert_emb_path, 768)\n",
    "# len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Seed Entities (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-emb (seed instances k-NN)\n",
    "(using all seed instances of a concept to find neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|██████████████| 8063/8063 [00:01<00:00, 4398.92it/s]\n",
      "100%|███████████████████████████████████████████| 14/14 [00:01<00:00, 13.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "!python compute_concept_seeds_knn.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-e $base_dir/data/$data_ac/intermediate/BERTembed.txt \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_concept_knn_k=None.csv \\\n",
    "-kdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check results \n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_knn_k=None.csv')\n",
    "\n",
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'job_position'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-LM-probe (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compute_EE_LM_probe.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-lm mlm \\\n",
    "-lm_model bert-base-uncased \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_bert_k=None.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 14/14 [00:58<00:00,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use RoBERTa \n",
    "\n",
    "# !python compute_EE_LM_probe.py \\\n",
    "# -d $base_dir/data/$data_ac/intermediate \\\n",
    "# -b $base_dir/data/indeed-benchmark \\\n",
    "# -lm mlm \\\n",
    "# -lm_model roberta-base \\\n",
    "# -o $base_dir/data/$data_ac/intermediate/ee_LM_roberta_k=None.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:54<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "## Domain-adapted BERT, avg probs across templates (default is max)\n",
    "\n",
    "# !python compute_EE_LM_probe.py \\\n",
    "# -d $base_dir/data/$data_ac/intermediate \\\n",
    "# -b $base_dir/data/indeed-benchmark \\\n",
    "# -lm bert \\\n",
    "# -lm_model /home/ubuntu/users/nikita/models/bert_finetuned_lm/indeed_reviews_ques_ans \\\n",
    "# -agg avg \\\n",
    "# -o $base_dir/data/$data_ac/intermediate/ee_LM_bert_DA_avg_k=None.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postproc contrastive EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postproc_contrastive_EE(ee_in_path, ee_contr_out_path, score_col, seed_concepts_path,\n",
    "                            seed_score=0.0, keep_all_entities=False):\n",
    "    \n",
    "    # score_col: column name of score to use for ranking \n",
    "    # seed_score: default score for seeds, since their scores are not computed in EE methods\n",
    "    # keep_all_entities: if False, only assign entities to best concepts (contr);\n",
    "    #   if True, keep all entities for each concept (acontr)\n",
    "    \n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    seed_instances_dict = dict(zip(\n",
    "        seed_concepts_df['alignedCategoryName'].tolist(),\n",
    "        seed_concepts_df['seedInstances'].tolist()\n",
    "    ))\n",
    "    all_seed_instances = set([_e for _seeds in seed_instances_dict.values() for _e in _seeds])\n",
    "    print(sorted(list(all_seed_instances)))\n",
    "    ee_in_df = pd.read_csv(ee_in_path)\n",
    "    \n",
    "    cc_list = list(set(ee_in_df['concept'].tolist()))\n",
    "    all_ent_list = list(set(ee_in_df['neighbor'].tolist()))\n",
    "    \n",
    "    cc_scores_dict = dict()\n",
    "    for cc in cc_list:\n",
    "        cc_df = ee_in_df[ee_in_df['concept'] == cc]\n",
    "        cc_scores_dict[cc] = dict(zip(\n",
    "            cc_df['neighbor'].tolist(),\n",
    "            cc_df[score_col].tolist()\n",
    "        ))\n",
    "    \n",
    "    cands_for_concepts = [[] for _ in range(len(cc_list))]\n",
    "    for _e in all_ent_list:\n",
    "#         if _e in all_seed_instances:\n",
    "#             continue\n",
    "        _scores = [cc_scores_dict[cc].get(_e, seed_score) for cc in cc_list]\n",
    "        _cc_ranking = np.argsort(_scores).tolist()[::-1]\n",
    "        _max_cc_id = _cc_ranking[0]\n",
    "        _second_cc_id = _cc_ranking[1]\n",
    "#         _score = _scores[_max_cc_id]\n",
    "#         _2nd_score = _scores[_second_cc_id]\n",
    "#         _margin = _score - _2nd_score\n",
    "        \n",
    "        for cc_id, cc in enumerate(cc_list):\n",
    "            if _e in seed_instances_dict[cc]:\n",
    "                continue\n",
    "            if not keep_all_entities and cc_id != _max_cc_id:\n",
    "                continue\n",
    "            obest_cc_id = _second_cc_id if cc_id == _max_cc_id else _max_cc_id\n",
    "            _score = _scores[cc_id]\n",
    "            _obest_score = _scores[obest_cc_id]\n",
    "            _margin = _score - _obest_score\n",
    "            cands_for_concepts[cc_id].append({\n",
    "                'concept': cc,\n",
    "                'obest_concept': cc_list[obest_cc_id],\n",
    "                'neighbor': _e,\n",
    "                score_col: _score,\n",
    "                f'obest_{score_col}': _obest_score,\n",
    "                'margin': _margin,\n",
    "                f'{score_col}+margin': _score + _margin\n",
    "            })\n",
    "\n",
    "    out_records = []\n",
    "    for cc_id, cands in enumerate(cands_for_concepts):\n",
    "        cands_sorted = sorted(cands, key=lambda d: d[f'{score_col}+margin'], reverse=True)\n",
    "        out_records.extend(cands_sorted)\n",
    "    \n",
    "    pd.DataFrame(out_records).to_csv(ee_contr_out_path, index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate contrastive scoring preds for EE-emb \n",
    "\n",
    "ee_in_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_knn_k=None.csv')\n",
    "ee_contr_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None.csv')\n",
    "\n",
    "postproc_contrastive_EE(ee_in_path=ee_in_path,\n",
    "                        ee_contr_out_path=ee_contr_out_path,\n",
    "                        score_col='sim',\n",
    "                        seed_concepts_path=seed_aligned_concepts_path,\n",
    "                        keep_all_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate contrastive scoring preds for EE-LM \n",
    "\n",
    "ee_in_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')\n",
    "ee_contr_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_contr_k=None.csv')\n",
    "\n",
    "postproc_contrastive_EE(ee_in_path=ee_in_path,\n",
    "                        ee_contr_out_path=ee_contr_out_path,\n",
    "                        score_col='lm_score',\n",
    "                        seed_concepts_path=seed_aligned_concepts_path,\n",
    "                        keep_all_entities=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_contr_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_concept_contr_knn_k=None.csv')\n",
    "ee_LM_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_LM_bert_k=None.csv')\n",
    "\n",
    "# ee_labels_path = os.path.join(benchmark_dir, 'ee-labels-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company',\n",
       " 'dress_code',\n",
       " 'job_position',\n",
       " 'pay_schedule',\n",
       " 'benefits',\n",
       " 'compensation',\n",
       " 'payment_option',\n",
       " 'background_screening',\n",
       " 'person',\n",
       " 'hire_prerequisite',\n",
       " 'shifts',\n",
       " 'schedule',\n",
       " 'employee_type',\n",
       " 'onboarding_steps']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee_contr_emb_df = pd.read_csv(ee_contr_emb_path)\n",
    "ee_LM_df = pd.read_csv(ee_LM_path)\n",
    "\n",
    "concept_list = ee_LM_df['concept'].drop_duplicates().to_list()\n",
    "concept_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using MRR to combine ranking \n",
    "\n",
    "ee_mrr_combine_list = []\n",
    "\n",
    "for _cc in sorted(concept_list):\n",
    "    _ce_df = ee_contr_emb_df[ee_contr_emb_df['concept'] == _cc].sort_values(by='sim+margin', ascending=False)\n",
    "    _ee_contr_emb_list = _ce_df['neighbor'].tolist()\n",
    "    \n",
    "    _ee_LM_list = ee_LM_df[ee_LM_df['concept'] == _cc]['neighbor'].tolist()\n",
    "    \n",
    "    _all_entities_mrr = defaultdict(float)\n",
    "    for i, _e in enumerate(_ee_contr_emb_list):\n",
    "        _all_entities_mrr[_e] += 1.0 / (i+1)\n",
    "    for i, _e in enumerate(_ee_LM_list):\n",
    "        _all_entities_mrr[_e] += 1.0 / (i+1)\n",
    "\n",
    "    _all_entities_mrr_list = sorted(list(_all_entities_mrr.items()), key=lambda p: p[-1], reverse=True)\n",
    "    \n",
    "    for _e, _mrr in _all_entities_mrr_list:\n",
    "        ee_mrr_combine_list.append((_cc, _e, _mrr))\n",
    "\n",
    "len(ee_mrr_combine_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_mrr_combine_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/ee_mrr_combine_k=None.csv')\n",
    "pd.DataFrame(ee_mrr_combine_list, columns=['concept', 'neighbor', 'MRR']).to_csv(ee_mrr_combine_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity expansion evaluation\n",
    "Now using benchmark entities, mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_concept_knn_k=None.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_concept_knn_k=None_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_concept_contr_knn_k=None.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_concept_contr_knn_k=None_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_LM_bert_k=None.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_bert_k=None_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_LM_bert_contr_k=None.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_LM_bert_contr_k=None_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_entities.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/ee_mrr_combine_k=None.csv \\\n",
    "-o $base_dir/data/$data_ac/intermediate/ee_mrr_combine_k=None_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction Baselines\n",
    "Currently only for single relation. TODO: include all relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null baseline - Cartesian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use script \n",
    "!python relation_extraction_cartesian.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $benchmark_dir \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-EE=mrr_combine-K=100-RE=Ct.csv \\\n",
    "-cknn $base_dir/data/$data_ac/intermediate/ee_mrr_combine_k=None.csv \\\n",
    "-topk 100 \\\n",
    "--exclude_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $benchmark_dir \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-EE=mrr_combine-K=100-RE=Ct.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python eval_relations.py \\\n",
    "# -b $base_dir/data/indeed-benchmark \\\n",
    "# -pred $base_dir/data/$data_ac/intermediate/rel_extraction-EE=mrr_combine-K=100-RE=Ct+KV=0.9.csv \\\n",
    "# -r has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Verification baseline\n",
    "(finding co-occurrences of head / tail from corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Finding evidence for rels: 100%|████████████████| 68/68 [00:08<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script \n",
    "!python knowledge_verification_entail.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-in $base_dir/data/$data_ac/intermediate/rel_extraction-EE=mrr_combine-K=100-RE=Ct.csv \\\n",
    "-o_kv $base_dir/data/$data_ac/intermediate/kv_evidences-EE=mrr_combine-K=100-RE=Ct.json \\\n",
    "-o_re $base_dir/data/$data_ac/intermediate/rel_extraction-EE=mrr_combine-K=100-RE=Ct+KV=0.9.csv \\\n",
    "-r $yutong_base_dir/models/roberta-large \\\n",
    "-rs $yutong_base_dir/repos/Roberta_SES/checkpoints/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt \\\n",
    "-p_kv 0.7 \\\n",
    "-p_re 0.9 \\\n",
    "--fast_skip 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate relations \n",
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-EE=mrr_combine-K=100-RE=Ct+KV=0.9.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_transformers_nikita)",
   "language": "python",
   "name": "conda_transformers_nikita"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
