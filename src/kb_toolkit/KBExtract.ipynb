{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: text corpus\n",
    "# step 1: extract key phrases (autophrase)\n",
    "# step 2: generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/keyword_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n"
     ]
    }
   ],
   "source": [
    "#change to keyword extractor directory\n",
    "%cd ../keyword_extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./corpusProcess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset and thread no\n",
    "data = 'sample-meg-ac'\n",
    "thread = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n",
      "\u001b[32m===Corpus Name: sample-meg-ac===\u001b[m\n",
      "\u001b[32m===Current Path: /mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction===\u001b[m\n",
      "\u001b[32m===Cleaning input corpus===\u001b[m\n",
      "\u001b[32m===Running AutoPhrase===\u001b[m\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[32m===RAW_TRAIN: ../../../data/sample-meg-ac/source/corpus.clean.txt===\u001b[m\n",
      "auto_phrase.sh parameters: sample-meg-ac ../../../data/sample-meg-ac/source/corpus.clean.txt 10 data/EN/wiki_quality.txt 8\n",
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m0.627s\n",
      "user\t0m1.300s\n",
      "sys\t0m0.128s\n",
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n",
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Disabled\n",
      "Discard Ratio = 0.050000\n",
      "Number of threads = 8\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 5792\n",
      "max word token id = 1429\n",
      "# of documents = 100\n",
      "# of distinct POS tags = 0\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 1433\n",
      "# of frequent phrases = 1442\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 203\n",
      "\tThe size of the negative pool = 1230\n",
      "# truth patterns = 815\n",
      "Estimating Phrase Quality...\n",
      "0 12\n",
      "[ERROR] not enough training data found!\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "0 12\n",
      "[ERROR] not enough training data found!\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m1.887s\n",
      "user\t0m2.368s\n",
      "sys\t0m0.016s\n",
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "phrasal_segmentation.sh parameters: sample-meg-ac ../../../data/sample-meg-ac/source/corpus.clean.txt 0.5 0.9 8\n",
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m0.582s\n",
      "user\t0m1.312s\n",
      "sys\t0m0.088s\n",
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n",
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/sample-meg-ac/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.500000\n",
      "\tQ(single-word phrases) >= 0.900000\n",
      "=======\n",
      "Length penalty model loaded.\n",
      "\tpenalty = 199.805\n",
      "# of loaded patterns = 92\n",
      "# of loaded truth patterns = 1018\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 547\n",
      "   # of total processed sentences = 806\n",
      "   avg highlights per sentence = 0.67866\n",
      "\n",
      "real\t0m0.051s\n",
      "user\t0m0.012s\n",
      "sys\t0m0.000s\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Generating Phrase Text===\u001b[m\n",
      "process_segmentation.py parameters: ../../../data/sample-meg-ac/intermediate/ 0.5 0.9\n",
      "55.54\n",
      "\u001b[32m===Running NLP Feature Extraction===\u001b[m\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 23.00it/s]\n",
      "Finish NLP processing, using time 0.4049372673034668 (second)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 27.27it/s]\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:00<00:00, 21.19it/s]\n",
      " 55%|████████████████████████                    | 6/11 [00:00<00:00, 24.90it/s]Finish NLP processing, using time 0.46374964714050293 (second)\n",
      "Finish NLP processing, using time 0.4802677631378174 (second)\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 30.82it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 22.77it/s]\n",
      " 91%|███████████████████████████████████████    | 10/11 [00:00<00:00, 26.17it/s]Finish NLP processing, using time 0.5851848125457764 (second)\n",
      "100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 36.36it/s]\n",
      "Finish NLP processing, using time 0.5972788333892822 (second)\n",
      "Finish NLP processing, using time 0.6108806133270264 (second)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 24.52it/s]\n",
      "Finish NLP processing, using time 0.502152681350708 (second)\n",
      "100%|███████████████████████████████████████████| 13/13 [00:00<00:00, 25.64it/s]\n",
      "Finish NLP processing, using time 0.5564460754394531 (second)\n",
      "\u001b[32m===Clean unnecessary files===\u001b[m\n",
      "\u001b[32m===Key Term Extraction===\u001b[m\n",
      "Extract Key Terms from Corpus: 100%|███████| 458/458 [00:00<00:00, 20049.80it/s]\n",
      "\u001b[32m===Sentence-wise Entity Segmentation===\u001b[m\n",
      "loading corpus for word2vec training: 100%|█| 458/458 [00:00<00:00, 192793.18it/\n",
      "100%|█████████████████████████████████████| 117/117 [00:00<00:00, 443701.24it/s]\n",
      "100%|█████████████████████████████████████| 144/144 [00:00<00:00, 450395.06it/s]\n",
      "100%|█████████████████████████████████████| 137/137 [00:00<00:00, 395743.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# process corpus and generate key prhases\n",
    "!./corpusProcess.sh $data $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy these results to sample-meg-pt\n",
    "!cp -r ../../data/sample-meg-ac ../../data/sample-meg-pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd ../concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|█████████████████████| 458/458 [00:00<00:00, 140928.12it/s]\n",
      "computing entity-wise embedding: 100%|████████| 175/175 [00:04<00:00, 41.24it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!python compute_keyphrase_embeddings.py -m bert-base-uncased -et ac -d ../../data/$data/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pt = 'sample-meg-pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|█████████████████████| 458/458 [00:00<00:00, 152556.48it/s]\n",
      "computing entity-wise embedding: 100%|████████| 175/175 [00:03<00:00, 53.71it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!python compute_keyphrase_embeddings.py -m bert-base-uncased -et pt -d ../../data/$data_pt/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/tools/AutoPhrase\n"
     ]
    }
   ],
   "source": [
    "# change directory to autophrase\n",
    "%cd ../tools/AutoPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corel = 'sample-corel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2021-05-07 20:21:04,464 : INFO : loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2021-05-07 20:21:04,782 : INFO : loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ubuntu/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2021-05-07 20:21:04,782 : INFO : Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2021-05-07 20:21:05,097 : INFO : loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "loading corpus for retrieving bert embedding: 100%|█| 458/458 [00:00<00:00, 494.\n",
      "finish bert embedding calculation\n",
      "100%|██████████████████████████████████████| 162/162 [00:00<00:00, 41143.04it/s]\n",
      "Saving embedding: model_size=768,vocab_size=162\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extractBertEmbedding.py ../../../data/$data_corel/intermediate/ $thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Seed Entities (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd ../../concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn sentence-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 175/175 [00:00<00:00, 5488.53it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 175/175 [00:00<00:00, 7537.05it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 175/175 [00:00<00:00, 3644.47it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 175/175 [00:00<00:00, 4095.89it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_pt/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 162/162 [00:00<00:00, 5528.57it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 162/162 [00:00<00:00, 7408.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_corel/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore various techniques\n",
    "# Get prompts \"between\" entities\n",
    "# Get prompts by syntactic parsing\n",
    "# Get prompts by paraphrasing\n",
    "# Get prompts uisng AutoPrompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Prompt Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggest Quality Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_meg_nikita)",
   "language": "python",
   "name": "conda_meg_nikita"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
