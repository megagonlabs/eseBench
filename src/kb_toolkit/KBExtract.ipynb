{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"/mnt/efs/shared/meg_shared_scripts/meg-kb\"\n",
    "data_ac=\"indeeda-meg-ac\"\n",
    "data_pt=\"indeeda-meg-pt\"\n",
    "yutong_base_dir=\"/home/ubuntu/users/yutong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr, entropy, gmean\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import logging\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from annoy import AnnoyIndex\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "\n",
    "from compute_concept_clusters import knn\n",
    "from compute_keyphrase_embeddings import ensure_tensor_on_device, mean_pooling\n",
    "\n",
    "from lm_probes import LMProbe, LMProbe_GPT2\n",
    "from utils import load_embeddings, load_seed_aligned_concepts, load_seed_aligned_relations, load_benchmark\n",
    "from utils import get_masked_contexts, bert_untokenize\n",
    "\n",
    "from roberta_ses.interface import Roberta_SES_Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: text corpus\n",
    "# step 1: extract key phrases (autophrase)\n",
    "# step 2: generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/keyword_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n"
     ]
    }
   ],
   "source": [
    "#change to keyword extractor directory\n",
    "%cd $base_dir/src/keyword_extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./corpusProcess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset and thread no\n",
    "data_ac = 'indeeda-meg-ac'\n",
    "data_pt = 'indeeda-meg-pt'\n",
    "thread = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n",
      "\u001b[32m===Corpus Name: sample-indeeda-meg-ac===\u001b[m\n",
      "\u001b[32m===Current Path: /mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction===\u001b[m\n",
      "\u001b[32m===Cleaning input corpus===\u001b[m\n",
      "\u001b[32m===Running AutoPhrase===\u001b[m\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[32m===RAW_TRAIN: ../../../data/sample-indeeda-meg-ac/source/corpus.clean.txt===\u001b[m\n",
      "auto_phrase.sh parameters: sample-indeeda-meg-ac ../../../data/sample-indeeda-meg-ac/source/corpus.clean.txt 10 data/EN/wiki_quality.txt 8\n",
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m0.702s\n",
      "user\t0m1.668s\n",
      "sys\t0m0.100s\n",
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n",
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Disabled\n",
      "Discard Ratio = 0.050000\n",
      "Number of threads = 8\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 6553\n",
      "max word token id = 1450\n",
      "# of documents = 500\n",
      "# of distinct POS tags = 0\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 1451\n",
      "# of frequent phrases = 1483\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 192\n",
      "\tThe size of the negative pool = 1282\n",
      "# truth patterns = 1202\n",
      "Estimating Phrase Quality...\n",
      "0 32\n",
      "[ERROR] not enough training data found!\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "0 32\n",
      "[ERROR] not enough training data found!\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m1.922s\n",
      "user\t0m2.496s\n",
      "sys\t0m0.016s\n",
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "phrasal_segmentation.sh parameters: sample-indeeda-meg-ac ../../../data/sample-indeeda-meg-ac/source/corpus.clean.txt 0.5 0.9 8\n",
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m0.568s\n",
      "user\t0m1.396s\n",
      "sys\t0m0.108s\n",
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n",
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/sample-indeeda-meg-ac/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.500000\n",
      "\tQ(single-word phrases) >= 0.900000\n",
      "=======\n",
      "Length penalty model loaded.\n",
      "\tpenalty = 199.805\n",
      "# of loaded patterns = 136\n",
      "# of loaded truth patterns = 1394\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 715\n",
      "   # of total processed sentences = 828\n",
      "   avg highlights per sentence = 0.863527\n",
      "\n",
      "real\t0m0.050s\n",
      "user\t0m0.016s\n",
      "sys\t0m0.000s\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Generating Phrase Text===\u001b[m\n",
      "process_segmentation.py parameters: ../../../data/sample-indeeda-meg-ac/intermediate/ 0.5 0.9\n",
      "11.152\n",
      "\u001b[32m===Running NLP Feature Extraction===\u001b[m\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 66.53it/s]\n",
      " 71%|██████████████████████████████▌            | 47/66 [00:00<00:00, 58.53it/s]Finish NLP processing, using time 0.8209497928619385 (second)\n",
      "100%|███████████████████████████████████████████| 57/57 [00:00<00:00, 65.72it/s]\n",
      " 80%|██████████████████████████████████▌        | 53/66 [00:00<00:00, 57.33it/s]Finish NLP processing, using time 0.9230477809906006 (second)\n",
      "100%|███████████████████████████████████████████| 62/62 [00:00<00:00, 70.06it/s]\n",
      "100%|███████████████████████████████████████████| 46/46 [00:00<00:00, 57.63it/s]\n",
      "Finish NLP processing, using time 0.9408359527587891 (second)\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:00<00:00, 74.56it/s]Finish NLP processing, using time 0.8718316555023193 (second)\n",
      "100%|███████████████████████████████████████████| 66/66 [00:01<00:00, 62.34it/s]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:00<00:00, 68.87it/s]\n",
      "Finish NLP processing, using time 1.1157433986663818 (second)\n",
      "100%|███████████████████████████████████████████| 76/76 [00:00<00:00, 80.52it/s]\n",
      "Finish NLP processing, using time 0.9821081161499023 (second)\n",
      "Finish NLP processing, using time 1.003368854522705 (second)\n",
      "100%|███████████████████████████████████████████| 79/79 [00:01<00:00, 76.95it/s]\n",
      "Finish NLP processing, using time 1.0861637592315674 (second)\n",
      "\u001b[32m===Clean unnecessary files===\u001b[m\n",
      "\u001b[32m===Key Term Extraction===\u001b[m\n",
      "Extract Key Terms from Corpus: 100%|███████| 694/694 [00:00<00:00, 24407.16it/s]\n",
      "\u001b[32m===Sentence-wise Entity Segmentation===\u001b[m\n",
      "loading corpus for word2vec training: 100%|█| 694/694 [00:00<00:00, 288117.09it/\n",
      "100%|███████████████████████████████████████| 94/94 [00:00<00:00, 431834.15it/s]\n",
      "100%|█████████████████████████████████████| 122/122 [00:00<00:00, 423947.88it/s]\n",
      "100%|█████████████████████████████████████| 118/118 [00:00<00:00, 377519.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# process corpus and generate key prhases\n",
    "!./corpusProcess.sh $data_ac $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy these results to sample-meg-pt\n",
    "!cp -r ../../data/$data_ac ../../data/$data_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus with company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/question_answers.csv'\n",
    "company_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/fccid-companyName.csv'\n",
    "entity_emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')\n",
    "out_corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences_with_company.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8053"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(entity_emb_num_path, 'r') as f:\n",
    "    entities = [l.strip().rsplit(' ', 1)[0] for l in f.readlines()]\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(dataset_path) \n",
    "df_dataset = df_dataset[df_dataset['answerContent'].notna()]\n",
    "df_company = pd.read_csv(company_path)\n",
    "\n",
    "df_merged_dataset = df_dataset.merge(df_company, how='inner', on='fccompanyId')\n",
    "df_merged_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307122, 11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, applied, online, and, submitted, all, attachments, that, I, could, .]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df_merged_dataset.iloc[1]\n",
    "_d = nlp(row[\"answerContent\"])\n",
    "list(_d.sents)\n",
    "list(list(_d.sents)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_corpus = []\n",
    "\n",
    "for i, row in df_merged_dataset.iterrows():\n",
    "    if i > 0 and i % 5000 == 0:\n",
    "        print(f'Progress: {i} / {df_merged_dataset.shape[0]}')\n",
    "    \n",
    "    company = row[\"companyName\"]\n",
    "    ans = row[\"answerContent\"]\n",
    "    ans_nlp = nlp(ans)\n",
    "    for sent in ans_nlp.sents:\n",
    "        sent_tok_list = [str(t) for t in sent]\n",
    "        _s = f' {company} : {\" \".join(sent_tok_list)} '.lower()\n",
    "        _ents = []\n",
    "        for _e in entities:\n",
    "            if f' {_e} ' in _s:\n",
    "                _ents.append(_e)\n",
    "        out_corpus.append({\n",
    "            \"tokens\": sent_tok_list,\n",
    "            \"company\": company,\n",
    "            \"entities\": _ents,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_corpus), out_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_corpus_path, 'w') as f:\n",
    "    for d in out_corpus:\n",
    "        f.write(json.dumps(d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|████████████████| 307122/307122 [11:51<00:00, 431.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "!python build_corpus_with_companies.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-rd /home/ubuntu/users/nikita/data/indeed/indeedQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|█████████████████████| 694/694 [00:00<00:00, 194471.34it/s]\n",
      "computing entity-wise embedding: 100%|████████| 177/177 [00:03<00:00, 50.59it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!python compute_keyphrase_embeddings.py -m bert-base-uncased -et ac -d ../../data/$data_ac/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|█████████████████████| 694/694 [00:00<00:00, 191566.11it/s]\n",
      "computing entity-wise embedding: 100%|████████| 177/177 [00:03<00:00, 53.88it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!python compute_keyphrase_embeddings.py -m bert-base-uncased -et pt -d ../../data/$data_pt/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/tools/AutoPhrase\n"
     ]
    }
   ],
   "source": [
    "# change directory to autophrase\n",
    "%cd $base_dir/src/tools/AutoPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corel = 'sample-indeeda-corel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2021-06-18 00:36:18,384 : INFO : loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2021-06-18 00:36:18,776 : INFO : loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ubuntu/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2021-06-18 00:36:18,777 : INFO : Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2021-06-18 00:36:19,108 : INFO : loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "Traceback (most recent call last):\n",
      "  File \"extractBertEmbedding.py\", line 86, in <module>\n",
      "    with open(inputFilePath, \"r\") as fin:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../../../data/sample-indeeda-corel/intermediate//sent_segmentation.txt'\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extractBertEmbedding.py ../../../data/$data_corel/intermediate/ $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add embeddings for seed instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_masked_contexts_for_entities(entities, input_file):\n",
    "    \"\"\"Return a (list of) sentence(s) with entity replaced with MASK.\"\"\"\n",
    "    \"\"\"YS: input should be sentences.json\"\"\"\n",
    "    \n",
    "    ent_freq = {ent : 0 for ent in entities}\n",
    "    ent_context = {ent : [] for ent in entities}\n",
    "    \n",
    "    with open(input_file, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in tqdm(lines, total=len(lines), desc=\"loading corpus\"):\n",
    "            json_dict = json.loads(line)\n",
    "            sent = ' ' + ' '.join(json_dict['tokens']).lower() + ' '\n",
    "            #entities = [match.group(1) for match in re.finditer(pat, line)]\n",
    "            \n",
    "            for entity in entities:\n",
    "                pat = f' {entity} '\n",
    "                if pat not in sent:\n",
    "                    continue\n",
    "\n",
    "                context = sent.replace(pat, ' [MASK] ').strip()\n",
    "                c = context.split('[MASK]')\n",
    "                if len(c) != 2:  # sanity to not have too many repeating phrases in the context\n",
    "                    continue\n",
    "\n",
    "                # ignore too short contexts\n",
    "                if len(context) < 15:\n",
    "                    continue\n",
    "\n",
    "                # print(entity)\n",
    "                # print(context)\n",
    "                \n",
    "                _freq = ent_freq.get(entity, 0)\n",
    "                ent_freq[entity] = _freq + 1\n",
    "\n",
    "                context_lst = ent_context.get(entity, [])\n",
    "                context_lst.append(context)\n",
    "                ent_context[entity] = context_lst\n",
    "\n",
    "    dedup_context = {}\n",
    "    for e, v in ent_context.items():\n",
    "        dedup_context[e] = list(set(v))\n",
    "    return ent_freq, dedup_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_avg_context_embedding_for_entities(entities, model_path, input_file, max_context_ct):\n",
    "    '''\n",
    "    mean pooling from sentence-transformers\n",
    "    :param entity: List[str], the entities to compute embeddings for\n",
    "    :param model_path:\n",
    "    :param input_file:\n",
    "    :return:\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "    ent_freq, ent_context = get_masked_contexts_for_entities(entities, input_file)\n",
    "    \n",
    "    entity_embeddings = {}\n",
    "    for entity, en_context_lst in tqdm(ent_context.items(), total=len(ent_context), desc=\"computing entity-wise embedding\"):\n",
    "#     for entity, en_context_lst in ent_context.items():\n",
    "        print(entity)\n",
    "        en_context_lst = random.sample(en_context_lst, min(len(en_context_lst), max_context_ct))\n",
    "        chunks = [en_context_lst[i:i + 100] for i in range(0, len(en_context_lst), 100)]\n",
    "        # print(entity)\n",
    "        # print(len(en_context_lst))\n",
    "        all_context_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            encoded_input = tokenizer.batch_encode_plus(chunk, return_token_type_ids=True, add_special_tokens=True, max_length=128, return_tensors='pt', padding=True, pad_to_max_length=True, truncation=True)\n",
    "            mask = encoded_input['input_ids'] != mask_token_id\n",
    "            with torch.no_grad():\n",
    "                encoded_input = ensure_tensor_on_device(device, **encoded_input)\n",
    "                model_output = model(**encoded_input)  # Compute token embeddings\n",
    "            context_embeddings = mean_pooling(model_output, mask)  # mean pooling\n",
    "            all_context_embeddings.append(context_embeddings)\n",
    "            \n",
    "        assert len(all_context_embeddings) > 0\n",
    "            \n",
    "        entity_embedding = torch.mean(torch.cat(all_context_embeddings, dim=0), dim=0).cpu().detach().numpy().tolist()\n",
    "        entity_embeddings[entity] = entity_embedding\n",
    "    \n",
    "    return entity_embeddings, ent_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences.json')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "\n",
    "orig_bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed.txt')\n",
    "orig_bert_emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum.txt')\n",
    "\n",
    "new_bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "new_bert_emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')\n",
    "\n",
    "orig_emb_df = load_embeddings(bert_emb_path, 768)\n",
    "emb_dict = dict(zip(orig_emb_df['entity'].to_list(), orig_emb_df['embedding'].to_list()))\n",
    "\n",
    "with open(orig_bert_emb_num_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    emb_freq_dict = dict([l.strip().rsplit(' ', 1) for l in lines])\n",
    "\n",
    "concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_instances_list = [inst for _, (_a_con, _u_con, _gnrl, _seed_instances) in concepts_df.iterrows()\n",
    "                           for inst in _seed_instances]\n",
    "\n",
    "## debug\n",
    "seed_instances_list = seed_instances_list[::10]\n",
    "\n",
    "print(seed_instances_list)\n",
    "\n",
    "entity_embeddings, ent_freq = \\\n",
    "    get_avg_context_embedding_for_entities(entities=seed_instances_list, \n",
    "                                           model_path='bert-base-uncased',\n",
    "                                           input_file=corpus_path,\n",
    "                                           max_context_ct=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "for inst in seed_instances_list:\n",
    "    emb = entity_embeddings[inst]\n",
    "    freq = ent_freq[inst]\n",
    "    if inst in emb_dict:\n",
    "        print(f'Already exists: {inst}')\n",
    "#         assert np.allclose(emb_dict[inst], emb)\n",
    "#         assert emb_freq_dict[inst] == freq, f'{inst}: orig {emb_freq_dict[inst]} != new {freq}'\n",
    "#         print(f'Check passed: {inst}')\n",
    "    else:\n",
    "        emb_dict[inst] = emb\n",
    "        emb_freq_dict[inst] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "entity_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "with open(new_bert_emb_path, 'w') as f, open(new_bert_emb_num_path, 'w') as f2:\n",
    "    for inst in seed_instances_list:\n",
    "        emb = emb_dict[inst]\n",
    "        freq = ent_freq[inst]\n",
    "        f.write(\"{} {}\\n\".format(inst, ' '.join([str(x) for x in emb])))\n",
    "        f2.write(\"{} {}\\n\".format(inst, freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed instances: ['walmart', 'amazon', 'subway', 'microsoft', 'target', 'business casual', 'uniform', 'hair color', 'tattoos', 'facial hair', 'shoes', 'piercings', 'delivery driver', 'store manager', 'cashier', 'package handler', 'sales associate', 'barista', 'dishwasher', 'weekly', 'biweekly', 'friday', 'saturday', 'health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance', 'base pay', 'stock options', 'benefits', 'overtime pay', 'bonus', 'checks', 'direct deposit', 'prepaid card', 'drug test', 'criminal background check', 'employment verification', 'felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'misdemeanor', 'pregnant', 'students', 'seniors', 'hiring age', 'bachelors degree', 'prior experience', 'working permit', 'heavy lifting', 'night shift', 'dinner shift', 'early morning shift', '8 hour shift', 'christmas eve', 'early morning', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend', 'full time', 'part time', 'seasonal', 'orientation', 'introduction', 'workstation', 'training', 'team lunch']\n",
      "New instances: ['uniform', 'hair color', 'tattoos', 'shoes', 'cashier', 'weekly', 'biweekly', 'friday', 'saturday', '401k', 'stock options', 'benefits', 'overtime pay', 'bonus', 'checks', 'employment verification', 'felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'pregnant', 'students', 'seniors', '8 hour shift', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend', 'full time', 'part time', 'seasonal', 'orientation', 'introduction', 'training', 'team lunch']\n",
      "loading corpus: 100%|████████████████| 465226/465226 [00:09<00:00, 47556.15it/s]\n",
      "computing entity-wise embedding: 100%|██████████| 36/36 [00:41<00:00,  1.14s/it]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "# Using script\n",
    "\n",
    "!python add_seed_instances_embeddings.py -m bert-base-uncased -et ac -d $base_dir/data/$data_ac/intermediate -b $base_dir/data/indeed-benchmark -c 750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub_dir = data_ac\n",
    "bert_emb_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/BERTembed+seeds.txt')\n",
    "\n",
    "embeddings = load_embeddings(bert_emb_path, 768)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8023</th>\n",
       "      <td>biweekly</td>\n",
       "      <td>[0.06975648552179337, -0.06970633566379547, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                                          embedding\n",
       "8023  biweekly  [0.06975648552179337, -0.06970633566379547, 0...."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[embeddings['entity'] == 'biweekly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (X) Other ways of embeddings / clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|██████████| 458/458 [00:00<00:00, 73813.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175, 175)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/sent_segmentation.txt')\n",
    "ent_freq, dedup_context = get_masked_contexts(input_file_path)\n",
    "len(ent_freq), len(dedup_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " [\"we dropped by in hopes of finding atkinson 's peanut_butter bars ( we first tried them from honey salt 's [MASK] bowl ) and after searching a few minutes , we found it .\",\n",
       "  \"if you 're searching for a [MASK] or soda_pop you grew up with and can no longer find , there 's a good chance you 'll find it here .\"])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_freq['candy'], dedup_context['candy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_context_embeddings(model_path, input_file, max_context_ct):\n",
    "    '''\n",
    "    Adapted from get_avg_context_embeddings()\n",
    "    keep all context embeddings, using max similarity for knn\n",
    "    :param model_path:\n",
    "    :param input_file:\n",
    "    :return:\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "    ent_freq, ent_context = get_masked_contexts(input_file)\n",
    "    entity_embeddings = {}\n",
    "    for entity, en_context_lst in tqdm(ent_context.items(), total=len(ent_context), desc=\"computing entity-wise embedding\"):\n",
    "        en_context_lst = random.sample(en_context_lst, min(len(en_context_lst), max_context_ct))\n",
    "        chunks = [en_context_lst[i:i + 100] for i in range(0, len(en_context_lst), 100)]\n",
    "        # print(entity)\n",
    "        # print(len(en_context_lst))\n",
    "        all_context_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            encoded_input = tokenizer.batch_encode_plus(chunk, return_token_type_ids=True, add_special_tokens=True, max_length=128, return_tensors='pt', padding=True, pad_to_max_length=True, truncation=True)\n",
    "            mask = encoded_input['input_ids'] != mask_token_id\n",
    "            with torch.no_grad():\n",
    "                encoded_input = ensure_tensor_on_device(device, **encoded_input)\n",
    "                model_output = model(**encoded_input)  # Compute token embeddings\n",
    "            context_embeddings = mean_pooling(model_output, mask)  # mean pooling\n",
    "            # print(context_embeddings.size())\n",
    "            all_context_embeddings.append(context_embeddings)\n",
    "            \n",
    "        # entity_embedding = torch.mean(torch.cat(all_context_embeddings, dim=0), dim=0).cpu().detach().numpy().tolist()\n",
    "        # entity_embeddings[entity] = entity_embedding\n",
    "        entity_embeddings[entity] = torch.cat(all_context_embeddings, dim=0).cpu().detach().numpy().tolist()\n",
    "        \n",
    "    return entity_embeddings, ent_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|██████████| 458/458 [00:00<00:00, 150194.78it/s]\n",
      "computing entity-wise embedding: 100%|██████████| 175/175 [00:04<00:00, 41.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175, 175)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'bert-base-uncased'\n",
    "input_file_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/sent_segmentation.txt')\n",
    "max_context_ct = 10\n",
    "\n",
    "entity_embeddings, ent_freq = get_all_context_embeddings(model_path, input_file_path, max_context_ct)\n",
    "len(entity_embeddings), len(ent_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_embeddings['candy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _knn(entity_embeddings, embedding_dim, cluster_size, thread_ct=None, cluster_dest=None, **kwargs):\n",
    "    # entity_embeddings = load_embeddings(embed_src, embedding_dim)\n",
    "    \n",
    "    t = AnnoyIndex(embedding_dim, 'angular')\n",
    "    # entities = entity_embeddings['entity'].tolist()\n",
    "    entities = [f'{entity}-{_i}' for entity, embs in entity_embeddings.items() for _i in range(len(embs))]\n",
    "    # print(entities)\n",
    "    # for i, row in tqdm(entity_embeddings.iterrows(), total=entity_embeddings.shape[0], desc=\"building entity index\"):\n",
    "    #     t.add_item(i, row['embedding'])\n",
    "    i = 0\n",
    "    for entity, embs in tqdm(entity_embeddings.items(), total=len(entity_embeddings)):\n",
    "        for emb in embs:\n",
    "            t.add_item(i, emb)\n",
    "            i += 1\n",
    "    assert i == len(entities)\n",
    "    \n",
    "    t.build(100)\n",
    "    \n",
    "    neighbors = []\n",
    "    for i, entity in enumerate(tqdm(entities, desc=\"finding nearest neighbors by entity\")):\n",
    "        # print(i, entity)\n",
    "        nns, dists = t.get_nns_by_item(i, cluster_size + 1, include_distances=True)\n",
    "        cos_sim_scores = [(2 - d ** 2) / 2 for d in dists]  # convert angular distance to cosine similarity\n",
    "        zipped = list(zip(nns, cos_sim_scores))\n",
    "        sorted_nns = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "        if len(sorted_nns) > 0:\n",
    "            for nn_idx, d in sorted_nns:\n",
    "                neighbor_entity = entities[nn_idx]\n",
    "                if neighbor_entity == entity:\n",
    "                    continue\n",
    "                neighbors.append({\"entity\": entity, \"neighbor\": neighbor_entity, \"sim\": d})\n",
    "    c_df = pd.DataFrame(neighbors)\n",
    "    return c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:00<00:00, 24854.50it/s]\n",
      "finding nearest neighbors by entity: 100%|██████████| 269/269 [00:00<00:00, 6006.44it/s]\n"
     ]
    }
   ],
   "source": [
    "knn_results = _knn(entity_embeddings, 768, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'meat'\n",
    "\n",
    "df = knn_results\n",
    "\n",
    "n_embs = len(entity_embeddings[query])\n",
    "sub_frames = []\n",
    "for _i in range(n_embs):\n",
    "    ent_name = f'{query}-{_i}'\n",
    "    sub_frames.append(df[df['entity'] == ent_name])\n",
    "\n",
    "pd.concat(sub_frames).sort_values('sim', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original avg context knn \n",
    "knn_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/knn_100.csv')\n",
    "\n",
    "knn_results = pd.read_csv(knn_path)\n",
    "df = knn_results\n",
    "\n",
    "query = 'walmart'\n",
    "sub_frame = df[df['entity'] == query]\n",
    "sub_frame.sort_values('sim', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Seed Entities (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd ../../concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn sentence-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 100\n",
    "output = '../../data/'+data_ac+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 177/177 [00:00<00:00, 5435.26it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 177/177 [00:00<00:00, 2001.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_ac/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 177/177 [00:00<00:00, 3661.18it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 177/177 [00:00<00:00, 4052.00it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_pt/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_corel/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed instances clustering\n",
    "(using all seed instances of a concept to find neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "# seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_concept_knn(embed_src, embedding_dim, seed_aligned_concept_src, cluster_size, thread_ct, cluster_dest, **kwargs):\n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concept_src)\n",
    "    \n",
    "    entity_embeddings = load_embeddings(embed_src, embedding_dim)\n",
    "    t = AnnoyIndex(embedding_dim, 'angular')\n",
    "    entities = entity_embeddings['entity'].tolist()\n",
    "    for i, row in tqdm(entity_embeddings.iterrows(), total=entity_embeddings.shape[0], desc=\"building entity index\"):\n",
    "        t.add_item(i, row['embedding'])\n",
    "    t.build(100)\n",
    "    \n",
    "    entity_emb_dict = dict(zip(entities, entity_embeddings['embedding'].tolist()))\n",
    "\n",
    "    neighbors = []\n",
    "    for i, (a_concept, u_concept, gnrl, seed_instances) in tqdm(seed_concepts_df.iterrows(), desc=\"finding nearest neighbors by concept\"):\n",
    "        embs = []\n",
    "        for inst in seed_instances:\n",
    "            try:\n",
    "                embs.append(entity_emb_dict[inst])\n",
    "            except KeyError:\n",
    "                print(f\"{inst} not found in entity_emb_dict??\")\n",
    "                continue\n",
    "        if len(embs) == 0:\n",
    "            continue\n",
    "        concept_emb = np.mean(embs, axis=0)\n",
    "        \n",
    "        nns, dists = t.get_nns_by_vector(concept_emb, cluster_size + 1, include_distances=True)\n",
    "        cos_sim_scores = [(2 - d ** 2) / 2 for d in dists]  # convert angular distance to cosine similarity\n",
    "        zipped = list(zip(nns, cos_sim_scores))\n",
    "        sorted_nns = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "        if len(sorted_nns) > 0:\n",
    "            for nn_idx, d in sorted_nns:\n",
    "                neighbor_entity = entities[nn_idx]\n",
    "                if neighbor_entity in seed_instances:\n",
    "                    continue\n",
    "                neighbors.append({\"concept\": a_concept, \"neighbor\": neighbor_entity, \"sim\": d})\n",
    "    c_df = pd.DataFrame(neighbors)\n",
    "    c_df.to_csv(cluster_dest, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "cluster_size = 1000\n",
    "\n",
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_{cluster_size}.csv')\n",
    "\n",
    "get_concept_knn(embed_src=bert_emb_path,\n",
    "            embedding_dim=768,\n",
    "            seed_aligned_concept_src=seed_aligned_concepts_path,\n",
    "            cluster_size=1000,\n",
    "            thread_ct=1,\n",
    "            cluster_dest=concept_knn_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|██████████████| 8053/8053 [00:01<00:00, 6521.18it/s]\n",
      "finding nearest neighbors by concept: 14it [00:00, 433.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "cluster_size = 100\n",
    "!python compute_concept_seeds_knn.py -d $base_dir/data/$data_ac/intermediate -b $base_dir/data/indeed-benchmark -s 100 -o $base_dir/data/$data_ac/intermediate/concept_knn_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check results \n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_{cluster_size}.csv')\n",
    "\n",
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'company'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'pay_schedule'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit here: /meg_shared_scripts/meg-kb/src/analysis/concept_learning-test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')\n",
    "\n",
    "concept_knn_path = concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = load_embeddings(bert_emb_path, 768)\n",
    "entity_emb_dict = dict(zip(entity_embeddings['entity'].tolist(), entity_embeddings['embedding'].tolist()))\n",
    "\n",
    "seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "\n",
    "concepts_knn_df = pd.read_csv(concept_knn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_records = []\n",
    "# record item: List[\n",
    "#     (con_name, con_emb),\n",
    "#     List[(seed_name, seed_emb)],\n",
    "#     List[(inst_name, inst_emb)],\n",
    "# ]\n",
    "\n",
    "for i, (a_concept, u_concept, gnrl, _seed_instances) in seed_concepts_df.iterrows():\n",
    "    _seed_embs = []\n",
    "    for inst in _seed_instances:\n",
    "        try:\n",
    "            _emb = entity_emb_dict[inst]\n",
    "            _seed_embs.append(_emb)\n",
    "        except KeyError:\n",
    "            print(f\"{inst} not found in entity_emb_dict??\")\n",
    "            continue\n",
    "    if len(_seed_embs) == 0:\n",
    "        continue\n",
    "    _concept_emb = np.mean(_seed_embs, axis=0)\n",
    "    \n",
    "    _concept_knns = concepts_knn_df[concepts_knn_df['concept'] == a_concept]['neighbor'].tolist()\n",
    "    _concept_knn_embs = [entity_emb_dict[inst] for inst in _concept_knns]\n",
    "    \n",
    "    _record = [(a_concept, _concept_emb),\n",
    "       list(zip(seed_instances, _seed_embs)),\n",
    "       list(zip(_concept_knns, _concept_knn_embs))\n",
    "    ]\n",
    "    \n",
    "    vis_records.append(_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "_label_list = []\n",
    "_raw_emb_list = []\n",
    "_size_list = []   # knn = 1, seed = 2, concept = 3 \n",
    "_color_id_list = []\n",
    "\n",
    "_CONCEPT_SIZE = 100\n",
    "_SEED_SIZE = 30\n",
    "_CAND_SIZE = 1\n",
    "\n",
    "for c_id, ((_con, _con_emb), _seeds, _knns) in enumerate(vis_records):\n",
    "    _label_list.append(_con)\n",
    "    _raw_emb_list.append(_con_emb)\n",
    "    _size_list.append(_CONCEPT_SIZE)\n",
    "    _color_id_list.append(c_id)\n",
    "    for _seed, _seed_emb in _seeds:\n",
    "        _label_list.append(_seed)\n",
    "        _raw_emb_list.append(_seed_emb)\n",
    "        _size_list.append(_SEED_SIZE)\n",
    "        _color_id_list.append(c_id)\n",
    "    for _inst, _inst_emb in _knns:\n",
    "        _label_list.append(_inst)\n",
    "        _raw_emb_list.append(_inst_emb)\n",
    "        _size_list.append(_CAND_SIZE)\n",
    "        _color_id_list.append(c_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1432"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_raw_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "_tsne_emb_list = tsne.fit_transform(_raw_emb_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_tsne_x, _tsne_y = list(zip(*_tsne_emb_list))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.gca()\n",
    "\n",
    "# _scatter = ax.scatter(_tsne_x, _tsne_y, s=_size_list, c=_color_id_list)\n",
    "\n",
    "cmap = plt.get_cmap(name='viridis')\n",
    "\n",
    "for c_id, _record in enumerate(vis_records[:10]):\n",
    "    _data_ids = [i for i, c in enumerate(_color_id_list) if c == c_id]\n",
    "    _x = [_tsne_x[i] for i in _data_ids]\n",
    "    _y = [_tsne_y[i] for i in _data_ids]\n",
    "    _s = [_size_list[i] for i in _data_ids]\n",
    "#     _c = [c_id for i in _data_ids]\n",
    "#     _c = matplotlib.colors.rgb2hex(cmap(1.0 * c_id / len(vis_records)))\n",
    "    ax.scatter(_x, _y, s=_s, label=_record[0][0])\n",
    "\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend1 = ax.legend(*_scatter.legend_elements(), title=\"Classes\")\n",
    "# # ax.add_artist(legend1)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib.colors.rgb2hex(cmap(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity expansion evaluation\n",
    "Now using benchmark entities, mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'dress_code', 'job_position', 'pay_schedule', 'benefits', 'compensation', 'payment_option', 'background_screening', 'person', 'hire_prerequisite', 'shifts', 'schedule', 'employee_type', 'onboarding_steps']\n",
      "['has_pay_schedule', 'has_pay_schedule', 'has_dress_code', 'has_dress_code', 'has_background_screening', 'has_benefits', 'has_benefits', 'hires_person', 'has_compensation', 'has_compensation', 'has_hire_prerequisite', 'operates_on', 'hires_employee_type', 'has_onboarding_steps', 'has_shifts', 'has_shifts', 'has_job_position', 'has_hiring_policy', 'has_payment_option']\n",
      "{'onboarding_steps', 'pay_schedule', 'shifts', 'schedule', 'company', 'job_position', 'hire_prerequisite', 'payment_option', 'benefits', 'background_screening', 'person', 'employee_type', 'dress_code', 'compensation'}\n",
      "(706, 17)\n"
     ]
    }
   ],
   "source": [
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')\n",
    "benchmark_path = os.path.join(base_dir, f'data/indeed-benchmark/benchmark_evidence_clean.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_1000.csv')\n",
    "\n",
    "seed_aligned_concepts = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_aligned_relations = load_seed_aligned_relations(seed_aligned_relations_path)\n",
    "benchmark = pd.read_csv(benchmark_path)\n",
    "concept_knn = pd.read_csv(concept_knn_path)\n",
    "\n",
    "print(seed_aligned_concepts['alignedCategoryName'].tolist())\n",
    "print(seed_aligned_relations['alignedRelationName'].tolist())\n",
    "print(set(concept_knn['concept'].tolist()))\n",
    "print(benchmark.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_EE(predictions_path,\n",
    "                seed_concepts_path,\n",
    "                seed_relations_path,\n",
    "                benchmark_path):\n",
    "    '''Format of prediction file: CSV, with column \"concept\" and \"neighbor\"(entity) '''\n",
    "    preds_df = pd.read_csv(predictions_path)\n",
    "    \n",
    "    all_benchmark_instances, _ = load_benchmark(benchmark_path, seed_concepts_path, seed_relations_path)\n",
    "    seed_aligned_concepts = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    \n",
    "    mrr_dict = dict()\n",
    "    for i, d in seed_aligned_concepts.iterrows():\n",
    "        a_concept = d[\"alignedCategoryName\"]\n",
    "        u_concept = d[\"unalignedCategoryName\"]\n",
    "        seed_instances = d[\"seedInstances\"]\n",
    "\n",
    "#         concept_knn_instances = concept_knn[concept_knn[\"concept\"] == a_concept][\"neighbor\"].to_list()\n",
    "        pred_instances = preds_df[preds_df[\"concept\"] == a_concept][\"neighbor\"].to_list()\n",
    "\n",
    "#         _b_head_instances = benchmark[benchmark[\"n_head_category\"] == a_concept][\"n_head\"].to_list()\n",
    "#         _b_tail_instances = benchmark[benchmark[\"n_tail_category\"] == a_concept][\"n_tail\"].to_list()\n",
    "#         benchmark_instances = list(set(_b_head_instances + _b_tail_instances))\n",
    "        benchmark_instances = all_benchmark_instances[a_concept]\n",
    "\n",
    "        print(f'Concept: {a_concept} / {u_concept}')\n",
    "        print(f'seeds: {seed_instances}')\n",
    "        b_inst_ranks = dict()\n",
    "        recip_ranks = []\n",
    "        for _inst in benchmark_instances:\n",
    "            if _inst in seed_instances:\n",
    "                b_inst_ranks[_inst] = -1\n",
    "            elif _inst in pred_instances:\n",
    "                _rank = pred_instances.index(_inst) + 1\n",
    "                b_inst_ranks[_inst] = _rank\n",
    "                recip_ranks.append(1.0 / _rank)\n",
    "            else:\n",
    "                b_inst_ranks[_inst] = float('nan')\n",
    "                recip_ranks.append(0.0)\n",
    "                \n",
    "        mrr = np.mean(recip_ranks) if len(recip_ranks) > 0 else 0.0\n",
    "        mrr_dict[a_concept] = mrr\n",
    "        print(json.dumps(b_inst_ranks, indent=4))\n",
    "        print('MRR:', mrr)\n",
    "        print()\n",
    "\n",
    "    print('--- Summary ---')\n",
    "    print(json.dumps(mrr_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations_nodup.csv')\n",
    "benchmark_path = os.path.join(base_dir, f'data/indeed-benchmark/benchmark_evidence_clean.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_1000.csv')\n",
    "\n",
    "evaluate_EE(predictions_path=concept_knn_path,\n",
    "            seed_concepts_path=seed_aligned_concepts_path,\n",
    "            seed_relations_path=seed_aligned_relations_path,\n",
    "            benchmark_path=benchmark_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: company / company\r\n",
      "seeds: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\r\n",
      "{\r\n",
      "    \"frito\": NaN,\r\n",
      "    \"verizon\": NaN,\r\n",
      "    \"olive garden\": NaN,\r\n",
      "    \"lowe's\": NaN,\r\n",
      "    \"family dollar\": NaN,\r\n",
      "    \"wells fargo\": NaN,\r\n",
      "    \"panera\": NaN,\r\n",
      "    \"instacart\": NaN,\r\n",
      "    \"t-mobile\": NaN,\r\n",
      "    \"chipotle\": NaN,\r\n",
      "    \"training\": NaN,\r\n",
      "    \"barnes & noble\": NaN,\r\n",
      "    \"american eagle outfitters\": NaN,\r\n",
      "    \"concentrix\": NaN,\r\n",
      "    \"domino's\": NaN,\r\n",
      "    \"mcdonald's\": NaN,\r\n",
      "    \"foot locker\": NaN,\r\n",
      "    \"ihop\": NaN,\r\n",
      "    \"panera bread\": NaN,\r\n",
      "    \"dunkin donuts\": NaN,\r\n",
      "    \"frito lay\": NaN,\r\n",
      "    \"fedex ground\": NaN,\r\n",
      "    \"electric\": NaN,\r\n",
      "    \"marshalls\": 272,\r\n",
      "    \"victoria's secret\": NaN,\r\n",
      "    \"g4s\": NaN,\r\n",
      "    \"chilis\": NaN,\r\n",
      "    \"cvs\": 426,\r\n",
      "    \"pepsi\": NaN,\r\n",
      "    \"dollar general\": 251,\r\n",
      "    \"sonic drive-in\": NaN,\r\n",
      "    \"costco\": NaN,\r\n",
      "    \"allied universal security services, systems and solutions\": NaN,\r\n",
      "    \"wendys\": NaN,\r\n",
      "    \"pizza hut\": NaN,\r\n",
      "    \"geico\": NaN,\r\n",
      "    \"spectrum\": NaN,\r\n",
      "    \"chipotle mexican grill\": NaN,\r\n",
      "    \"whataburger\": NaN,\r\n",
      "    \"lowes\": NaN,\r\n",
      "    \"cracker barrel\": NaN,\r\n",
      "    \"subway\": -1,\r\n",
      "    \"united states postal service\": NaN,\r\n",
      "    \"goodwill industries\": NaN,\r\n",
      "    \"taco bell\": NaN,\r\n",
      "    \"starbucks\": 34,\r\n",
      "    \"old navy\": NaN,\r\n",
      "    \"teleperformance\": NaN,\r\n",
      "    \"quiktrip\": NaN,\r\n",
      "    \"frito-lay\": NaN,\r\n",
      "    \"tj maxx\": NaN,\r\n",
      "    \"target\": -1,\r\n",
      "    \"planet fitness\": NaN,\r\n",
      "    \"mcdonalds\": 295,\r\n",
      "    \"marriott international, inc.\": NaN,\r\n",
      "    \"ulta\": NaN,\r\n",
      "    \"whole foods market\": NaN,\r\n",
      "    \"publix\": NaN,\r\n",
      "    \"dd\": NaN,\r\n",
      "    \"kroger stores\": NaN,\r\n",
      "    \"little caesars\": NaN,\r\n",
      "    \"doordash\": NaN,\r\n",
      "    \"macy's\": NaN,\r\n",
      "    \"petsmart\": NaN,\r\n",
      "    \"t.j. maxx\": NaN,\r\n",
      "    \"heb\": NaN,\r\n",
      "    \"mcdonald\": NaN,\r\n",
      "    \"best buy\": NaN,\r\n",
      "    \"amazon\": -1,\r\n",
      "    \"fedex\": NaN,\r\n",
      "    \"subways\": NaN,\r\n",
      "    \"chili's\": NaN,\r\n",
      "    \"ross dress for less\": NaN,\r\n",
      "    \"sam's club\": NaN,\r\n",
      "    \"walmart\": -1,\r\n",
      "    \"dollar tree\": NaN,\r\n",
      "    \"amazon.com\": NaN,\r\n",
      "    \"costco wholesale\": NaN,\r\n",
      "    \"chick-fil-a\": NaN,\r\n",
      "    \"the wendy's company\": NaN,\r\n",
      "    \"the home depot\": NaN,\r\n",
      "    \"alorica\": NaN,\r\n",
      "    \"at&t\": NaN,\r\n",
      "    \"walgreens\": NaN,\r\n",
      "    \"primark\": NaN,\r\n",
      "    \"applebee's\": NaN,\r\n",
      "    \"hobby lobby\": NaN,\r\n",
      "    \"burger king\": NaN,\r\n",
      "    \"kfc\": NaN,\r\n",
      "    \"enterprise holdings\": NaN,\r\n",
      "    \"dick's sporting goods\": NaN,\r\n",
      "    \"cvs health\": 375,\r\n",
      "    \"menards\": NaN,\r\n",
      "    \"kroger\": NaN,\r\n",
      "    \"burlington stores\": NaN,\r\n",
      "    \"jcpenney\": NaN,\r\n",
      "    \"ups\": NaN,\r\n",
      "    \"safeway\": NaN,\r\n",
      "    \"dunkin' donuts\": NaN,\r\n",
      "    \"sitel\": 428,\r\n",
      "    \"pepsico\": NaN,\r\n",
      "    \"home depot\": 228,\r\n",
      "    \"kohl's\": NaN,\r\n",
      "    \"aldi\": NaN,\r\n",
      "    \"tim hortons\": NaN\r\n",
      "}\r\n",
      "MRR: 0.0005168180947036233\r\n",
      "\r\n",
      "Concept: dress_code / dress code\r\n",
      "seeds: ['business casual', 'uniform', 'hair color', 'tattoos', 'facial hair', 'shoes', 'piercings']\r\n",
      "{\r\n",
      "    \"facial piercings\": 3,\r\n",
      "    \"black jeans\": 413,\r\n",
      "    \"jeans\": NaN,\r\n",
      "    \"resistant shoes\": 76,\r\n",
      "    \"strict dress code\": 252,\r\n",
      "    \"color hair\": 36,\r\n",
      "    \"unnatural hair colors\": NaN,\r\n",
      "    \"hat\": NaN,\r\n",
      "    \"attire\": NaN,\r\n",
      "    \"blue collar\": 37,\r\n",
      "    \"wear fake nails\": NaN,\r\n",
      "    \"hairnets\": NaN,\r\n",
      "    \"professional\": 16,\r\n",
      "    \"wear jeans\": 198,\r\n",
      "    \"dress pants\": 27,\r\n",
      "    \"uniform\": -1,\r\n",
      "    \"ponytail\": NaN,\r\n",
      "    \"uniform shirts\": 44,\r\n",
      "    \"shoes\": -1,\r\n",
      "    \"uniform policy\": 144,\r\n",
      "    \"dress shirts\": 46,\r\n",
      "    \"black slacks\": NaN,\r\n",
      "    \"fake nails\": 429,\r\n",
      "    \"skirts\": NaN,\r\n",
      "    \"uniforms\": NaN,\r\n",
      "    \"non slip shoes\": NaN,\r\n",
      "    \"red shirts\": NaN,\r\n",
      "    \"brown pants\": 264,\r\n",
      "    \"mustaches\": NaN,\r\n",
      "    \"shorts\": NaN,\r\n",
      "    \"hats\": NaN,\r\n",
      "    \"unnatural colored hair\": 142,\r\n",
      "    \"wear shorts\": NaN,\r\n",
      "    \"polo shirts\": NaN,\r\n",
      "    \"shirt\": NaN,\r\n",
      "    \"natural colored hair\": 47,\r\n",
      "    \"scrubs\": 123,\r\n",
      "    \"hair color\": -1,\r\n",
      "    \"casual dress code\": 82,\r\n",
      "    \"casual\": NaN,\r\n",
      "    \"colorful hair\": 417,\r\n",
      "    \"nose rings\": 55,\r\n",
      "    \"pants\": NaN,\r\n",
      "    \"shirts\": NaN,\r\n",
      "    \"piercings\": -1,\r\n",
      "    \"face tattoos\": 4,\r\n",
      "    \"hair colors\": NaN,\r\n",
      "    \"facial hair\": -1,\r\n",
      "    \"hair net\": NaN,\r\n",
      "    \"jewelry\": NaN,\r\n",
      "    \"natural colors\": 407,\r\n",
      "    \"black pants\": 283,\r\n",
      "    \"unnatural hair color\": NaN,\r\n",
      "    \"business casual\": -1,\r\n",
      "    \"lab coats\": NaN\r\n",
      "}\r\n",
      "MRR: 0.01826566099506606\r\n",
      "\r\n",
      "Concept: job_position / job position\r\n",
      "seeds: ['delivery driver', 'store manager', 'cashier', 'package handler', 'sales associate', 'barista', 'dishwasher']\r\n",
      "{\r\n",
      "    \"server\": NaN,\r\n",
      "    \"truck drivers\": NaN,\r\n",
      "    \"servers\": NaN,\r\n",
      "    \"shift leader\": 207,\r\n",
      "    \"cashier\": -1\r\n",
      "}\r\n",
      "MRR: 0.0012077294685990338\r\n",
      "\r\n",
      "Concept: pay_schedule / pay period\r\n",
      "seeds: ['weekly', 'biweekly', 'friday', 'saturday']\r\n",
      "{\r\n",
      "    \"weekly\": -1,\r\n",
      "    \"paid bi weekly\": 21,\r\n",
      "    \"paid weekly\": 8,\r\n",
      "    \"biweekly\": -1,\r\n",
      "    \"bi weekly\": 2,\r\n",
      "    \"friday\": -1,\r\n",
      "    \"tuesday\": NaN,\r\n",
      "    \"tuesdays\": NaN,\r\n",
      "    \"week\": NaN,\r\n",
      "    \"weeks\": NaN,\r\n",
      "    \"paid biweekly\": NaN,\r\n",
      "    \"fridays\": NaN\r\n",
      "}\r\n",
      "MRR: 0.07473544973544974\r\n",
      "\r\n",
      "Concept: benefits / benefits\r\n",
      "seeds: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\r\n",
      "{\r\n",
      "    \"sick leave\": -1,\r\n",
      "    \"schooling\": NaN,\r\n",
      "    \"pto\": NaN,\r\n",
      "    \"prescription drugs\": NaN,\r\n",
      "    \"health\": 133,\r\n",
      "    \"healthcare\": 63,\r\n",
      "    \"pension\": NaN,\r\n",
      "    \"tuition assistance\": 93,\r\n",
      "    \"discounts\": NaN,\r\n",
      "    \"retirement plan\": 105,\r\n",
      "    \"health plans\": NaN,\r\n",
      "    \"relocation\": NaN,\r\n",
      "    \"discount\": NaN,\r\n",
      "    \"health benefits\": 72,\r\n",
      "    \"health care\": 37,\r\n",
      "    \"paid vacations\": 54,\r\n",
      "    \"relocate\": NaN,\r\n",
      "    \"free lunch\": NaN,\r\n",
      "    \"vacations\": NaN,\r\n",
      "    \"retirement\": NaN,\r\n",
      "    \"sick days\": 22,\r\n",
      "    \"health coverage\": NaN,\r\n",
      "    \"401k\": -1,\r\n",
      "    \"401 k\": NaN,\r\n",
      "    \"health insurance\": -1,\r\n",
      "    \"401k plan\": NaN,\r\n",
      "    \"monthly bonus\": NaN,\r\n",
      "    \"breakfast\": NaN,\r\n",
      "    \"life insurance\": 8,\r\n",
      "    \"vacation\": NaN\r\n",
      "}\r\n",
      "MRR: 0.010131751498160353\r\n",
      "\r\n",
      "Concept: compensation / compensation\r\n",
      "seeds: ['base pay', 'stock options', 'benefits', 'overtime pay', 'bonus']\r\n",
      "{\r\n",
      "    \"benefits\": -1,\r\n",
      "    \"benfits\": NaN,\r\n",
      "    \"compensation\": NaN\r\n",
      "}\r\n",
      "MRR: 0.0\r\n",
      "\r\n",
      "Concept: payment_option / nan\r\n",
      "seeds: ['checks', 'direct deposit', 'prepaid card']\r\n",
      "{\r\n",
      "    \"direct deposits\": 19,\r\n",
      "    \"checks\": -1,\r\n",
      "    \"prepaid card\": -1,\r\n",
      "    \"paycheck\": NaN,\r\n",
      "    \"direct deposit\": -1,\r\n",
      "    \"paper checks\": NaN\r\n",
      "}\r\n",
      "MRR: 0.017543859649122806\r\n",
      "\r\n",
      "Concept: background_screening / background screening\r\n",
      "seeds: ['drug test', 'criminal background check', 'employment verification']\r\n",
      "{\r\n",
      "    \"credit checks\": 182,\r\n",
      "    \"random drug test\": 9,\r\n",
      "    \"saliva drug test\": 27,\r\n",
      "    \"follicle test\": NaN,\r\n",
      "    \"urine tests\": 71,\r\n",
      "    \"previous employment\": 78,\r\n",
      "    \"testing\": NaN,\r\n",
      "    \"mouth swab\": NaN,\r\n",
      "    \"criminal background checks\": NaN,\r\n",
      "    \"criminal record\": 50,\r\n",
      "    \"mouth swap\": NaN,\r\n",
      "    \"background checks\": 146,\r\n",
      "    \"cannabis\": NaN,\r\n",
      "    \"cheek swab\": NaN,\r\n",
      "    \"urine\": NaN,\r\n",
      "    \"blood test\": 3,\r\n",
      "    \"backround check\": NaN,\r\n",
      "    \"criminal background check\": -1,\r\n",
      "    \"swab test\": NaN,\r\n",
      "    \"saliva test\": 23,\r\n",
      "    \"test\": 48,\r\n",
      "    \"credit history\": 105,\r\n",
      "    \"hair follicle test\": NaN,\r\n",
      "    \"mouth swabs\": NaN,\r\n",
      "    \"drug screened\": NaN,\r\n",
      "    \"social security number\": 61,\r\n",
      "    \"drug tested\": NaN,\r\n",
      "    \"urine drug test\": 18,\r\n",
      "    \"urine test\": 6,\r\n",
      "    \"urine testing\": NaN,\r\n",
      "    \"criminal background\": 100,\r\n",
      "    \"saliva\": NaN,\r\n",
      "    \"dui\": NaN,\r\n",
      "    \"urine sample\": NaN,\r\n",
      "    \"hair sample\": NaN,\r\n",
      "    \"previous employer\": NaN,\r\n",
      "    \"credit score\": 195,\r\n",
      "    \"urine drug screen\": NaN,\r\n",
      "    \"random tests\": 177,\r\n",
      "    \"background check\": 36,\r\n",
      "    \"background report\": 119,\r\n",
      "    \"drug\": NaN,\r\n",
      "    \"drug screen\": NaN,\r\n",
      "    \"drug tests\": 14,\r\n",
      "    \"previous employers\": NaN,\r\n",
      "    \"criminal records\": 99,\r\n",
      "    \"drugs\": NaN,\r\n",
      "    \"previous jobs\": NaN,\r\n",
      "    \"credit report\": 40,\r\n",
      "    \"random drug testing\": 106,\r\n",
      "    \"alcohol\": NaN,\r\n",
      "    \"drug test\": -1,\r\n",
      "    \"screen\": NaN,\r\n",
      "    \"criminal backgrounds\": NaN,\r\n",
      "    \"criminal history\": 218,\r\n",
      "    \"drugged tested\": NaN,\r\n",
      "    \"backround\": NaN,\r\n",
      "    \"drug text\": NaN,\r\n",
      "    \"credit check\": 35,\r\n",
      "    \"backround checks\": NaN,\r\n",
      "    \"drug testing\": 44,\r\n",
      "    \"drug screens\": NaN,\r\n",
      "    \"previously worked\": NaN,\r\n",
      "    \"screening process\": NaN,\r\n",
      "    \"social security\": NaN,\r\n",
      "    \"driving record\": 94,\r\n",
      "    \"backgrounds\": NaN,\r\n",
      "    \"random drug tests\": 57,\r\n",
      "    \"screening\": NaN,\r\n",
      "    \"drug screening\": 134,\r\n",
      "    \"social media\": NaN,\r\n",
      "    \"mouth\": NaN,\r\n",
      "    \"pre employment drug screening\": NaN\r\n",
      "}\r\n",
      "MRR: 0.015741333766030093\r\n",
      "\r\n",
      "Concept: person / nan\r\n",
      "seeds: ['felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'misdemeanor', 'pregnant', 'students', 'seniors']\r\n",
      "{\r\n",
      "    \"pregnant\": -1,\r\n",
      "    \"felony\": NaN,\r\n",
      "    \"seniors\": -1,\r\n",
      "    \"high school students\": NaN,\r\n",
      "    \"high schoolers\": -1,\r\n",
      "    \"disabled\": -1,\r\n",
      "    \"felony record\": NaN,\r\n",
      "    \"felonys\": NaN,\r\n",
      "    \"school students\": NaN,\r\n",
      "    \"disabilities\": NaN,\r\n",
      "    \"misdemeanor charges\": NaN,\r\n",
      "    \"ex felons\": NaN,\r\n",
      "    \"high school graduate\": NaN,\r\n",
      "    \"high school\": NaN,\r\n",
      "    \"pregnant women\": 361,\r\n",
      "    \"schoolers\": NaN,\r\n",
      "    \"seniority\": NaN,\r\n",
      "    \"misdemeanor theft\": NaN,\r\n",
      "    \"sex offenders\": 6,\r\n",
      "    \"felonies\": NaN,\r\n",
      "    \"felons\": -1,\r\n",
      "    \"senior citizens\": 30,\r\n",
      "    \"misdemeanor\": -1,\r\n",
      "    \"criminals\": -1,\r\n",
      "    \"convicted felons\": NaN\r\n",
      "}\r\n",
      "MRR: 0.01126500461680517\r\n",
      "\r\n",
      "Concept: hire_prerequisite / qualification\r\n",
      "seeds: ['hiring age', 'bachelors degree', 'prior experience', 'working permit', 'heavy lifting']\r\n",
      "{\r\n",
      "    \"gpa\": NaN,\r\n",
      "    \"college degree\": 14,\r\n",
      "    \"ged\": NaN,\r\n",
      "    \"workers permit\": 179,\r\n",
      "    \"working permit\": -1,\r\n",
      "    \"diploma\": 205,\r\n",
      "    \"high school diploma\": 30,\r\n",
      "    \"hs diploma\": 243,\r\n",
      "    \"degrees\": NaN,\r\n",
      "    \"birth certificate\": 192,\r\n",
      "    \"high school education\": 77,\r\n",
      "    \"bachelor degree\": 6\r\n",
      "}\r\n",
      "MRR: 0.02765488954960228\r\n",
      "\r\n",
      "Concept: shifts / work shift\r\n",
      "seeds: ['night shift', 'dinner shift', 'early morning shift', '8 hour shift']\r\n",
      "{\r\n",
      "    \"3rd shift\": 31,\r\n",
      "    \"12 hour shifts\": 50,\r\n",
      "    \"night shifts\": 4,\r\n",
      "    \"open 24 hours\": NaN,\r\n",
      "    \"weekend shift\": 2\r\n",
      "}\r\n",
      "MRR: 0.1604516129032258\r\n",
      "\r\n",
      "Concept: schedule / nan\r\n",
      "seeds: ['christmas eve', 'early morning', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend']\r\n",
      "{\r\n",
      "    \"saturday\": -1,\r\n",
      "    \"saturdays\": NaN,\r\n",
      "    \"federal holidays\": NaN,\r\n",
      "    \"early morning\": -1,\r\n",
      "    \"open 7 days\": NaN,\r\n",
      "    \"christmas eve\": -1,\r\n",
      "    \"weekend\": -1,\r\n",
      "    \"sunday\": -1,\r\n",
      "    \"weekends\": NaN,\r\n",
      "    \"hoildays\": NaN\r\n",
      "}\r\n",
      "MRR: 0.0\r\n",
      "\r\n",
      "Concept: employee_type / nan\r\n",
      "seeds: ['full time', 'part time', 'seasonal']\r\n",
      "{\r\n",
      "    \"seasonal positions\": NaN,\r\n",
      "    \"ft\": NaN,\r\n",
      "    \"season\": NaN,\r\n",
      "    \"seasonal workers\": NaN,\r\n",
      "    \"seasons\": NaN,\r\n",
      "    \"seasonal employees\": 183,\r\n",
      "    \"fulltime\": NaN,\r\n",
      "    \"seasonal\": -1,\r\n",
      "    \"seasonals\": NaN\r\n",
      "}\r\n",
      "MRR: 0.0006830601092896175\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: onboarding_steps / onboarding process steps\r\n",
      "seeds: ['orientation', 'introduction', 'workstation', 'training', 'team lunch']\r\n",
      "{\r\n",
      "    \"training classes\": 179,\r\n",
      "    \"training program\": 54,\r\n",
      "    \"training\": -1,\r\n",
      "    \"orientation\": -1\r\n",
      "}\r\n",
      "MRR: 0.012052555348644734\r\n",
      "\r\n",
      "--- Summary ---\r\n",
      "{\r\n",
      "  \"company\": 0.0005168180947036233,\r\n",
      "  \"dress_code\": 0.01826566099506606,\r\n",
      "  \"job_position\": 0.0012077294685990338,\r\n",
      "  \"pay_schedule\": 0.07473544973544974,\r\n",
      "  \"benefits\": 0.010131751498160353,\r\n",
      "  \"compensation\": 0.0,\r\n",
      "  \"payment_option\": 0.017543859649122806,\r\n",
      "  \"background_screening\": 0.015741333766030093,\r\n",
      "  \"person\": 0.01126500461680517,\r\n",
      "  \"hire_prerequisite\": 0.02765488954960228,\r\n",
      "  \"shifts\": 0.1604516129032258,\r\n",
      "  \"schedule\": 0.0,\r\n",
      "  \"employee_type\": 0.0006830601092896175,\r\n",
      "  \"onboarding_steps\": 0.012052555348644734\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "!python eval_entities.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/concept_corr_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: company / company\n",
      "seeds: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\n",
      "{\n",
      "    \"geico\": 30,\n",
      "    \"goodwill industries\": NaN,\n",
      "    \"dunkin' donuts\": NaN,\n",
      "    \"wells fargo\": 50,\n",
      "    \"sonic drive-in\": NaN,\n",
      "    \"domino's\": NaN,\n",
      "    \"pizza hut\": 9,\n",
      "    \"frito-lay\": NaN,\n",
      "    \"marshalls\": 35,\n",
      "    \"fedex ground\": NaN,\n",
      "    \"whole foods market\": NaN,\n",
      "    \"burger king\": 32,\n",
      "    \"target\": -1,\n",
      "    \"tim hortons\": NaN,\n",
      "    \"t.j. maxx\": NaN,\n",
      "    \"cracker barrel\": 20,\n",
      "    \"subways\": NaN,\n",
      "    \"ulta\": NaN,\n",
      "    \"olive garden\": 12,\n",
      "    \"kroger\": 7,\n",
      "    \"spectrum\": 18,\n",
      "    \"verizon\": 38,\n",
      "    \"united states postal service\": NaN,\n",
      "    \"frito\": NaN,\n",
      "    \"taco bell\": 37,\n",
      "    \"mcdonald\": NaN,\n",
      "    \"dick's sporting goods\": NaN,\n",
      "    \"training\": 92,\n",
      "    \"ups\": NaN,\n",
      "    \"panera bread\": 11,\n",
      "    \"lowe's\": NaN,\n",
      "    \"walgreens\": 6,\n",
      "    \"primark\": NaN,\n",
      "    \"mcdonalds\": 14,\n",
      "    \"foot locker\": NaN,\n",
      "    \"applebee's\": NaN,\n",
      "    \"chipotle\": 34,\n",
      "    \"dollar tree\": 17,\n",
      "    \"t-mobile\": NaN,\n",
      "    \"costco wholesale\": NaN,\n",
      "    \"cvs\": 25,\n",
      "    \"little caesars\": 74,\n",
      "    \"best buy\": 13,\n",
      "    \"kohl's\": NaN,\n",
      "    \"allied universal security services, systems and solutions\": NaN,\n",
      "    \"marriott international, inc.\": NaN,\n",
      "    \"chilis\": NaN,\n",
      "    \"cvs health\": NaN,\n",
      "    \"costco\": 2,\n",
      "    \"tj maxx\": 49,\n",
      "    \"enterprise holdings\": NaN,\n",
      "    \"at&t\": 41,\n",
      "    \"electric\": NaN,\n",
      "    \"fedex\": 36,\n",
      "    \"whataburger\": NaN,\n",
      "    \"mcdonald's\": NaN,\n",
      "    \"the wendy's company\": NaN,\n",
      "    \"panera\": NaN,\n",
      "    \"planet fitness\": 65,\n",
      "    \"sam's club\": NaN,\n",
      "    \"menards\": 24,\n",
      "    \"walmart\": -1,\n",
      "    \"teleperformance\": NaN,\n",
      "    \"alorica\": NaN,\n",
      "    \"amazon\": -1,\n",
      "    \"macy's\": NaN,\n",
      "    \"kfc\": 19,\n",
      "    \"kroger stores\": NaN,\n",
      "    \"publix\": 8,\n",
      "    \"american eagle outfitters\": NaN,\n",
      "    \"starbucks\": 10,\n",
      "    \"burlington stores\": NaN,\n",
      "    \"subway\": -1,\n",
      "    \"sitel\": 42,\n",
      "    \"home depot\": 5,\n",
      "    \"ross dress for less\": NaN,\n",
      "    \"hobby lobby\": 73,\n",
      "    \"aldi\": NaN,\n",
      "    \"concentrix\": NaN,\n",
      "    \"chipotle mexican grill\": NaN,\n",
      "    \"lowes\": NaN,\n",
      "    \"instacart\": NaN,\n",
      "    \"old navy\": 16,\n",
      "    \"dunkin donuts\": 43,\n",
      "    \"petsmart\": 40,\n",
      "    \"safeway\": 22,\n",
      "    \"the home depot\": NaN,\n",
      "    \"quiktrip\": NaN,\n",
      "    \"doordash\": NaN,\n",
      "    \"wendys\": NaN,\n",
      "    \"chick-fil-a\": NaN,\n",
      "    \"pepsi\": 39,\n",
      "    \"chili's\": NaN,\n",
      "    \"victoria's secret\": NaN,\n",
      "    \"dollar general\": 3,\n",
      "    \"family dollar\": 15,\n",
      "    \"pepsico\": 53,\n",
      "    \"barnes & noble\": NaN,\n",
      "    \"jcpenney\": 21,\n",
      "    \"g4s\": 44,\n",
      "    \"heb\": NaN,\n",
      "    \"amazon.com\": NaN,\n",
      "    \"ihop\": 26,\n",
      "    \"frito lay\": 33,\n",
      "    \"dd\": 84\n",
      "}\n",
      "MRR: 0.030340620401210584\n",
      "\n",
      "Concept: dress_code / dress code\n",
      "seeds: ['business casual', 'uniform', 'hair color', 'tattoos', 'facial hair', 'shoes', 'piercings']\n",
      "{\n",
      "    \"hats\": NaN,\n",
      "    \"red shirts\": NaN,\n",
      "    \"unnatural hair colors\": NaN,\n",
      "    \"wear jeans\": NaN,\n",
      "    \"ponytail\": NaN,\n",
      "    \"shirts\": NaN,\n",
      "    \"unnatural colored hair\": NaN,\n",
      "    \"uniform policy\": 23,\n",
      "    \"face tattoos\": NaN,\n",
      "    \"shorts\": NaN,\n",
      "    \"shirt\": NaN,\n",
      "    \"black slacks\": NaN,\n",
      "    \"hair color\": -1,\n",
      "    \"mustaches\": NaN,\n",
      "    \"black jeans\": NaN,\n",
      "    \"jewelry\": NaN,\n",
      "    \"hair net\": NaN,\n",
      "    \"wear fake nails\": NaN,\n",
      "    \"dress shirts\": NaN,\n",
      "    \"hairnets\": NaN,\n",
      "    \"nose rings\": 9,\n",
      "    \"business casual\": -1,\n",
      "    \"attire\": NaN,\n",
      "    \"uniform shirts\": NaN,\n",
      "    \"non slip shoes\": NaN,\n",
      "    \"professional\": 21,\n",
      "    \"hair colors\": NaN,\n",
      "    \"strict dress code\": 48,\n",
      "    \"dress pants\": NaN,\n",
      "    \"uniforms\": NaN,\n",
      "    \"scrubs\": 58,\n",
      "    \"facial piercings\": 3,\n",
      "    \"lab coats\": NaN,\n",
      "    \"casual dress code\": NaN,\n",
      "    \"hat\": NaN,\n",
      "    \"brown pants\": NaN,\n",
      "    \"pants\": NaN,\n",
      "    \"jeans\": NaN,\n",
      "    \"natural colored hair\": NaN,\n",
      "    \"piercings\": -1,\n",
      "    \"shoes\": -1,\n",
      "    \"color hair\": 35,\n",
      "    \"fake nails\": 8,\n",
      "    \"colorful hair\": 62,\n",
      "    \"unnatural hair color\": NaN,\n",
      "    \"uniform\": -1,\n",
      "    \"blue collar\": NaN,\n",
      "    \"wear shorts\": NaN,\n",
      "    \"polo shirts\": NaN,\n",
      "    \"facial hair\": -1,\n",
      "    \"casual\": NaN,\n",
      "    \"natural colors\": 78,\n",
      "    \"skirts\": NaN,\n",
      "    \"resistant shoes\": NaN,\n",
      "    \"black pants\": NaN\n",
      "}\n",
      "MRR: 0.015431376310749822\n",
      "\n",
      "Concept: job_position / job position\n",
      "seeds: ['delivery driver', 'store manager', 'cashier', 'package handler', 'sales associate', 'barista', 'dishwasher']\n",
      "{\n",
      "    \"cashier\": -1,\n",
      "    \"servers\": NaN,\n",
      "    \"shift leader\": 10,\n",
      "    \"truck drivers\": NaN,\n",
      "    \"server\": NaN\n",
      "}\n",
      "MRR: 0.025\n",
      "\n",
      "Concept: pay_schedule / pay period\n",
      "seeds: ['weekly', 'biweekly', 'friday', 'saturday']\n",
      "{\n",
      "    \"tuesdays\": NaN,\n",
      "    \"week\": NaN,\n",
      "    \"paid biweekly\": NaN,\n",
      "    \"bi weekly\": 3,\n",
      "    \"tuesday\": NaN,\n",
      "    \"paid weekly\": NaN,\n",
      "    \"weeks\": NaN,\n",
      "    \"friday\": -1,\n",
      "    \"paid bi weekly\": NaN,\n",
      "    \"fridays\": NaN,\n",
      "    \"biweekly\": -1,\n",
      "    \"weekly\": -1\n",
      "}\n",
      "MRR: 0.037037037037037035\n",
      "\n",
      "Concept: benefits / benefits\n",
      "seeds: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\n",
      "{\n",
      "    \"health insurance\": -1,\n",
      "    \"tuition assistance\": NaN,\n",
      "    \"health care\": 2,\n",
      "    \"health\": 7,\n",
      "    \"retirement\": NaN,\n",
      "    \"health coverage\": NaN,\n",
      "    \"retirement plan\": 74,\n",
      "    \"relocation\": NaN,\n",
      "    \"sick days\": 10,\n",
      "    \"monthly bonus\": NaN,\n",
      "    \"paid vacations\": 65,\n",
      "    \"vacation\": NaN,\n",
      "    \"401k\": -1,\n",
      "    \"life insurance\": 8,\n",
      "    \"relocate\": NaN,\n",
      "    \"schooling\": NaN,\n",
      "    \"pto\": NaN,\n",
      "    \"health benefits\": NaN,\n",
      "    \"breakfast\": NaN,\n",
      "    \"pension\": 63,\n",
      "    \"401 k\": NaN,\n",
      "    \"free lunch\": NaN,\n",
      "    \"sick leave\": -1,\n",
      "    \"prescription drugs\": NaN,\n",
      "    \"discounts\": NaN,\n",
      "    \"discount\": NaN,\n",
      "    \"vacations\": NaN,\n",
      "    \"healthcare\": 1,\n",
      "    \"401k plan\": NaN,\n",
      "    \"health plans\": NaN\n",
      "}\n",
      "MRR: 0.0708380847269736\n",
      "\n",
      "Concept: compensation / compensation\n",
      "seeds: ['base pay', 'stock options', 'benefits', 'overtime pay', 'bonus']\n",
      "{\n",
      "    \"benfits\": NaN,\n",
      "    \"benefits\": -1,\n",
      "    \"compensation\": NaN\n",
      "}\n",
      "MRR: 0.0\n",
      "\n",
      "Concept: payment_option / nan\n",
      "seeds: ['checks', 'direct deposit', 'prepaid card']\n",
      "{\n",
      "    \"prepaid card\": -1,\n",
      "    \"direct deposits\": 13,\n",
      "    \"checks\": -1,\n",
      "    \"paycheck\": 14,\n",
      "    \"direct deposit\": -1,\n",
      "    \"paper checks\": NaN\n",
      "}\n",
      "MRR: 0.04945054945054945\n",
      "\n",
      "Concept: background_screening / background screening\n",
      "seeds: ['drug test', 'criminal background check', 'employment verification']\n",
      "{\n",
      "    \"drug tests\": 82,\n",
      "    \"mouth swap\": NaN,\n",
      "    \"hair sample\": NaN,\n",
      "    \"criminal background\": 8,\n",
      "    \"criminal backgrounds\": NaN,\n",
      "    \"random drug testing\": NaN,\n",
      "    \"credit check\": 18,\n",
      "    \"backround checks\": NaN,\n",
      "    \"cheek swab\": NaN,\n",
      "    \"background checks\": NaN,\n",
      "    \"credit score\": NaN,\n",
      "    \"hair follicle test\": NaN,\n",
      "    \"saliva\": NaN,\n",
      "    \"saliva drug test\": NaN,\n",
      "    \"saliva test\": NaN,\n",
      "    \"social media\": 54,\n",
      "    \"social security number\": 50,\n",
      "    \"drug screening\": NaN,\n",
      "    \"drug screens\": NaN,\n",
      "    \"swab test\": NaN,\n",
      "    \"urine drug screen\": NaN,\n",
      "    \"credit history\": 89,\n",
      "    \"criminal records\": NaN,\n",
      "    \"urine drug test\": NaN,\n",
      "    \"drugs\": 40,\n",
      "    \"drug screen\": NaN,\n",
      "    \"previous employer\": NaN,\n",
      "    \"previous employment\": 34,\n",
      "    \"screening\": NaN,\n",
      "    \"drug text\": NaN,\n",
      "    \"drug test\": -1,\n",
      "    \"mouth swab\": NaN,\n",
      "    \"blood test\": NaN,\n",
      "    \"background report\": NaN,\n",
      "    \"urine\": NaN,\n",
      "    \"previous employers\": NaN,\n",
      "    \"credit report\": 63,\n",
      "    \"backround check\": NaN,\n",
      "    \"mouth\": NaN,\n",
      "    \"dui\": 29,\n",
      "    \"previously worked\": NaN,\n",
      "    \"urine sample\": NaN,\n",
      "    \"background check\": 14,\n",
      "    \"screen\": NaN,\n",
      "    \"test\": 11,\n",
      "    \"drug tested\": NaN,\n",
      "    \"follicle test\": NaN,\n",
      "    \"urine tests\": NaN,\n",
      "    \"alcohol\": NaN,\n",
      "    \"mouth swabs\": NaN,\n",
      "    \"social security\": NaN,\n",
      "    \"urine testing\": NaN,\n",
      "    \"criminal record\": 10,\n",
      "    \"previous jobs\": NaN,\n",
      "    \"drugged tested\": NaN,\n",
      "    \"criminal background checks\": NaN,\n",
      "    \"testing\": NaN,\n",
      "    \"screening process\": NaN,\n",
      "    \"urine test\": NaN,\n",
      "    \"credit checks\": NaN,\n",
      "    \"drug testing\": NaN,\n",
      "    \"random drug tests\": 75,\n",
      "    \"cannabis\": NaN,\n",
      "    \"drug screened\": NaN,\n",
      "    \"criminal history\": 1,\n",
      "    \"random tests\": NaN,\n",
      "    \"criminal background check\": -1,\n",
      "    \"driving record\": 6,\n",
      "    \"random drug test\": 80,\n",
      "    \"pre employment drug screening\": NaN,\n",
      "    \"drug\": 77,\n",
      "    \"backgrounds\": NaN,\n",
      "    \"backround\": NaN\n",
      "}\n",
      "MRR: 0.025564751628249813\n",
      "\n",
      "Concept: person / nan\n",
      "seeds: ['felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'misdemeanor', 'pregnant', 'students', 'seniors']\n",
      "{\n",
      "    \"disabilities\": NaN,\n",
      "    \"high schoolers\": -1,\n",
      "    \"felony record\": NaN,\n",
      "    \"high school\": 55,\n",
      "    \"felonies\": 44,\n",
      "    \"pregnant women\": 25,\n",
      "    \"seniors\": -1,\n",
      "    \"disabled\": -1,\n",
      "    \"misdemeanor theft\": NaN,\n",
      "    \"pregnant\": -1,\n",
      "    \"sex offenders\": NaN,\n",
      "    \"convicted felons\": NaN,\n",
      "    \"felonys\": NaN,\n",
      "    \"high school graduate\": NaN,\n",
      "    \"schoolers\": NaN,\n",
      "    \"seniority\": NaN,\n",
      "    \"misdemeanor\": -1,\n",
      "    \"felons\": -1,\n",
      "    \"criminals\": -1,\n",
      "    \"felony\": NaN,\n",
      "    \"high school students\": NaN,\n",
      "    \"senior citizens\": 57,\n",
      "    \"misdemeanor charges\": NaN,\n",
      "    \"ex felons\": 50,\n",
      "    \"school students\": NaN\n",
      "}\n",
      "MRR: 0.006580719475456317\n",
      "\n",
      "Concept: hire_prerequisite / qualification\n",
      "seeds: ['hiring age', 'bachelors degree', 'prior experience', 'working permit', 'heavy lifting']\n",
      "{\n",
      "    \"birth certificate\": NaN,\n",
      "    \"working permit\": -1,\n",
      "    \"high school diploma\": NaN,\n",
      "    \"gpa\": NaN,\n",
      "    \"bachelor degree\": NaN,\n",
      "    \"degrees\": NaN,\n",
      "    \"diploma\": 76,\n",
      "    \"high school education\": NaN,\n",
      "    \"college degree\": 97,\n",
      "    \"hs diploma\": NaN,\n",
      "    \"workers permit\": NaN,\n",
      "    \"ged\": NaN\n",
      "}\n",
      "MRR: 0.0021333793715779606\n",
      "\n",
      "Concept: shifts / work shift\n",
      "seeds: ['night shift', 'dinner shift', 'early morning shift', '8 hour shift']\n",
      "{\n",
      "    \"weekend shift\": 12,\n",
      "    \"12 hour shifts\": 17,\n",
      "    \"3rd shift\": 37,\n",
      "    \"night shifts\": 10,\n",
      "    \"open 24 hours\": NaN\n",
      "}\n",
      "MRR: 0.05383677795442501\n",
      "\n",
      "Concept: schedule / nan\n",
      "seeds: ['christmas eve', 'early morning', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend']\n",
      "{\n",
      "    \"open 7 days\": NaN,\n",
      "    \"early morning\": -1,\n",
      "    \"hoildays\": NaN,\n",
      "    \"saturday\": -1,\n",
      "    \"christmas eve\": -1,\n",
      "    \"saturdays\": NaN,\n",
      "    \"sunday\": -1,\n",
      "    \"federal holidays\": NaN,\n",
      "    \"weekends\": NaN,\n",
      "    \"weekend\": -1\n",
      "}\n",
      "MRR: 0.0\n",
      "\n",
      "Concept: employee_type / nan\n",
      "seeds: ['full time', 'part time', 'seasonal']\n",
      "{\n",
      "    \"seasonal\": -1,\n",
      "    \"seasons\": NaN,\n",
      "    \"seasonals\": NaN,\n",
      "    \"seasonal employees\": 10,\n",
      "    \"ft\": NaN,\n",
      "    \"fulltime\": NaN,\n",
      "    \"season\": NaN,\n",
      "    \"seasonal workers\": 64,\n",
      "    \"seasonal positions\": NaN\n",
      "}\n",
      "MRR: 0.014453125\n",
      "\n",
      "Concept: onboarding_steps / onboarding process steps\n",
      "seeds: ['orientation', 'introduction', 'workstation', 'training', 'team lunch']\n",
      "{\n",
      "    \"training\": -1,\n",
      "    \"training classes\": 38,\n",
      "    \"orientation\": -1,\n",
      "    \"training program\": NaN\n",
      "}\n",
      "MRR: 0.013157894736842105\n",
      "\n",
      "--- Summary ---\n",
      "{\n",
      "  \"company\": 0.030340620401210584,\n",
      "  \"dress_code\": 0.015431376310749822,\n",
      "  \"job_position\": 0.025,\n",
      "  \"pay_schedule\": 0.037037037037037035,\n",
      "  \"benefits\": 0.0708380847269736,\n",
      "  \"compensation\": 0.0,\n",
      "  \"payment_option\": 0.04945054945054945,\n",
      "  \"background_screening\": 0.025564751628249813,\n",
      "  \"person\": 0.006580719475456317,\n",
      "  \"hire_prerequisite\": 0.0021333793715779606,\n",
      "  \"shifts\": 0.05383677795442501,\n",
      "  \"schedule\": 0.0,\n",
      "  \"employee_type\": 0.014453125,\n",
      "  \"onboarding_steps\": 0.013157894736842105\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Use script\n",
    "!python eval_entities.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/concept_knn_100.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM correlation-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences_with_company.json')\n",
    "\n",
    "# with open(corpus_path, 'r') as f:\n",
    "#     sent_dicts = [json.loads(l) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lm_probe = LMProbe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_file_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sent_segmentation.txt')\n",
    "# ent_freq, dedup_context = get_masked_contexts(input_file_path)\n",
    "\n",
    "corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences_with_company.json')\n",
    "embed_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')\n",
    "\n",
    "entities, dedup_context = get_masked_contexts(corpus_path, embed_num_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8064, 8064, 7921)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities), len(set(entities)), len(dedup_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2372, 4132, 1144, 314, 55]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ents_tokenized = [tuple(lm_probe.tokenizer.tokenize(e)) for e in entities]\n",
    "all_ents_tokenized = list(set(all_ents_tokenized))\n",
    "[sum([len(e_t) == _l for e_t in all_ents_tokenized]) for _l in (1,2,3,4,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: keep rank in output\n",
    "\n",
    "def entity_expansion_corr(seed_concepts_path=seed_aligned_concepts_path,\n",
    "                          all_contexts=dedup_context, \n",
    "                          all_ents_tokenized=all_ents_tokenized, \n",
    "                          lm_probe=lm_probe,\n",
    "                          max_allowed_ngrams=3,\n",
    "                          max_contexts=50,\n",
    "                          top_k=100):\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "        \n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    \n",
    "#     if contexts is None:\n",
    "#         try:\n",
    "#             contexts = dedup_context[entity]\n",
    "#         except KeyError:\n",
    "#             print(f'\"{entity}\" not an extracted entity!')\n",
    "#             return None\n",
    "\n",
    "    _out_records = []\n",
    "\n",
    "    for i, (a_concept, u_concept, gnrl, seed_instances) in tqdm(seed_concepts_df.iterrows(), total=seed_concepts_df.shape[0]):\n",
    "        _expand_set = set()\n",
    "        _expand_records = []\n",
    "        \n",
    "        for _inst in seed_instances:\n",
    "            print(f'{a_concept} :: {_inst}')\n",
    "            try:\n",
    "                contexts = all_contexts[_inst]\n",
    "            except KeyError:\n",
    "                print(f'\"{_inst}\" not an extracted entity!')\n",
    "                continue\n",
    "            if len(contexts) < 2:\n",
    "                print(f'\"{_inst}\" only have {len(contexts)} context')\n",
    "                continue\n",
    "\n",
    "            _entity_pieces = lm_probe.tokenizer.tokenize(_inst)\n",
    "            if len(_entity_pieces) > max_allowed_ngrams:\n",
    "                print(f'{_entity_pieces} too many word pieces (max {max_allowed_ngrams})')\n",
    "                continue\n",
    "\n",
    "            entity2probs = defaultdict(list)\n",
    "\n",
    "            for _context in contexts[:max_contexts]:\n",
    "                for n_grams in range(1, max_allowed_ngrams+1):\n",
    "                    _ctxt = _context.replace('[MASK]', '[MASK]' + ' [MASK]' * (n_grams-1))\n",
    "                    _ctxt = '[CLS] ' + _ctxt + ' [SEP]'\n",
    "                    _cands = [e_t for e_t in all_ents_tokenized if len(e_t) == n_grams]\n",
    "                    _cand_scores = lm_probe.score_candidates(_ctxt, _cands)\n",
    "\n",
    "                    for _d in _cand_scores:\n",
    "                        _c = ' '.join(_d['cand']).replace(' ##', '')\n",
    "                        _s = _d['score']\n",
    "                        entity2probs[_c].append(_s)\n",
    "\n",
    "        #     print('entity2probs:', len(entity2probs), len(entity2probs[entity]))\n",
    "            for _e, _ss in entity2probs.items():\n",
    "                assert len(_ss) == len(entity2probs[_inst]), \\\n",
    "                    f'entity: {_e} | {lm_probe.tokenizer.tokenize(_e)}\\n\\\n",
    "                    len(_ss) = {len(_ss)}\\n\\\n",
    "                    len(entity2probs[\"{entity}\"]) = {len(entity2probs[entity])}'\n",
    "\n",
    "            _target_ss = entity2probs[_inst]\n",
    "            _target_ss = _target_ss / np.sum(_target_ss)\n",
    "\n",
    "        #     print(_target_ss.shape, _target_ss)\n",
    "\n",
    "            mean_l = [(_e, np.mean(_ss)) for _e, _ss in entity2probs.items()]\n",
    "            mean_l.sort(key=lambda p : p[-1], reverse=True)\n",
    "            kl_l = [(_e, entropy(_target_ss, _ss)) for _e, _ss in entity2probs.items()]\n",
    "            kl_l.sort(key=lambda p : p[-1], reverse=False)\n",
    "            pearson_l = [(_e, pearsonr(_target_ss, _ss)[0]) for _e, _ss in entity2probs.items()]\n",
    "            pearson_l.sort(key=lambda p : p[-1], reverse=True)\n",
    "\n",
    "            entity2ranks = defaultdict(list)\n",
    "            entity2scores = defaultdict(dict)\n",
    "            for i, (_e, _s) in enumerate(mean_l):\n",
    "                entity2ranks[_e].append(i)\n",
    "                entity2scores[_e][\"mean\"] = _s\n",
    "            for i, (_e, _s) in enumerate(kl_l):\n",
    "                entity2ranks[_e].append(i)\n",
    "                entity2scores[_e][\"kl\"] = _s\n",
    "            for i, (_e, _s) in enumerate(pearson_l):\n",
    "                entity2ranks[_e].append(i)\n",
    "                entity2scores[_e][\"pearson\"] = _s\n",
    "            # To simile top-k set intersection, keep the highest rank of _e among each criteria\n",
    "            entity_overall_ranks = [(_e, max(_ranks)) for _e, _ranks in entity2ranks.items()]\n",
    "            entity_overall_ranks.sort(key=lambda p : p[-1])\n",
    "            entity_overall_ranks_dict = dict(entity_overall_ranks)\n",
    "#             # Now, the top-k is for the final selection, not for each criteria\n",
    "            sel_entities = [_e for _e, _ in entity_overall_ranks[:top_k]]\n",
    "\n",
    "#             ints_mean_l = [p for p in mean_l if p[0] in sel_entities]\n",
    "#             ints_kl_l = [p for p in kl_l if p[0] in sel_entities]\n",
    "#             ints_pearson_l = [p for p in pearson_l if p[0] in sel_entities]\n",
    "\n",
    "#             return {\n",
    "#                 \"entity2probs\": entity2probs,\n",
    "#                 \"mean_l\": mean_l,\n",
    "#                 \"kl_l\": kl_l,\n",
    "#                 \"pearson_l\": pearson_l,\n",
    "#                 \"sel_entities\": sel_entities,\n",
    "#                 \"ints_mean_l\": ints_mean_l,\n",
    "#                 \"ints_kl_l\": ints_kl_l,\n",
    "#                 \"ints_pearson_l\": ints_pearson_l,\n",
    "#             }\n",
    "\n",
    "            for _e in sel_entities:\n",
    "                if (_e in _expand_set) or (_e in seed_instances):\n",
    "                    continue\n",
    "                _expand_set.add(_e)\n",
    "                _d = dict(entity2scores[_e])\n",
    "                _d['max_rank'] = entity_overall_ranks_dict[_e]\n",
    "                _expand_records.append((_e, _d))\n",
    "\n",
    "#         for _inst in seed_instances:\n",
    "#             _expand_set.discard(_inst)\n",
    "\n",
    "        for _e, _d in _expand_records:\n",
    "            _out_d = dict(_d)\n",
    "            _out_d['concept'] = a_concept\n",
    "            _out_d['neighbor'] = _e\n",
    "            _out_records.append(_out_d)\n",
    "\n",
    "    return _out_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full run & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_expansion_out_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_corr_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_expansion_corr(max_contexts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]company :: walmart\n",
      "company :: amazon\n",
      "company :: subway\n",
      "company :: microsoft\n",
      "company :: target\n",
      "  7%|███▏                                        | 1/14 [01:13<16:01, 73.94s/it]dress_code :: business casual\n",
      "dress_code :: uniform\n",
      "dress_code :: hair color\n",
      "dress_code :: tattoos\n",
      "dress_code :: facial hair\n",
      "dress_code :: shoes\n",
      "dress_code :: piercings\n",
      " 14%|██████▎                                     | 2/14 [03:16<17:43, 88.60s/it]job_position :: delivery driver\n",
      "job_position :: store manager\n",
      "job_position :: cashier\n",
      "job_position :: package handler\n",
      "job_position :: sales associate\n",
      "job_position :: barista\n",
      "job_position :: dishwasher\n",
      " 21%|█████████▍                                  | 3/14 [05:17<18:00, 98.21s/it]pay_schedule :: weekly\n",
      "pay_schedule :: biweekly\n",
      "pay_schedule :: friday\n",
      "pay_schedule :: saturday\n",
      " 29%|████████████▌                               | 4/14 [06:27<14:56, 89.66s/it]benefits :: health insurance\n",
      "benefits :: flexible schedule\n",
      "benefits :: 401k\n",
      "benefits :: paid vacation\n",
      "benefits :: sick leave\n",
      "benefits :: vision insurance\n",
      " 36%|███████████████▋                            | 5/14 [08:02<13:42, 91.37s/it]compensation :: base pay\n",
      "compensation :: stock options\n",
      "compensation :: benefits\n",
      "compensation :: overtime pay\n",
      "compensation :: bonus\n",
      " 43%|██████████████████▊                         | 6/14 [09:24<11:49, 88.67s/it]payment_option :: checks\n",
      "payment_option :: direct deposit\n",
      "payment_option :: prepaid card\n",
      " 50%|██████████████████████                      | 7/14 [10:06<08:41, 74.55s/it]background_screening :: drug test\n",
      "background_screening :: criminal background check\n",
      "background_screening :: employment verification\n",
      " 57%|█████████████████████████▏                  | 8/14 [10:44<06:20, 63.48s/it]person :: felons\n",
      "person :: criminals\n",
      "person :: disabled\n",
      "person :: drug addicts\n",
      "person :: high schoolers\n",
      "person :: misdemeanor\n",
      "['mis', '##de', '##me', '##anor'] too many word pieces (max 3)\n",
      "person :: pregnant\n",
      "person :: students\n",
      "person :: seniors\n",
      " 64%|████████████████████████████▎               | 9/14 [12:34<06:27, 77.49s/it]hire_prerequisite :: hiring age\n",
      "hire_prerequisite :: bachelors degree\n",
      "hire_prerequisite :: prior experience\n",
      "hire_prerequisite :: working permit\n",
      "hire_prerequisite :: heavy lifting\n",
      " 71%|██████████████████████████████▋            | 10/14 [13:44<05:01, 75.42s/it]shifts :: night shift\n",
      "shifts :: dinner shift\n",
      "shifts :: early morning shift\n",
      "shifts :: 8 hour shift\n",
      " 79%|█████████████████████████████████▊         | 11/14 [14:28<03:17, 65.96s/it]schedule :: christmas eve\n",
      "schedule :: early morning\n",
      "schedule :: hoilday\n",
      "schedule :: 7 days\n",
      "schedule :: saturday\n",
      "schedule :: sunday\n",
      "schedule :: weekend\n",
      " 86%|████████████████████████████████████▊      | 12/14 [16:11<02:33, 76.86s/it]employee_type :: full time\n",
      "employee_type :: part time\n",
      "employee_type :: seasonal\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [17:01<01:08, 68.90s/it]onboarding_steps :: orientation\n",
      "onboarding_steps :: introduction\n",
      "onboarding_steps :: workstation\n",
      "onboarding_steps :: training\n",
      "onboarding_steps :: team lunch\n",
      "\"team lunch\" only have 1 context\n",
      "100%|███████████████████████████████████████████| 14/14 [17:41<00:00, 75.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use scripts\n",
    "!python compute_EE_corr.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/concept_corr_100.csv \\\n",
    "-ng 3 \\\n",
    "-ct 50 \\\n",
    "-top_k 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5220cc3e4090474b913a67258c0d98e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_entity = 'health insurance'\n",
    "_results = entity_expansion_multiways(_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_results['sel_entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insurance',\n",
       " 'health insurance',\n",
       " 'life insurance',\n",
       " 'property insurance',\n",
       " 'disability insurance',\n",
       " 'health care insurance',\n",
       " 'personal life',\n",
       " 'car insurance',\n",
       " 'offer health insurance',\n",
       " 'vision insurance',\n",
       " 'medical insurance',\n",
       " 'dental insurance',\n",
       " 'social life',\n",
       " 'healthcare',\n",
       " 'vehicle insurance',\n",
       " 'health care',\n",
       " 'social services',\n",
       " 'financial services',\n",
       " 'personal property',\n",
       " 'insurance company']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results['sel_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42782ed502f8477b8db39b8079ec0903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_entity = 'paid vacation'\n",
    "_results = entity_expansion_multiways(_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " ['paid vacation',\n",
       "  'free food',\n",
       "  'extra hours',\n",
       "  'personal life',\n",
       "  'additional training',\n",
       "  'pay rent',\n",
       "  'extra cash',\n",
       "  'employment',\n",
       "  'extra money',\n",
       "  'regular employee',\n",
       "  'actual training',\n",
       "  'free market',\n",
       "  'home office',\n",
       "  'paid weekly',\n",
       "  'lunch break',\n",
       "  'employment contract',\n",
       "  'paid vacations',\n",
       "  'starting pay',\n",
       "  'training class',\n",
       "  'cash office'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_results['sel_entities']), _results['sel_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid vacation', 0.0044874584203954135),\n",
       " ('free food', 0.0030260901631086125),\n",
       " ('employment', 0.0026738146242238693),\n",
       " ('additional training', 0.002321403750532552),\n",
       " ('pay rent', 0.0016830003481619713),\n",
       " ('extra money', 0.0012204012291167322),\n",
       " ('extra hours', 0.0012082061174470255),\n",
       " ('paid vacations', 0.00120581120557009),\n",
       " ('personal life', 0.001053836581509607),\n",
       " ('extra cash', 0.0009903957854880568),\n",
       " ('free market', 0.0007903035967739137),\n",
       " ('employment contract', 0.0007457378170482977),\n",
       " ('regular employee', 0.0007120051084749063),\n",
       " ('actual training', 0.0006806927853699615),\n",
       " ('home office', 0.0006772145559134888),\n",
       " ('paid weekly', 0.0006510896439229592),\n",
       " ('lunch break', 0.0006262294878335945),\n",
       " ('starting pay', 0.0005959591850205348),\n",
       " ('training class', 0.0005751933163302262),\n",
       " ('cash office', 0.0005385432788353982)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results['ints_mean_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid vacation', 0.0),\n",
       " ('paid weekly', 0.3553680094673442),\n",
       " ('regular employee', 0.5651236328627899),\n",
       " ('starting pay', 0.6403464447016876),\n",
       " ('home office', 0.702709761379911),\n",
       " ('extra hours', 0.7882322979791239),\n",
       " ('cash office', 0.8216374067668454),\n",
       " ('additional training', 0.8231515801756707),\n",
       " ('free food', 0.8235410993620799),\n",
       " ('extra cash', 0.8406818348940776),\n",
       " ('lunch break', 0.8554870473017729),\n",
       " ('extra money', 0.8671142026312799),\n",
       " ('actual training', 0.8861361628894645),\n",
       " ('free market', 0.8934745945926472),\n",
       " ('personal life', 0.905080634158609),\n",
       " ('pay rent', 0.9191154476332102),\n",
       " ('training class', 0.9618010548607787),\n",
       " ('employment', 1.0064966364964134),\n",
       " ('employment contract', 1.0666759660272174),\n",
       " ('paid vacations', 1.07702367185203)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results['ints_kl_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paid vacation', 1.0),\n",
       " ('paid weekly', 0.6898791401226259),\n",
       " ('personal life', 0.6470476793642793),\n",
       " ('home office', 0.6137368598658344),\n",
       " ('employment', 0.5939260343586834),\n",
       " ('regular employee', 0.5934659341843702),\n",
       " ('free food', 0.5814593375358278),\n",
       " ('actual training', 0.5171393046252428),\n",
       " ('extra hours', 0.48931553441132725),\n",
       " ('cash office', 0.4649905535739707),\n",
       " ('training class', 0.45922641771917155),\n",
       " ('additional training', 0.453205530878079),\n",
       " ('starting pay', 0.4452037644119071),\n",
       " ('employment contract', 0.439553353761904),\n",
       " ('pay rent', 0.4366836227238857),\n",
       " ('lunch break', 0.43380180618257874),\n",
       " ('extra cash', 0.4258937516890552),\n",
       " ('extra money', 0.4070871841193142),\n",
       " ('paid vacations', 0.4019298655642553),\n",
       " ('free market', 0.40088368759801624)]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results['ints_pearson_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Both medical and dental insurance is provided as well as [MASK]',\n",
       "  6.578818802360679e-06,\n",
       "  0.00015682175491799882,\n",
       "  0.0018069056141044838),\n",
       " ('No the [MASK] tome will be put on your final pay check',\n",
       "  9.477934070825445e-05,\n",
       "  0.00179112735515401,\n",
       "  0.01783658171282455),\n",
       " (\"A new policy came out apparently saying that you do n't get a week of [MASK] in the first year , if you start after June 1st .\",\n",
       "  0.4888658700418125,\n",
       "  1.0,\n",
       "  0.08162912113546698),\n",
       " ('It is very true that dollar tree do nt pay out your owed [MASK] when you are no longer working for them .',\n",
       "  0.0022030048390930365,\n",
       "  0.0038430425562789427,\n",
       "  0.6893102387382053),\n",
       " ('But they offer no [MASK] and no sick leave .',\n",
       "  0.026742685344147428,\n",
       "  0.05176876375876567,\n",
       "  0.25499081609946683),\n",
       " ('They give [MASK] and sick time , health and dental insurance , 401k , life insurance',\n",
       "  0.9423141194865929,\n",
       "  0.027970293703116284,\n",
       "  0.8023113287226477),\n",
       " ('only assistant managers and general managers received [MASK] pay and sick pay',\n",
       "  0.23182283459346942,\n",
       "  0.07829888924495584,\n",
       "  0.2731482003476367),\n",
       " ('For full time : [MASK] , Sick time , PTO .',\n",
       "  0.24500194640098488,\n",
       "  0.7806978269661807,\n",
       "  0.25867774481661543),\n",
       " ('Once being hired a year [MASK]', 1.4996146523943373e-06, 0.0, 0.0),\n",
       " ('Chase has disability benefits , along with [MASK] ,',\n",
       "  0.0037400833352556565,\n",
       "  0.04770937225247506,\n",
       "  0.10147697923671468),\n",
       " ('health medical , Dental , etc . ; Training ; 401 K , [MASK] .',\n",
       "  0.0030263398273425797,\n",
       "  0.017775233716358415,\n",
       "  0.8941691680646828),\n",
       " ('No [MASK] , no health insurance , nothing !',\n",
       "  0.002597835448280048,\n",
       "  0.03854780193934496,\n",
       "  0.09310450978417306),\n",
       " ('Such as , health care and [MASK] .',\n",
       "  0.0002824426112442476,\n",
       "  0.010915291304438643,\n",
       "  0.019053301564483566),\n",
       " ('I never took any vacations day , I m sure it was nt a [MASK]',\n",
       "  1.976805280428444e-05,\n",
       "  0.0001259792190053521,\n",
       "  0.000134409594121865),\n",
       " ('No [MASK] after a year .',\n",
       "  0.003960501793724179,\n",
       "  0.06561470134708737,\n",
       "  0.08128028100733416),\n",
       " ('Each store is independently owned , so you may have a [MASK] system at your store .',\n",
       "  0.0032324147746924593,\n",
       "  0.014717367664602299,\n",
       "  0.10989044586843134),\n",
       " ('I had to work for a whole year or twelve calendar months before getting a [MASK] time .',\n",
       "  0.30591641551090204,\n",
       "  0.152602106761983,\n",
       "  0.07817021447081042),\n",
       " (\"publix offers [MASK] and sick day but I have n't work long enough to use the benefit .\",\n",
       "  0.23402681471194037,\n",
       "  0.07906629652148292,\n",
       "  0.050667007914498345),\n",
       " ('You receive about a $ 1 . 00 raise , but also you receive [MASK] , sick leave , and guaranteed 2 days off .',\n",
       "  0.3446607765500741,\n",
       "  0.13353111458553363,\n",
       "  0.3233051276691392),\n",
       " ('Vision , Dental , [MASK] , 401 K',\n",
       "  0.0005212524718306747,\n",
       "  0.007993453839044417,\n",
       "  0.017517814788640586),\n",
       " ('After one year and depending on the number of hours you work will depend on how many [MASK] hours / days you will receive .',\n",
       "  0.3262417605927743,\n",
       "  0.1185306531558913,\n",
       "  0.26145063044661676),\n",
       " ('Paid Vacation accumulated over the weeks , non [MASK] was also available',\n",
       "  0.14561309892795496,\n",
       "  0.014946605749923899,\n",
       "  0.20467539264024986),\n",
       " (\"Part time does n't get any [MASK] .\",\n",
       "  0.004818101540371127,\n",
       "  0.014704183796864093,\n",
       "  0.05149127556073687),\n",
       " (\"I did n't qualify for [MASK]\",\n",
       "  0.0,\n",
       "  0.00013833383264332213,\n",
       "  2.200855882550398e-05),\n",
       " ('401k , [MASK] , sick time , medical , dental , vision , bonuses and other incentives',\n",
       "  0.06065868847182519,\n",
       "  0.21872545029662457,\n",
       "  1.0),\n",
       " ('Eligible workers gain access to healthcare coverage , 401 ( k ) retirement plans , sick leave , personal days , and [MASK] , flexible spending accounts , and life insurance options',\n",
       "  0.1725192364944353,\n",
       "  0.007755238396722399,\n",
       "  0.06916860209127944),\n",
       " ('None we do nt recieve [MASK] .',\n",
       "  0.00011854472739062176,\n",
       "  0.0005104552857867912,\n",
       "  0.0010478279669635128),\n",
       " ('Managers receive [MASK] and one free meal a day .',\n",
       "  0.11762298466492851,\n",
       "  0.13096663181722554,\n",
       "  0.39809365947980163),\n",
       " ('401k plan medical and dental [MASK] raise after 90 days',\n",
       "  0.0019179289900465575,\n",
       "  0.041361645716974614,\n",
       "  0.17611902760501727),\n",
       " ('Yes for full time there is a week [MASK]',\n",
       "  6.786200812762701e-05,\n",
       "  0.0003338251452162233,\n",
       "  0.00044098957539795696),\n",
       " ('crew members do not receive [MASK]',\n",
       "  7.044122238550834e-06,\n",
       "  0.0026251276287963398,\n",
       "  0.0008307958414410135),\n",
       " ('You have to work 40 hours to get 1 hour of [MASK] time',\n",
       "  0.16521148326526203,\n",
       "  0.6045480850184173,\n",
       "  0.34156337793211217),\n",
       " ('All the managers and the asst managers got a [MASK]',\n",
       "  1.2137215756066526e-05,\n",
       "  4.405567898891748e-05,\n",
       "  0.000350441742950686),\n",
       " ('No there is not a [MASK]',\n",
       "  4.056053445523595e-06,\n",
       "  2.1407120805241987e-05,\n",
       "  8.023251704585847e-05),\n",
       " (\"U do n't get any [MASK] time are sick time at all\",\n",
       "  0.019711473143794515,\n",
       "  0.006265320269293292,\n",
       "  0.05247547496634478),\n",
       " ('Only managers are full time which means no [MASK]',\n",
       "  3.752162015695963e-05,\n",
       "  0.0035209274956039724,\n",
       "  0.0007831260955182229),\n",
       " ('Unless you are a full time career employee , you do not get any [MASK] time',\n",
       "  1.0,\n",
       "  0.3411526648071449,\n",
       "  0.7082231131298945),\n",
       " (\"If you just went past your 90 day mark then i do n't see why you would n't be able to , but keep in mind , you do not get a [MASK] , and most places will most likely let you go because they ca n't leave a vacant spot for almost two months .\",\n",
       "  0.11149749877991277,\n",
       "  0.017488687437352903,\n",
       "  0.03329519275389829),\n",
       " ('On your 5 year anniversary you will get 500 dollars or a [MASK] .',\n",
       "  0.018360237747319093,\n",
       "  0.0246834791755306,\n",
       "  0.16996853648783594),\n",
       " ('Yes bonuses an [MASK]',\n",
       "  4.2854114280321426e-05,\n",
       "  0.0004991132662695892,\n",
       "  0.0010505137273542167),\n",
       " ('80 hrs of [MASK] per year .',\n",
       "  0.0005614906462792095,\n",
       "  0.0022177594685687013,\n",
       "  0.000172809475151608),\n",
       " ('You do need to be a shift manager for at least a year before you get your [MASK] ( at least in my experience with a franchise location )',\n",
       "  0.012128350336706043,\n",
       "  0.04237491910517959,\n",
       "  0.05330261507094856),\n",
       " ('Store managers   15 + an hour with guaranteed overtime and a week of [MASK] .',\n",
       "  0.6706696047262527,\n",
       "  0.39104137513535925,\n",
       "  0.1529266104320991),\n",
       " ('6 [MASK] days .',\n",
       "  0.0009005486490436924,\n",
       "  0.002073908147796622,\n",
       "  0.005603226966475566),\n",
       " ('menards does not offer maternity leave only [MASK] .',\n",
       "  0.0080900575648695,\n",
       "  0.004675857911426127,\n",
       "  0.012895954396048733),\n",
       " ('How can you get a [MASK] on under 40 hours ? ? ? ?',\n",
       "  0.002360870884724027,\n",
       "  0.07419871925889387,\n",
       "  0.05708052606396038),\n",
       " ('None I barely had hours to get a [MASK]',\n",
       "  3.6816430912121303e-06,\n",
       "  9.8261197820452e-06,\n",
       "  2.154213247477965e-05),\n",
       " ('There is no [MASK] , however they are lenient about your schedule',\n",
       "  0.0018703822220601277,\n",
       "  0.355697917601864,\n",
       "  0.09015526800849852),\n",
       " (\"During my time working there I did n't hear or know about [MASK] .\",\n",
       "  0.0006690955461960481,\n",
       "  0.02424166740629815,\n",
       "  0.022162387860887593),\n",
       " (\"If you do n't have enough hours for [MASK] you can still go on vacation without payment .\",\n",
       "  0.022655908428214726,\n",
       "  0.13233636096964954,\n",
       "  0.02674802956927661)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity2probs = _results['entity2probs']\n",
    "_ser1 = entity2probs['paid vacation']\n",
    "_ser2 = entity2probs['training class']\n",
    "_ser3 = entity2probs['cash office']\n",
    "\n",
    "_ser1 = (_ser1 - min(_ser1)) / (max(_ser1) - min(_ser1))\n",
    "_ser2 = (_ser2 - min(_ser2)) / (max(_ser2) - min(_ser2))\n",
    "_ser3 = (_ser3 - min(_ser3)) / (max(_ser3) - min(_ser3))\n",
    "\n",
    "list(zip(dedup_context['paid vacation'], _ser1, _ser2, _ser3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_entity = 'flexible schedule'\n",
    "_results = entity_expansion_multiways(_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_results['mean_set'] & _results['kl_set'] & _results['pearson_set']), \\\n",
    "_results['mean_set'] & _results['kl_set'] & _results['pearson_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_results['ints_mean_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_results['ints_pearson_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2probs = _results['entity2probs']\n",
    "_ser1 = entity2probs['flexible schedule']\n",
    "_ser2 = entity2probs['model']\n",
    "_ser3 = entity2probs['low level']\n",
    "\n",
    "_ser1 = (_ser1 - min(_ser1)) / (max(_ser1) - min(_ser1))\n",
    "_ser2 = (_ser2 - min(_ser2)) / (max(_ser2) - min(_ser2))\n",
    "_ser3 = (_ser3 - min(_ser3)) / (max(_ser3) - min(_ser3))\n",
    "\n",
    "list(zip(dedup_context['flexible schedule'], _ser1, _ser2, _ser3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_probe.fill_multi_mask(\"[CLS] They have a very [MASK] [MASK] for most departments and if your schedule does n't fit , you 'll more than likely just be moved . [SEP]\", topk=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_entity = 'black jeans'\n",
    "mean_set, kl_set, pearson_set = entity_expansion_multiways(_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(mean_set & kl_set & pearson_set), mean_set & kl_set & pearson_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_entity = 'walmart'\n",
    "mean_set, kl_set, pearson_set = entity_expansion_multiways(_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mean_set & kl_set & pearson_set), mean_set & kl_set & pearson_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_probe_gpt2 = LMProbe_GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_expansion_multiways_GPT2(entity, \n",
    "                                    contexts=None, \n",
    "                                    entities=entities, \n",
    "                                    lm_probe=lm_probe_gpt2, \n",
    "                                    top_k=20):\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe_GPT2()\n",
    "    if contexts is None:\n",
    "        contexts = dedup_context[entity]\n",
    "    \n",
    "    # test: speed up x100\n",
    "    entities = entities[::100]\n",
    "    \n",
    "    entity2probs = defaultdict(list)\n",
    "\n",
    "    for _context in tqdm(contexts[:50]):\n",
    "        _cand_scores = lm_probe.score_candidates(_context, entities)\n",
    "\n",
    "        for _d in _cand_scores:\n",
    "            _c = _d['cand']\n",
    "            _s = _d['score']\n",
    "            entity2probs[_c].append(_s)\n",
    "    \n",
    "#     print('entity2probs:', len(entity2probs), len(entity2probs[entity]))\n",
    "    for _e, _ss in entity2probs.items():\n",
    "        assert len(_ss) == len(entity2probs[entity]), \\\n",
    "            f'entity: {_e} | {lm_probe.tokenizer.tokenize(_e)}; len(_ss): {len(_ss)}'\n",
    "    \n",
    "    _target_ss = entity2probs[entity]\n",
    "    _target_ss = _target_ss / np.sum(_target_ss)\n",
    "    \n",
    "    mean_l = [(_e, np.mean(_ss)) for _e, _ss in entity2probs.items()]\n",
    "    mean_l.sort(key=lambda p : p[-1], reverse=True)\n",
    "    kl_l = [(_e, entropy(_target_ss, _ss)) for _e, _ss in entity2probs.items()]\n",
    "    kl_l.sort(key=lambda p : p[-1], reverse=False)\n",
    "    pearson_l = [(_e, pearsonr(_target_ss, _ss)[0]) for _e, _ss in entity2probs.items()]\n",
    "    pearson_l.sort(key=lambda p : p[-1], reverse=True)\n",
    "    \n",
    "    entity2ranks = defaultdict(list)\n",
    "    for i, (_e, _s) in enumerate(mean_l):\n",
    "        entity2ranks[_e].append(i)\n",
    "    for i, (_e, _s) in enumerate(kl_l):\n",
    "        entity2ranks[_e].append(i)\n",
    "    for i, (_e, _s) in enumerate(pearson_l):\n",
    "        entity2ranks[_e].append(i)\n",
    "    # To simile top-k set intersection, keep the highest rank of _e among each criteria\n",
    "    entity_overall_ranks = [(_e, max(_ranks)) for _e, _ranks in entity2ranks.items()]\n",
    "    entity_overall_ranks.sort(key=lambda p : p[-1])\n",
    "    # Now, the top-k is for the final selection, not for each criteria\n",
    "    sel_entities = [_e for _e, _ in entity_overall_ranks[:top_k]]\n",
    "    \n",
    "#     mean_set = set([_e for _e, _s in mean_l[:top_k]])\n",
    "#     kl_set = set([_e for _e, _s in kl_l[:top_k]])\n",
    "#     pearson_set = set([_e for _e, _s in pearson_l[:top_k]])\n",
    "    \n",
    "#     sel_entities = mean_set & kl_set & pearson_set\n",
    "    ints_mean_l = [p for p in mean_l if p[0] in sel_entities]\n",
    "    ints_kl_l = [p for p in kl_l if p[0] in sel_entities]\n",
    "    ints_pearson_l = [p for p in pearson_l if p[0] in sel_entities]\n",
    "    \n",
    "    return {\n",
    "        \"entity2probs\": entity2probs,\n",
    "        \"mean_l\": mean_l,\n",
    "        \"kl_l\": kl_l,\n",
    "        \"pearson_l\": pearson_l,\n",
    "#         \"mean_set\": mean_set,\n",
    "#         \"kl_set\": kl_set,\n",
    "#         \"pearson_set\": pearson_set,\n",
    "        \"sel_entities\": sel_entities,\n",
    "        \"ints_mean_l\": ints_mean_l,\n",
    "        \"ints_kl_l\": ints_kl_l,\n",
    "        \"ints_pearson_l\": ints_pearson_l,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_entity = 'health insurance'\n",
    "_results = entity_expansion_multiways_GPT2(_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT greedy-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMProbe_PMI(object):\n",
    "    def __init__(self, model_name='bert-base-uncased', use_gpu=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForMaskedLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.mask_token = self.tokenizer.mask_token\n",
    "\n",
    "    def fill_multi_mask(self, input_txt, topk=3):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def score_tail(self, input_txt, tail, head=None, head_len=None, head_first=True):\n",
    "        # input_txt: str, with [HEAD] for head and [TAIL] for tail \n",
    "        # tail: str, the tail entity \n",
    "        # head: str, the head entity \n",
    "        # head_len: int, the length of head entity\n",
    "        # Should only give head or head_len \n",
    "        # head_first: bool, whether the first [MASK] is the head \n",
    "        \n",
    "        assert (head is None) + (head_len is None) == 1, \\\n",
    "            f\"head = {head}, head_len = {head_len}\"\n",
    "        assert input_txt.count(\"[HEAD]\") == input_txt.count(\"[TAIL]\") == 1, \\\n",
    "            f\"Input string must have [HEAD] and [TAIL], got {input_txt}\"\n",
    "        \n",
    "        \n",
    "        tail_toks = self.tokenizer.tokenize(tail)\n",
    "        tail_len = len(tail_toks)\n",
    "        input_txt = input_txt.replace('[TAIL]', '[MASK]' + ' [MASK]' * (tail_len-1))\n",
    "        \n",
    "        if head is not None:\n",
    "            head_toks = self.tokenizer.tokenize(head)\n",
    "            head_len = len(head_toks)\n",
    "            print(head_toks, head_len)\n",
    "            input_txt = input_txt.replace('[HEAD]', head)\n",
    "        else:\n",
    "            input_txt = input_txt.replace('[HEAD]', '[MASK]' + ' [MASK]' * (head_len-1))\n",
    "        \n",
    "        tokenized_txt = self.tokenizer.tokenize(input_txt)\n",
    "        tokenized_txt = ['[CLS]'] + tokenized_txt + ['[SEP]']\n",
    "\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        mask_indices = [i for i, x in enumerate(tokenized_txt) if x == \"[MASK]\"]\n",
    "        if head is not None:\n",
    "            # head is not [MASK] \n",
    "            tail_indices = mask_indices\n",
    "        elif head_first:\n",
    "            # head is [MASK] and first \n",
    "            tail_indices = mask_indices[head_len:]\n",
    "        else:\n",
    "            # head is [MASK] and second \n",
    "            tail_indices = mask_indices[:tail_len]\n",
    "        print(tokenized_txt, tail_indices)\n",
    "        \n",
    "        segment_idx = tokens_tensor * 0\n",
    "        tokens_tensor = tokens_tensor.to(self.device)\n",
    "        segments_tensors = segment_idx.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        probs = torch.softmax(predictions, dim=-1)[0]\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        \n",
    "        _scores = []\n",
    "        tail_tok_ids = self.tokenizer.convert_tokens_to_ids(tail_toks)\n",
    "        for i, token_id in zip(tail_indices, tail_tok_ids):\n",
    "            _scores.append(probs[i, token_id].item())\n",
    "        score = gmean(_scores)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMProbe_PMI_greedy(object):\n",
    "    def __init__(self, model_name='bert-base-uncased', use_gpu=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForMaskedLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.mask_token = self.tokenizer.mask_token\n",
    "\n",
    "    def fill_multi_mask(self, input_txt, topk=3):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def score_tail(self, input_txt, tail, head=None, head_len=None, head_first=True):\n",
    "        # input_txt: str, with [HEAD] for head and [TAIL] for tail \n",
    "        # tail: str, the tail entity \n",
    "        # head: str, the head entity \n",
    "        # head_len: int, the length of head entity\n",
    "        # Should only give head or head_len \n",
    "        # head_first: bool, whether the first [MASK] is the head \n",
    "        \n",
    "        assert (head is None) + (head_len is None) == 1, \\\n",
    "            f\"head = {head}, head_len = {head_len}\"\n",
    "        assert input_txt.count(\"[HEAD]\") == input_txt.count(\"[TAIL]\") == 1, \\\n",
    "            f\"Input string must have [HEAD] and [TAIL], got {input_txt}\"\n",
    "                \n",
    "        tail_toks = self.tokenizer.tokenize(tail)\n",
    "        tail_len = len(tail_toks)\n",
    "        input_txt = input_txt.replace('[TAIL]', '[MASK]' + ' [MASK]' * (tail_len-1))\n",
    "        \n",
    "        if head is not None:\n",
    "            head_toks = self.tokenizer.tokenize(head)\n",
    "            head_len = len(head_toks)\n",
    "            print(head_toks, head_len)\n",
    "            input_txt = input_txt.replace('[HEAD]', head)\n",
    "        else:\n",
    "            input_txt = input_txt.replace('[HEAD]', '[MASK]' + ' [MASK]' * (head_len-1))\n",
    "        \n",
    "        tokenized_txt = self.tokenizer.tokenize(input_txt)\n",
    "        tokenized_txt = ['[CLS]'] + tokenized_txt + ['[SEP]']\n",
    "        mask_indices = [i for i, x in enumerate(tokenized_txt) if x == \"[MASK]\"]\n",
    "\n",
    "        if head is not None:\n",
    "            # head is not [MASK] \n",
    "            tail_indices = mask_indices\n",
    "        elif head_first:\n",
    "            # head is [MASK] and first \n",
    "            tail_indices = mask_indices[head_len:]\n",
    "        else:\n",
    "            # head is [MASK] and second \n",
    "            tail_indices = mask_indices[:tail_len]\n",
    "        \n",
    "        # Greedy filling \n",
    "        unfilled_indices = list(tail_indices)\n",
    "        scores = []\n",
    "        while len(unfilled_indices) > 0:\n",
    "            print(tokenized_txt, unfilled_indices)\n",
    "            \n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            \n",
    "            segment_idx = tokens_tensor * 0\n",
    "            tokens_tensor = tokens_tensor.to(self.device)\n",
    "            segments_tensors = segment_idx.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "                predictions = outputs[0]\n",
    "\n",
    "            probs = torch.softmax(predictions, dim=-1)[0]\n",
    "            probs = probs.detach().cpu().numpy()\n",
    "\n",
    "            _tok_scores = []\n",
    "            tail_tok_ids = self.tokenizer.convert_tokens_to_ids(tail_toks)\n",
    "            for i, token_id in zip(tail_indices, tail_tok_ids):\n",
    "                if i not in unfilled_indices:\n",
    "                    continue\n",
    "                _score = probs[i, token_id].item()\n",
    "                _tok_scores.append((i, token_id, _score))\n",
    "                \n",
    "            _tok_scores.sort(key=lambda p : p[1], reverse=True)\n",
    "            _fill_idx, _fill_tok, _score = _tok_scores[0]\n",
    "            unfilled_indices.remove(_fill_idx)\n",
    "            scores.append(_score)\n",
    "            tokenized_txt[_fill_idx] = self.tokenizer.convert_ids_to_tokens(_fill_tok)\n",
    "        \n",
    "        return np.prod(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lm_probe_greedy = LMProbe_PMI_greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microsoft'] 1\n",
      "['[CLS]', 'microsoft', 'hires', '[MASK]', '[MASK]', '.', '[SEP]'] [3, 4]\n",
      "['[CLS]', 'microsoft', 'hires', '[MASK]', 'engineers', '.', '[SEP]'] [3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0002394940119523417"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sent = '[HEAD] hires [TAIL] .'\n",
    "\n",
    "lm_probe_greedy.score_tail(_sent, tail='software engineers', head='microsoft', head_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target'] 1\n",
      "['[CLS]', 'target', 'hires', '[MASK]', '[MASK]', '.', '[SEP]'] [3, 4]\n",
      "['[CLS]', 'target', 'hires', '[MASK]', 'engineers', '.', '[SEP]'] [3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.490697362450423e-06"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probe_greedy.score_tail(_sent, tail='software engineers', head='target', head_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[MASK]', 'hires', '[MASK]', '[MASK]', '.', '[SEP]'] [3, 4]\n",
      "['[CLS]', '[MASK]', 'hires', '[MASK]', 'engineers', '.', '[SEP]'] [3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.044167127037733e-07"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probe_greedy.score_tail(_sent, tail='software engineers', head_len=1, head_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction Baselines\n",
    "Currently only for single relation. TODO: include all relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null baseline - Cartesian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_Cartesian_RE(seed_concepts_path,\n",
    "                      seed_relations_path,\n",
    "                      concept_knn_path,\n",
    "                      relation,\n",
    "                      topk=None,\n",
    "                      dest=None,\n",
    "                      **kwargs):\n",
    "    \n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    seed_relations_df = pd.read_csv(seed_relations_path)\n",
    "    relation_row = seed_relations_df[seed_relations_df['alignedRelationName'] == relation].iloc[0]\n",
    "    concept_knn_results = pd.read_csv(concept_knn_path)\n",
    "    \n",
    "    head_type = relation_row['domain']\n",
    "    tail_type = relation_row['range']\n",
    "    print(head_type, '\\t', tail_type)\n",
    "    seed_heads = seed_concepts_df[seed_concepts_df['alignedCategoryName'] == head_type]['seedInstances'].item()\n",
    "#     seed_heads = eval(list(seed_heads)[0])\n",
    "    seed_tails = seed_concepts_df[seed_concepts_df['alignedCategoryName'] == tail_type]['seedInstances'].item()\n",
    "#     seed_tails = eval(list(seed_tails)[0])\n",
    "    print('seed_heads:', seed_heads)\n",
    "    print('seed_tails:', seed_tails)\n",
    "\n",
    "    # Candidate heads / tails from concept knn \n",
    "    cand_heads_df = concept_knn_results[concept_knn_results['concept'] == head_type]\n",
    "    cand_tails_df = concept_knn_results[concept_knn_results['concept'] == tail_type]\n",
    "    cand_heads = [(_h, 1.0) for _h in seed_heads] + \\\n",
    "        list(zip(cand_heads_df['neighbor'].tolist(), cand_heads_df['sim'].tolist()))\n",
    "    cand_tails = [(_t, 1.0) for _t in seed_tails] + \\\n",
    "        list(zip(cand_tails_df['neighbor'].tolist(), cand_tails_df['sim'].tolist()))\n",
    "\n",
    "    if topk is not None:\n",
    "        cand_heads = cand_heads[:topk]\n",
    "        cand_tails = cand_tails[:topk]\n",
    "        \n",
    "    print('cand_heads:', list(zip(*cand_heads))[0])\n",
    "    print('cand_tails:', list(zip(*cand_tails))[0])\n",
    "    \n",
    "    out_rels = []\n",
    "    for _h, _hs in cand_heads:\n",
    "        for _t, _ts in cand_tails:\n",
    "            out_rels.append({\n",
    "                'head': _h, 'relation': relation, 'tail': _t,\n",
    "                'overall_score': _hs * _ts\n",
    "            })\n",
    "    out_rels.sort(key=lambda d : d['overall_score'], reverse=True)\n",
    "    \n",
    "    out_rels_df = pd.DataFrame(out_rels)\n",
    "    if dest is not None:\n",
    "        out_rels_df.to_csv(dest, index=False)\n",
    "    return out_rels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations_nodup.csv')\n",
    "# knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/knn_{cluster_size}.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_1000.csv')\n",
    "\n",
    "relation = 'has_benefits'\n",
    "cartesian_re_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_cartesian-{relation}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company \t benefits\n",
      "seed_heads: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\n",
      "seed_tails: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\n",
      "cand_heads: ('walmart', 'amazon', 'subway', 'microsoft', 'target', 'wal mart', 'costco', 'publix', 'walgreens', 'kroger', 'home depot', \"sam 's club\", 'dollar general', 'family dollar', 'jcpenney', 'pizza hut', 'starbucks', 'apple', 'kfc', 'dollar tree', 'panera bread', 'safeway', 'hobby lobby', 'cracker barrel', 'spectrum', 'menards', 'chick fil a', 'old navy', 'mcdonalds', 'taco bell', 'marshalls', 'burlington', 'olive garden', 'cvs', 'pepsico', 'sitel', 'burger king', 'petsmart', 'jcp', 'pepsi', \"macy 's\", 'geico', 'whole foods', 'ihop', 'fedex', 'best buy', 'frito lay', 'dunkin donuts', 'chipotle', 'tj maxx', 'verizon', 't mobile', 'g4s', 'usps', 'jc penney', 'at&t', 'planet fitness', 'little caesars', 'company', 'mcdonald')\n",
      "cand_tails: ('health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance', 'insurance', 'healthcare', 'medical insurance', 'health care', 'health', 'medical', 'paid vacations', 'sick days', 'life insurance', 'dental insurance', 'pension', '401k plan', 'holiday pay', 'maternity leave', 'tuition reimbursement', 'discount card', 'vacation days', 'profit sharing', 'employee discounts', 'employee discount', 'retirement plan', 'great benefits', 'health care insurance', 'disability', 'benefits package', 'tuition assistance', 'employee benefits', 'part timers', 'higher pay', 'medicare', 'flexible schedules', 'mandatory', 'pay increase', 'heath', 'education', 'union', 'weekly pay', 'free', 'cobra', 'job security', 'work life balance', 'previous experience', 'cards', 'family', 'child care', 'tax', 'medicaid', 'competitive pay', 'leaves', 'free food', '90 days', 'cigna', 'fair', 'p / t')\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "full_Cartesian_RE(seed_concepts_path=seed_aligned_concepts_path,\n",
    "                  seed_relations_path=seed_aligned_relations_path,\n",
    "                  concept_knn_path=concept_knn_path,\n",
    "                  relation=relation,\n",
    "                  topk=60,\n",
    "                  dest=cartesian_re_path)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company \t benefits\r\n",
      "seed_heads: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\r\n",
      "seed_tails: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\r\n",
      "cand_heads: ('walmart', 'amazon', 'subway', 'microsoft', 'target', 'wal mart', 'costco', 'publix', 'walgreens', 'kroger', 'home depot', \"sam 's club\", 'dollar general', 'family dollar', 'jcpenney', 'pizza hut', 'starbucks', 'apple', 'kfc', 'dollar tree', 'panera bread', 'safeway', 'hobby lobby', 'cracker barrel', 'spectrum', 'menards', 'chick fil a', 'old navy', 'mcdonalds', 'taco bell', 'marshalls', 'burlington', 'olive garden', 'cvs', 'pepsico', 'sitel', 'burger king', 'petsmart', 'jcp', 'pepsi', \"macy 's\", 'geico', 'whole foods', 'ihop', 'fedex', 'best buy', 'frito lay', 'dunkin donuts', 'chipotle', 'tj maxx', 'verizon', 't mobile', 'g4s', 'usps', 'jc penney', 'at&t', 'planet fitness', 'little caesars', 'company', 'mcdonald')\r\n",
      "cand_tails: ('health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance', 'insurance', 'healthcare', 'medical insurance', 'health care', 'health', 'medical', 'paid vacations', 'sick days', 'life insurance', 'dental insurance', 'pension', '401k plan', 'holiday pay', 'maternity leave', 'tuition reimbursement', 'discount card', 'vacation days', 'profit sharing', 'employee discounts', 'employee discount', 'retirement plan', 'great benefits', 'health care insurance', 'disability', 'benefits package', 'tuition assistance', 'employee benefits', 'part timers', 'higher pay', 'medicare', 'flexible schedules', 'mandatory', 'pay increase', 'heath', 'education', 'union', 'weekly pay', 'free', 'cobra', 'job security', 'work life balance', 'previous experience', 'cards', 'family', 'child care', 'tax', 'medicaid', 'competitive pay', 'leaves', 'free food', '90 days', 'cigna', 'fair', 'p / t')\r\n"
     ]
    }
   ],
   "source": [
    "# Use script \n",
    "!python relation_extraction_cartesian.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=Ct.csv \\\n",
    "-r has_benefits \\\n",
    "-cknn $base_dir/data/$data_ac/intermediate/concept_knn_1000.csv \\\n",
    "-topk 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction - GPT2 scores (analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation \n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_score(sentence):\n",
    "    tokenized_input = gpt2_tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_outputs = gpt2_model(**tokenized_input, labels=tokenized_input[\"input_ids\"])\n",
    "    score = model_outputs.loss.item()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NLL) X offers Y        \tbenefits\t401k plan\tpaid vacation\tfamily  \n",
      "company                 \t8.153534\t6.109497\t6.707372\t8.615242\n",
      "walmart                 \t9.049321\t6.962499\t8.100282\t8.999038\n",
      "google                  \t8.491952\t6.307848\t7.430027\t9.093484\n",
      "california              \t6.713108\t5.849362\t6.076878\t6.828558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_sent_template = '{0} offers {1}'\n",
    "_heads = ['company', 'walmart', 'google', 'california']\n",
    "_tails = ['benefits', '401k plan', 'paid vacation', 'family']\n",
    "\n",
    "_print_msg = f\"{'(NLL) ' + _sent_template.format('X', 'Y'):24s}\"\n",
    "_print_msg += '\\t' + '\\t'.join([f\"{_t:8s}\" for _t in _tails]) + '\\n'\n",
    "for _h in _heads:\n",
    "    _print_msg += f\"{_h:24s}\"\n",
    "    for _t in _tails:\n",
    "        _sent = _sent_template.format(_h, _t)\n",
    "        _score = gpt2_score(_sent)\n",
    "#         print(f\"(NLL = {_score:.6f}) {_sent}\")\n",
    "        _print_msg += f\"\\t{_score:.6f}\"\n",
    "    _print_msg += \"\\n\"\n",
    "\n",
    "print(_print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NLL) X allows Y        \tdress code\tjeans   \ttattoos \tdrugs   \n",
      "company                 \t7.646885\t11.025874\t10.445228\t9.259410\n",
      "walmart                 \t8.517221\t10.956227\t10.929914\t10.295705\n",
      "google                  \t8.461215\t12.038675\t12.965143\t10.222376\n",
      "california              \t6.353935\t7.937030\t7.359611\t6.910169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_sent_template = '{0} allows {1}'\n",
    "_heads = ['company', 'walmart', 'google', 'california']\n",
    "_tails = ['dress code', 'jeans', 'tattoos', 'drugs']\n",
    "\n",
    "_print_msg = f\"{'(NLL) ' + _sent_template.format('X', 'Y'):24s}\"\n",
    "_print_msg += '\\t' + '\\t'.join([f\"{_t:8s}\" for _t in _tails]) + '\\n'\n",
    "for _h in _heads:\n",
    "    _print_msg += f\"{_h:24s}\"\n",
    "    for _t in _tails:\n",
    "        _sent = _sent_template.format(_h, _t)\n",
    "        _score = gpt2_score(_sent)\n",
    "#         print(f\"(NLL = {_score:.6f}) {_sent}\")\n",
    "        _print_msg += f\"\\t{_score:.6f}\"\n",
    "    _print_msg += \"\\n\"\n",
    "\n",
    "print(_print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NLL) X pays Y          \tschedule\tweekly  \tevery friday\tminimum wage\n",
      "company                 \t11.049671\t9.899395\t7.250698\t6.185252\n",
      "walmart                 \t11.554404\t10.105996\t7.789978\t7.334626\n",
      "google                  \t11.968147\t10.422943\t7.292716\t7.170493\n",
      "california              \t8.314291\t7.803281\t6.406629\t5.754911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_sent_template = '{0} pays {1}'\n",
    "_heads = ['company', 'walmart', 'google', 'california']\n",
    "_tails = ['schedule', 'weekly', 'every friday', 'minimum wage']\n",
    "\n",
    "_print_msg = f\"{'(NLL) ' + _sent_template.format('X', 'Y'):24s}\"\n",
    "_print_msg += '\\t' + '\\t'.join([f\"{_t:8s}\" for _t in _tails]) + '\\n'\n",
    "for _h in _heads:\n",
    "    _print_msg += f\"{_h:24s}\"\n",
    "    for _t in _tails:\n",
    "        _sent = _sent_template.format(_h, _t)\n",
    "        _score = gpt2_score(_sent)\n",
    "#         print(f\"(NLL = {_score:.6f}) {_sent}\")\n",
    "        _print_msg += f\"\\t{_score:.6f}\"\n",
    "    _print_msg += \"\\n\"\n",
    "\n",
    "print(_print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction - scores weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "lm_probe = LMProbe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_direct_probing_candidates(templates,\n",
    "                                  lm_probe=None,\n",
    "                                  head_entity=None,\n",
    "                                  tail_entity=None,\n",
    "                                  context=None,\n",
    "                                  topk=10):\n",
    "    '''\n",
    "    Direct probing: let BERT propose possible entities  \n",
    "    :param templates: List[str]: each have 2 slots, {0} for head, {1} for tail \n",
    "    :return: Dict[str, float]: proposed entities and scores \n",
    "    '''\n",
    "    \n",
    "    # ensure given one and propose one \n",
    "    assert (head_entity is None) != (tail_entity is None), f'{head_entity} {tail_entity}'\n",
    "\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "    \n",
    "    names_scores = {}\n",
    "    for template in templates:\n",
    "        if head_entity is not None:\n",
    "            # head -> tail \n",
    "            _unigram_template = template.format(head_entity, '[MASK]')\n",
    "            _bigram_template = template.format(head_entity, '[MASK] [MASK]')\n",
    "        else:\n",
    "            # tail -> head \n",
    "            _unigram_template = template.format('[MASK]', tail_entity)\n",
    "            _bigram_template = template.format('[MASK] [MASK]', tail_entity)\n",
    "        \n",
    "        for _template in [_unigram_template, _bigram_template]:\n",
    "            if context:\n",
    "                query = '[CLS] ' + _template + '[SEP]' + context + '[SEP]'\n",
    "            else:\n",
    "                query = '[CLS] ' + _template + '[SEP]'\n",
    "            preds = lm_probe.fill_multi_mask(query, topk=topk)\n",
    "            for pred in preds:\n",
    "                name = ' '.join([p['token_str'] for p in pred])\n",
    "                name = name.replace(' ##', '')\n",
    "                score = np.prod([p['prob'] for p in pred])\n",
    "                scores = names_scores.get(name, [])\n",
    "                scores.append(score)\n",
    "                names_scores[name] = scores\n",
    "                \n",
    "    names_avg_scores = {k: float(sum(v))/ len(v) for k,v in names_scores.items()}\n",
    "    names_avg_scores = {k: v for k, v in sorted(names_avg_scores.items(), reverse=True, key=lambda item: item[1])[:topk]}\n",
    "    return names_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def direct_probing_RE_v4(seed_concepts_path,\n",
    "                         seed_relations_path,\n",
    "                         emb_path,\n",
    "                         concept_knn_path,\n",
    "                         templates_path,\n",
    "                         relation,\n",
    "                         lm_probe=None,\n",
    "                         embedding_dim=768,\n",
    "                         scores_agg_func=None,\n",
    "                         topk=10,\n",
    "                         dest=None,\n",
    "                         **kwargs):\n",
    "    '''\n",
    "    For each head / tail, rank candidate tails / heads by overall scores. \n",
    "    (v4: Not limited to base -> new; can be new -> new; however, only head->tail, no tail->head)\n",
    "    Current (default) overall score: 0.1 * ht_sim + 10 * concept_sim + 0.1 * log(lm_prob)\n",
    "    '''\n",
    "    \n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    seed_relations_df = pd.read_csv(seed_relations_path)\n",
    "    relation_row = seed_relations_df[seed_relations_df['alignedRelationName'] == relation].iloc[0]\n",
    "    entity_embeddings = load_embeddings(emb_path, embedding_dim)\n",
    "    entity_emb_dict = dict(zip(entity_embeddings['entity'].tolist(),\n",
    "                               entity_embeddings['embedding'].tolist()))\n",
    "    concept_knn_results = pd.read_csv(concept_knn_path)\n",
    "    \n",
    "    with open(templates_path, 'r') as f:\n",
    "        all_templates = json.load(f)\n",
    "    templates = all_templates[relation]\n",
    "    templates = templates['positive'] + templates['negative']\n",
    "\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "    if scores_agg_func is None:\n",
    "        scores_agg_func = lambda ht_sim, h_sim, t_sim, lm_prob : ht_sim + h_sim + t_sim + np.log10(lm_prob)\n",
    "    \n",
    "    head_type = relation_row['domain']\n",
    "    tail_type = relation_row['range']\n",
    "#     head_type = \"company\"\n",
    "#     tail_type = \"dress_code\"\n",
    "    print(head_type, '\\t', tail_type)\n",
    "    seed_heads = seed_concepts_df[seed_concepts_df['alignedCategoryName'] == head_type]['seedInstances'].item()\n",
    "#     seed_heads = eval(list(seed_heads)[0])\n",
    "    seed_tails = seed_concepts_df[seed_concepts_df['alignedCategoryName'] == tail_type]['seedInstances'].item()\n",
    "#     seed_tails = eval(list(seed_tails)[0])\n",
    "    print('seed_heads:', seed_heads)\n",
    "    print('seed_tails:', seed_tails)\n",
    "\n",
    "    # Candidate heads / tails from concept knn \n",
    "    cand_heads_df = concept_knn_results[concept_knn_results['concept'] == head_type]\n",
    "    cand_tails_df = concept_knn_results[concept_knn_results['concept'] == tail_type]\n",
    "    cand_heads_dict = dict(zip(cand_heads_df['neighbor'].tolist(), cand_heads_df['sim'].tolist()))\n",
    "    cand_tails_dict = dict(zip(cand_tails_df['neighbor'].tolist(), cand_tails_df['sim'].tolist()))\n",
    "    for h in seed_heads:\n",
    "        assert h not in cand_heads_dict\n",
    "        cand_heads_dict[h] = 1.0\n",
    "    for t in seed_tails:\n",
    "        assert t not in cand_tails_dict\n",
    "        cand_tails_dict[t] = 1.0\n",
    "        \n",
    "    \n",
    "    all_extraction_results = []\n",
    "    \n",
    "    for c_head in tqdm(cand_heads_dict.keys(), total=len(cand_heads_dict)):\n",
    "        c_head_tokenized = lm_probe.tokenizer.tokenize(c_head)\n",
    "        if len(c_head_tokenized) > 2:\n",
    "            continue\n",
    "\n",
    "        extraction_results = []\n",
    "\n",
    "        ## For each tail, extract concept sim, head sim, lm score, combine and report\n",
    "        \n",
    "        cand_bins = {1: [], 2: []} ## TODO: allow higher grams; switch to GPT-2 for fair probs \n",
    "        for c_tail in cand_tails_dict.keys():\n",
    "            if c_tail == c_head:\n",
    "                continue\n",
    "            c_tail_tokenized = lm_probe.tokenizer.tokenize(c_tail)\n",
    "            if len(c_tail_tokenized) in [1, 2]:\n",
    "                cand_bins[len(c_tail_tokenized)].append(c_tail_tokenized)\n",
    "        \n",
    "        cand_scores_per_template = []\n",
    "        for template in templates:\n",
    "            _unigram_template = '[CLS] ' + template.format(c_head, '[MASK]') + '[SEP]'\n",
    "            _bigram_template = '[CLS] ' + template.format(c_head, '[MASK] [MASK]') + '[SEP]'\n",
    "\n",
    "            _cand_scores_1 = lm_probe.score_candidates(_unigram_template, cand_bins[1])\n",
    "            _cand_scores_2 = lm_probe.score_candidates(_bigram_template, cand_bins[2])\n",
    "            _cand_scores = sorted(_cand_scores_1 + _cand_scores_2, key=lambda d : d[\"cand\"])\n",
    "            # List[Dict[\"cand\", \"score\"]]\n",
    "            cand_scores_per_template.append(_cand_scores)\n",
    "    \n",
    "        cand_scores = []  # List[Dict[\"cand\", \"score\"]], for each \"cand\" the average score \n",
    "        for _cand_score_lst in zip(*cand_scores_per_template):\n",
    "            # _cand_score_lst: List[Dict[\"cand\", \"score\"]], for the same \"cand\" and different template \n",
    "            _cand = _cand_score_lst[0][\"cand\"]\n",
    "            assert all(d[\"cand\"] == _cand for d in _cand_score_lst), _cand_score_lst\n",
    "            _score = np.mean([d[\"score\"] for d in _cand_score_lst])\n",
    "            cand_scores.append({\"cand\": _cand, \"score\": _score})\n",
    "#         cand_scores.sort(key = lambda d : d[\"score\"], reverse=True)\n",
    "\n",
    "        for d in cand_scores:\n",
    "            e_tail = ' '.join(d[\"cand\"]).replace(' ##', '')\n",
    "            if e_tail not in cand_tails_dict:\n",
    "                continue\n",
    "\n",
    "            lm_score = d[\"score\"]\n",
    "            try:\n",
    "                ht_sim_score = 1 - cosine(entity_emb_dict[c_head], entity_emb_dict[e_tail])\n",
    "            except KeyError:\n",
    "                print(f'** embedding of {c_head}: {(c_head in entity_emb_dict)}')\n",
    "                print(f'** embedding of {e_tail}: {(e_tail in entity_emb_dict)}')\n",
    "                ht_sim_score = float(\"nan\")\n",
    "            head_sim_score = cand_heads_dict[c_head]\n",
    "            tail_sim_score = cand_tails_dict[e_tail]\n",
    "            overall_score = scores_agg_func(ht_sim_score, head_sim_score, tail_sim_score, lm_score)\n",
    "\n",
    "            extraction_results.append({'head': c_head, 'relation': relation, 'tail': e_tail,\n",
    "                                       'ht_sim_score': ht_sim_score,\n",
    "                                       'head_sim_score': head_sim_score,\n",
    "                                       'tail_sim_score': tail_sim_score,\n",
    "                                       'lm_score': lm_score,\n",
    "                                       'overall_score': overall_score})\n",
    "        \n",
    "        # extraction_results.sort(key=lambda d : d['overall_score'], reverse=True)\n",
    "        all_extraction_results.extend(extraction_results[:topk])\n",
    "\n",
    "    all_extraction_results.sort(key=lambda d : d['overall_score'], reverse=True)\n",
    "    all_extraction_results = all_extraction_results[:topk]\n",
    "        \n",
    "    results_df = pd.DataFrame(all_extraction_results)\n",
    "    if dest is not None:\n",
    "        results_df.to_csv(dest, index=None)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company \t benefits\n",
      "seed_heads: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\n",
      "seed_tails: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9307f6c82c94efcae96615f9f922a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>ht_sim_score</th>\n",
       "      <th>head_sim_score</th>\n",
       "      <th>tail_sim_score</th>\n",
       "      <th>lm_score</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>company</td>\n",
       "      <td>has_benefits</td>\n",
       "      <td>business</td>\n",
       "      <td>0.993406</td>\n",
       "      <td>0.991990</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.415795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>home depot</td>\n",
       "      <td>has_benefits</td>\n",
       "      <td>car insurance</td>\n",
       "      <td>0.959858</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.973259</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.331895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fulfillment center</td>\n",
       "      <td>has_benefits</td>\n",
       "      <td>business</td>\n",
       "      <td>0.983125</td>\n",
       "      <td>0.987292</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.205897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corporate office</td>\n",
       "      <td>has_benefits</td>\n",
       "      <td>business</td>\n",
       "      <td>0.984971</td>\n",
       "      <td>0.988309</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.200351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store level</td>\n",
       "      <td>has_benefits</td>\n",
       "      <td>cards</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.988855</td>\n",
       "      <td>0.975875</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.181516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 head      relation           tail  ht_sim_score  \\\n",
       "0             company  has_benefits       business      0.993406   \n",
       "1          home depot  has_benefits  car insurance      0.959858   \n",
       "2  fulfillment center  has_benefits       business      0.983125   \n",
       "3    corporate office  has_benefits       business      0.984971   \n",
       "4         store level  has_benefits          cards      0.977417   \n",
       "\n",
       "   head_sim_score  tail_sim_score  lm_score  overall_score  \n",
       "0        0.991990        0.972477  0.002870       0.415795  \n",
       "1        0.996124        0.973259  0.002527       0.331895  \n",
       "2        0.987292        0.972477  0.001832       0.205897  \n",
       "3        0.988309        0.972477  0.001797       0.200351  \n",
       "4        0.988855        0.975875  0.001735       0.181516  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "# seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations_nodup.csv')\n",
    "# knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/knn_{cluster_size}.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_100.csv')\n",
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "\n",
    "templates_path = 'templates_manual.json'\n",
    "\n",
    "extraction_save_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "# extraction_save_path = None\n",
    "\n",
    "extraction_results = direct_probing_RE_v4(seed_concepts_path=seed_aligned_concepts_path,\n",
    "                                          seed_relations_path=seed_aligned_relations_path,\n",
    "                                          emb_path=bert_emb_path,\n",
    "                                          concept_knn_path=concept_knn_path,\n",
    "                                          templates_path=templates_path,\n",
    "                                          relation='has_benefits',\n",
    "                                          lm_probe=lm_probe,\n",
    "                                          topk=10,\n",
    "                                          save_path=extraction_save_path)\n",
    "\n",
    "extraction_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = extraction_results.copy()\n",
    "df[df['head'] == 'walmart'].head(50)\n",
    "# df['overall_score'] = df['ht_sim_score'] * 0.1 + df['concept_sim_score'] * 10 + np.log10(df['lm_score'])\n",
    "# df.sort_values(by='overall_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = extraction_results.copy()\n",
    "df[df['tail'] == 'hair color'].head(50)\n",
    "# df['overall_score'] = df['ht_sim_score'] * 0.1 + df['concept_sim_score'] * 10 + np.log10(df['lm_score'])\n",
    "# df.sort_values(by='overall_score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "company \t benefits\n",
      "seed_heads: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\n",
      "seed_tails: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\n",
      "seed_head: walmart\n",
      "seed_head: amazon\n",
      "seed_head: subway\n",
      "seed_head: microsoft\n",
      "seed_head: target\n",
      "seed_tail: health insurance\n",
      "seed_tail: flexible schedule\n",
      "seed_tail: 401k\n",
      "seed_tail: paid vacation\n",
      "seed_tail: sick leave\n",
      "seed_tail: vision insurance\n"
     ]
    }
   ],
   "source": [
    "# Use script \n",
    "!python relation_extraction_avg_scores.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE.csv \\\n",
    "-r has_benefits \\\n",
    "-cknn $base_dir/data/$data_ac/intermediate/concept_knn_1000.csv \\\n",
    "-topk 300 \\\n",
    "-dim 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction - LM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use script \n",
    "!python relation_extraction_LM.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=LM_joint.csv \\\n",
    "-r has_benefits \\\n",
    "-ee $base_dir/data/$data_ac/intermediate/concept_knn_100.csv \\\n",
    "-lm joint \\\n",
    "-topk 300 \\\n",
    "-dim 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python relation_extraction_LM.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-o $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=LM_pmi.csv \\\n",
    "-r has_benefits \\\n",
    "-ee $base_dir/data/$data_ac/intermediate/concept_knn_100.csv \\\n",
    "-lm pmi \\\n",
    "-topk 300 \\\n",
    "-dim 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Evaluation \n",
    "\n",
    "def load_benchmark(benchmark_full_path,\n",
    "                   seed_concepts_path,\n",
    "                   seed_relations_path):\n",
    "    benchmark = pd.read_csv(benchmark_full_path)\n",
    "    concepts_df = load_seed_aligned_concepts(seed_concepts_path)\n",
    "    relations_df = load_seed_aligned_relations(seed_relations_path)\n",
    "    \n",
    "    concepts_dict = dict(zip(concepts_df['alignedCategoryName'].tolist(), concepts_df.to_dict('records')))\n",
    "    relations_dict = dict(zip(relations_df['alignedRelationName'].tolist(), relations_df.to_dict('records')))\n",
    "    \n",
    "    # Dict[str(_type), Set[str(_e)]]\n",
    "    all_concepts = defaultdict(set)\n",
    "    # Dict[str(_r), Set[Tuple(_h, _r, _t)]]\n",
    "    all_rel_tuples = defaultdict(set)\n",
    "    \n",
    "    for i, row in benchmark.iterrows():\n",
    "        if row['type'] != 'fact':\n",
    "            continue\n",
    "        \n",
    "        _r = row['relation_name']\n",
    "        _h_type = row['n_head_category']\n",
    "        _t_type = row['n_tail_category']\n",
    "        \n",
    "        if _r not in relations_dict:\n",
    "            continue\n",
    "        _relation_row = relations_dict[_r]\n",
    "        if _relation_row['domain'] != _h_type or _relation_row['range'] != _t_type:\n",
    "            continue\n",
    "        \n",
    "        row_n_head = str(row['n_head']).lower()\n",
    "        row_n_tail = str(row['n_tail']).lower()\n",
    "        \n",
    "        if row_n_head == 'company':\n",
    "            evidence_sents = eval(str(row['sentences']))\n",
    "            head_instances = eval(str(row['Evidence']))\n",
    "            assert len(evidence_sents) == len(head_instances), f'Line {i} length mismatch'\n",
    "\n",
    "            for inst in head_instances:\n",
    "                if len(inst) > 0:\n",
    "                    all_concepts[_h_type].add(inst.lower())\n",
    "                    all_concepts[_t_type].add(row_n_tail)\n",
    "                    all_rel_tuples[_r].add(\n",
    "                        (inst.lower(), _r, row_n_tail)\n",
    "                    )\n",
    "        else:\n",
    "            # treat n_head directly as instance \n",
    "            all_concepts[_h_type].add(row_n_head)\n",
    "            all_concepts[_t_type].add(row_n_tail)\n",
    "            all_rel_tuples[_r].add(\n",
    "                (row_n_head, _r, row_n_tail)\n",
    "            )\n",
    "        \n",
    "    return all_concepts, all_rel_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 14)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_path = os.path.join(base_dir, f'data/indeed-benchmark/benchmark_evidence_clean.csv')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations_nodup.csv')\n",
    "\n",
    "benchmark_concepts, benchmark_relations = load_benchmark(benchmark_full_path=benchmark_path,\n",
    "                                              seed_concepts_path=seed_aligned_concepts_path,\n",
    "                                              seed_relations_path=seed_aligned_relations_path)\n",
    "\n",
    "len(benchmark_concepts), len(benchmark_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_extraction_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "\n",
    "rel_extraction = pd.read_csv(rel_extraction_path)\n",
    "rel_extraction_list = rel_extraction[['head', 'tail']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 3597, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_relations_set = set([tuple(d.values()) for d in benchmark_relations_list])\n",
    "rel_extraction_set = set([tuple(d.values()) for d in rel_extraction_list])\n",
    "\n",
    "intersection = benchmark_relations_set & rel_extraction_set\n",
    "\n",
    "len(benchmark_relations_set), len(rel_extraction_set), len(intersection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 107\r\n",
      "Predicted relations: 3600\r\n",
      "Intersection: 15\r\n",
      "P = 0.0042, R = 0.1402, F1 = 0.0081\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('jcpenney', 'has_dress_code', 'uniform policy')\r\n",
      "('olive garden', 'has_dress_code', 'facial hair')\r\n",
      "('walgreens', 'has_dress_code', 'hair color')\r\n",
      "('marshalls', 'has_dress_code', 'color hair')\r\n",
      "('at&t', 'has_dress_code', 'uniform')\r\n",
      "('costco', 'has_dress_code', 'hair color')\r\n",
      "('walmart', 'has_dress_code', 'uniform')\r\n",
      "('taco bell', 'has_dress_code', 'nose rings')\r\n",
      "('walmart', 'has_dress_code', 'face tattoos')\r\n",
      "('dollar general', 'has_dress_code', 'strict dress code')\r\n",
      "('best buy', 'has_dress_code', 'uniform')\r\n",
      "('family dollar', 'has_dress_code', 'facial hair')\r\n",
      "('subway', 'has_dress_code', 'piercings')\r\n",
      "('dollar tree', 'has_dress_code', 'uniform')\r\n",
      "('dollar tree', 'has_dress_code', 'professional')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_cartesian-has_dress_code.csv \\\n",
    "-r has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 107\r\n",
      "Predicted relations: 1018\r\n",
      "Intersection: 13\r\n",
      "P = 0.0128, R = 0.1215, F1 = 0.0231\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('taco bell', 'has_dress_code', 'nose rings')\r\n",
      "('olive garden', 'has_dress_code', 'facial hair')\r\n",
      "('costco', 'has_dress_code', 'hair color')\r\n",
      "('jcpenney', 'has_dress_code', 'uniform policy')\r\n",
      "('dollar tree', 'has_dress_code', 'uniform')\r\n",
      "('best buy', 'has_dress_code', 'uniform')\r\n",
      "('marshalls', 'has_dress_code', 'color hair')\r\n",
      "('walmart', 'has_dress_code', 'uniform')\r\n",
      "('subway', 'has_dress_code', 'piercings')\r\n",
      "('dollar tree', 'has_dress_code', 'professional')\r\n",
      "('walgreens', 'has_dress_code', 'hair color')\r\n",
      "('at&t', 'has_dress_code', 'uniform')\r\n",
      "('family dollar', 'has_dress_code', 'facial hair')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_dress_code-RE=Ct+KV=0.9.csv \\\n",
    "-r has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 107\r\n",
      "Predicted relations: 300\r\n",
      "Intersection: 0\r\n",
      "P = 0.0000, R = 0.0000, F1 = 0.0000\r\n",
      "\r\n",
      "Intersection:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_dress_code-RE=LM_bert.csv \\\n",
    "-r has_dress_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 107\r\n",
      "Predicted relations: 76\r\n",
      "Intersection: 0\r\n",
      "P = 0.0000, R = 0.0000, F1 = 0.0000\r\n",
      "\r\n",
      "Intersection:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_dress_code-RE=avg+KV=0.9.csv \\\n",
    "-r has_dress_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### has_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 56\r\n",
      "Predicted relations: 3600\r\n",
      "Intersection: 11\r\n",
      "P = 0.0031, R = 0.1964, F1 = 0.0060\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('burger king', 'has_benefits', 'health')\r\n",
      "('burger king', 'has_benefits', 'sick leave')\r\n",
      "('walmart', 'has_benefits', '401k')\r\n",
      "('walmart', 'has_benefits', '401k plan')\r\n",
      "('olive garden', 'has_benefits', '401k plan')\r\n",
      "('g4s', 'has_benefits', 'sick leave')\r\n",
      "('walmart', 'has_benefits', 'life insurance')\r\n",
      "('target', 'has_benefits', 'health insurance')\r\n",
      "('walmart', 'has_benefits', 'paid vacations')\r\n",
      "('starbucks', 'has_benefits', 'health')\r\n",
      "('dollar general', 'has_benefits', 'health insurance')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_cartesian-has_benefits.csv \\\n",
    "-r has_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 56\r\n",
      "Predicted relations: 1312\r\n",
      "Intersection: 11\r\n",
      "P = 0.0084, R = 0.1964, F1 = 0.0161\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('starbucks', 'has_benefits', 'health')\r\n",
      "('dollar general', 'has_benefits', 'health insurance')\r\n",
      "('walmart', 'has_benefits', '401k plan')\r\n",
      "('walmart', 'has_benefits', 'life insurance')\r\n",
      "('target', 'has_benefits', 'health insurance')\r\n",
      "('burger king', 'has_benefits', 'health')\r\n",
      "('walmart', 'has_benefits', 'paid vacations')\r\n",
      "('burger king', 'has_benefits', 'sick leave')\r\n",
      "('g4s', 'has_benefits', 'sick leave')\r\n",
      "('walmart', 'has_benefits', '401k')\r\n",
      "('olive garden', 'has_benefits', '401k plan')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=Ct+KV=0.9.csv \\\n",
    "-r has_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 56\r\n",
      "Predicted relations: 300\r\n",
      "Intersection: 0\r\n",
      "P = 0.0000, R = 0.0000, F1 = 0.0000\r\n",
      "\r\n",
      "Intersection:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=LM_bert.csv \\\n",
    "-r has_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 56\r\n",
      "Predicted relations: 300\r\n",
      "Intersection: 1\r\n",
      "P = 0.0033, R = 0.0179, F1 = 0.0056\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('dollar general', 'has_benefits', 'health insurance')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=LM_joint.csv \\\n",
    "-r has_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RE Results ---\r\n",
      "Benchmark relations: 56\r\n",
      "Predicted relations: 300\r\n",
      "Intersection: 2\r\n",
      "P = 0.0067, R = 0.0357, F1 = 0.0112\r\n",
      "\r\n",
      "Intersection:\r\n",
      "('dollar general', 'has_benefits', 'health insurance')\r\n",
      "('target', 'has_benefits', 'health insurance')\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python eval_relations.py \\\n",
    "-b $base_dir/data/indeed-benchmark \\\n",
    "-pred $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE=LM_gpt2.csv \\\n",
    "-r has_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMProbe-Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,    11,   616,  3290,   318, 13779]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_inputs = gpt2_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _outputs = gpt2_model(**_inputs, labels=_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9901607036590576"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_outputs.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lm_probe = LMProbe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_probe.fill_multi_mask(\"[CLS] The payment [MASK] [MASK] is nice. [SEP]\", topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "lm_probe_gpt2 = LMProbe_GPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cand': 'good plan', 'score': -4.1346964836120605},\n",
       " {'cand': 'good', 'score': -4.260444641113281},\n",
       " {'cand': 'plan', 'score': -5.5930562019348145}]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probe_gpt2.score_candidates(\"They have a very [MASK] .\", [\"good\", \"plan\", \"good plan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.47902297973633"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sent = \"Hello ! I would like to have a pizzaaaa\"\n",
    "_inputs = lm_probe_joint.gpt2_tokenizer(_sent, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    _outputs = lm_probe_joint.gpt2_model(**_inputs, labels=_inputs[\"input_ids\"])\n",
    "_outputs.loss.item() * (len(_sent.split(' ')) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 5145, 314, 561, 588, 284, 423, 257, 14256, 46071], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probe_joint.gpt2_tokenizer(_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,  5145,   314,   561,   588,   284,   423,   257, 14256, 46071]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Ġ!', 'ĠI', 'Ġwould', 'Ġlike', 'Ġto', 'Ġhave', 'Ġa', 'Ġpizza', 'aaa']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probe_joint.gpt2_tokenizer.convert_ids_to_tokens([15496,  5145,   314,   561,   588,   284,   423,   257, 14256, 46071])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "# LMProbe_Joint with bert probs renormalized by gpt2 \n",
    "\n",
    "def _bert_untokenize(pieces):\n",
    "    return ' '.join(pieces).replace(' ##', '')\n",
    "\n",
    "class LMProbe_Joint(object):\n",
    "    def __init__(self,\n",
    "                 bert_model_name='bert-base-uncased',\n",
    "                 gpt2_model_name='gpt2',\n",
    "                 max_n_grams=5,\n",
    "                 use_gpu=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "        self.bert_model.to(self.device)\n",
    "        self.bert_model.eval()\n",
    "        self.bert_mask_token = self.bert_tokenizer.mask_token\n",
    "        \n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name, return_dict=True)\n",
    "        self.gpt2_model.to(self.device)\n",
    "        self.gpt2_model.eval()\n",
    "        \n",
    "        self.max_n_grams = max_n_grams\n",
    "        \n",
    "    def joint_score_candidates(self, input_txt, cands, renorm_n=10):\n",
    "        # cands: List[str], list candidates (untokenzied) \n",
    "        \n",
    "        if input_txt.count(\"[MASK]\") != 1:\n",
    "            raise Exception(f'Input string must have exactly one mask token, got {input_txt}')\n",
    "\n",
    "        cand_bins = {i : [] for i in range(1, self.max_n_grams + 1)}\n",
    "        for c in cands:\n",
    "            c_tokenized = self.bert_tokenizer.tokenize(c)\n",
    "            if len(c_tokenized) > self.max_n_grams:\n",
    "                print(f'{c_tokenized}: too many wordpieces')\n",
    "                continue\n",
    "            cand_bins[len(c_tokenized)].append(c_tokenized)\n",
    "        \n",
    "        all_cand_scores = []\n",
    "        for c_len in range(1, self.max_n_grams + 1):\n",
    "            _cands = cand_bins[c_len]\n",
    "            if len(_cands) == 0:\n",
    "                continue\n",
    "            \n",
    "            _input = \"[CLS] \" + input_txt.replace(\"[MASK]\", \"[MASK]\" + \" [MASK]\" * (c_len - 1)) + \" [SEP]\"\n",
    "            _cand_scores = self.bert_score_candidates(_input, _cands)\n",
    "            \n",
    "            _renorm_cand_dicts = _cand_scores[:renorm_n]\n",
    "            _renorm_bert_scores = {_bert_untokenize(d['cand']) : d['score'] for d in _renorm_cand_dicts}\n",
    "            _renorm_cands = list(_renorm_bert_scores.keys())\n",
    "            \n",
    "            _gpt2_cand_scores = self.gpt2_score_candidates(input_txt, _renorm_cands)\n",
    "            _renorm_gpt2_scores = {d['cand'] : d['score'] for d in _gpt2_cand_scores}\n",
    "            \n",
    "#             print('BERT scores:')\n",
    "#             print(json.dumps(_renorm_bert_scores, indent=2))\n",
    "#             print('GPT2 scores:')\n",
    "#             print(json.dumps(_renorm_gpt2_scores, indent=2))\n",
    "#             if len(_renorm_cands) > 2:\n",
    "#                 print('Pearson:')\n",
    "#                 print(pearsonr(\n",
    "#                     np.exp([_renorm_bert_scores[c] for c in _renorm_cands]),\n",
    "#                     np.exp([_renorm_gpt2_scores[c] for c in _renorm_cands])))\n",
    "            \n",
    "            # bert_ll + _renorm_bias -> gpt2_ll\n",
    "            _renorm_bias = np.log(np.sum(np.exp(list(_renorm_gpt2_scores.values())))) \\\n",
    "                - np.log(np.sum(np.exp(list(_renorm_bert_scores.values()))))\n",
    "            \n",
    "            _gpt2_len = len(self.gpt2_tokenizer(input_txt.replace('[MASK]', _renorm_cands[0]))['input_ids'])\n",
    "            \n",
    "            _renormed_cand_scores = [\n",
    "                {'cand': _bert_untokenize(d['cand']),\n",
    "                 'score': (d['score'] + _renorm_bias) / _gpt2_len}\n",
    "                for d in _cand_scores\n",
    "            ]\n",
    "            all_cand_scores.extend(_renormed_cand_scores)\n",
    "        \n",
    "        all_cand_scores.sort(key=lambda d : d['score'], reverse=True)\n",
    "        return all_cand_scores\n",
    "    \n",
    "    \n",
    "    def bert_score_candidates(self, input_txt, cands):\n",
    "        # cands: List[List[str]], list of tokenized candidates \n",
    "        tokenized_txt = self.bert_tokenizer.tokenize(input_txt)\n",
    "        \n",
    "        if tokenized_txt[0] != \"[CLS]\" or tokenized_txt[-1] != \"[SEP]\":\n",
    "            raise Exception(f'Input string must start with [CLS] and end with [SEP], got {input_txt}')\n",
    "        if \"[MASK]\" not in tokenized_txt:\n",
    "            raise Exception(f'Input string must have at least one mask token, got {input_txt}')\n",
    "        \n",
    "        indexed_tokens = self.bert_tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        mask_indices = [i for i, x in enumerate(tokenized_txt) if x == \"[MASK]\"]\n",
    "        segment_idx = tokens_tensor * 0\n",
    "        tokens_tensor = tokens_tensor.to(self.device)\n",
    "        segments_tensors = segment_idx.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        probs = torch.softmax(predictions, dim=-1)[0]\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "\n",
    "        cand_scores = []\n",
    "        for c in cands:\n",
    "            assert len(c) == len(mask_indices), f'cand {c}; len(mask_indices) = {len(mask_indices)}'\n",
    "\n",
    "            _scores = []\n",
    "            c_token_ids = self.bert_tokenizer.convert_tokens_to_ids(c)\n",
    "            for i, token_id in zip(mask_indices, c_token_ids):\n",
    "                _scores.append(probs[i, token_id].item())\n",
    "            score = np.sum(np.log(_scores))  # sum(log(p))\n",
    "            cand_scores.append({\"cand\": c, \"score\": score})\n",
    "\n",
    "        cand_scores.sort(key=lambda d : d[\"score\"], reverse=True)\n",
    "        return cand_scores\n",
    "    \n",
    "    def gpt2_score_candidates(self, input_txt, cands):\n",
    "        # cands: List[str], list candidates (untokenzied) \n",
    "        \n",
    "        if input_txt.count(\"[MASK]\") != 1:\n",
    "            raise Exception(f'Input string must have exactly one mask token, got {input_txt}')\n",
    "\n",
    "        cand_scores = []\n",
    "        for c in cands:\n",
    "            cand_input_txt = input_txt.replace(\"[MASK]\", c)\n",
    "            tokenized_input = self.gpt2_tokenizer(cand_input_txt, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                model_outputs = self.gpt2_model(**tokenized_input, labels=tokenized_input[\"input_ids\"])\n",
    "                \n",
    "            _input_len = tokenized_input['input_ids'].size(1)\n",
    "            score = -model_outputs.loss.item() * (_input_len - 1)  # (log(p))\n",
    "            cand_scores.append({\"cand\": c, \"score\": score})\n",
    "\n",
    "        cand_scores.sort(key=lambda d : d[\"score\"], reverse=True)\n",
    "        return cand_scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lm_probe_joint = LMProbe_Joint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT scores:\n",
      "{\n",
      "  \"good\": -3.820184215986424,\n",
      "  \"plan\": -5.464548142423097,\n",
      "  \"nice\": -5.78993798238308\n",
      "}\n",
      "GPT2 scores:\n",
      "{\n",
      "  \"good\": -21.302223205566406,\n",
      "  \"nice\": -23.64457607269287,\n",
      "  \"plan\": -27.965281009674072\n",
      "}\n",
      "Pearson:\n",
      "(0.9899664636147664, 0.09025804686068711)\n",
      "BERT scores:\n",
      "{\n",
      "  \"good plan\": -7.813247270006006\n",
      "}\n",
      "GPT2 scores:\n",
      "{\n",
      "  \"good plan\": -24.808178901672363\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cand': 'good plan', 'score': -3.544025557381766},\n",
       " {'cand': 'good', 'score': -3.582741819734341},\n",
       " {'cand': 'plan', 'score': -3.856802474140453},\n",
       " {'cand': 'nice', 'score': -3.9110341141337837}]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_probe_joint.joint_score_candidates(\"They have a very [MASK] .\", [\"good\", \"plan\", \"nice\", \"good plan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Verification baseline\n",
    "(finding co-occurrences of head / tail from corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel_extraction_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "# # corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences.json')\n",
    "\n",
    "# indeed_dataset_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/question_answers.csv'\n",
    "# company_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/fccid-companyName.csv'\n",
    "\n",
    "# # with open(corpus_path, 'r') as f:\n",
    "# #     sent_dicts = [json.loads(l) for l in tqdm(f.readlines())]\n",
    "\n",
    "# indeed_dataset = pd.read_csv(indeed_dataset_path)\n",
    "# indeed_dataset = indeed_dataset[indeed_dataset['answerContent'].notna()]\n",
    "# company_df = pd.read_csv(company_path)\n",
    "# company_dict = dict(zip(company_df[\"fccompanyId\"].to_list(), company_df[\"companyName\"].to_list()))\n",
    "\n",
    "# indeed_dataset.shape, len(company_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_extraction_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences_with_company.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 7)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relations = pd.read_csv(rel_extraction_path)\n",
    "df_relations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1819f0b55e494a85aab302ac773cb8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=413232.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "413232"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(corpus_path, 'r') as f:\n",
    "    sent_dicts = [json.loads(l) for l in tqdm(f.readlines())]\n",
    "\n",
    "len(sent_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['What', 'is', 'the', 'age', 'limit', '.'],\n",
       " 'company': 'Marshalls',\n",
       " 'entities': ['age limit', 'marshalls']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dicts[1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entailment model \n",
    "yutong_base_dir = \"/home/ubuntu/users/yutong\"\n",
    "roberta_ses_dir = os.path.join(yutong_base_dir, \"repos\", \"Roberta_SES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = contra, 1 = neutral, 2 = entail\n",
    "entailment_model = Roberta_SES_Entailment(roberta_path='/home/ubuntu/users/yutong/models/roberta-large',\n",
    "        ckpt_path=os.path.join(roberta_ses_dir, 'checkpoints/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt'),\n",
    "        max_length=512,\n",
    "        device_name='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor([0.9978, 0.0012, 0.0010]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailment_model.predict(\n",
    "    \"walmart : no you can have tattoo\",\n",
    "    \"walmart allows tattoos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": [
     12
    ]
   },
   "outputs": [],
   "source": [
    "# KV for (walmart, has_dress_code, uniform)\n",
    "\n",
    "_pos_templates = [\n",
    "    '{0} allows {1}',\n",
    "    '{0} requires {1}',\n",
    "]\n",
    "\n",
    "_neg_templates = [\n",
    "    '{0} doesn\\'t allow {1}',\n",
    "    '{0} doesn\\'t require {1}',\n",
    "]\n",
    "\n",
    "def find_evidences(head, tail, corpus=sent_dicts):\n",
    "    # (s1(evid), s2(tmpl), score)\n",
    "    _pos_evidences = []\n",
    "    _neg_evidences = []\n",
    "\n",
    "    for i, d in enumerate(sent_dicts):\n",
    "        if i > 0 and i % 50000 == 0:\n",
    "            print(f'Progress: {i} / {len(sent_dicts)}')\n",
    "            \n",
    "#         _company_id = row['fccompanyId']\n",
    "#         _company = company_dict[_company_id]\n",
    "\n",
    "#         _answer = row['answerContent']\n",
    "#         _tokens = [str(t) for t in spacy_tokenizer(_answer)]\n",
    "#         _s = f\" {_company.lower()} : {' '.join(_tokens).lower()} \"\n",
    "        _company = d['company']\n",
    "        _tokens = d['tokens']\n",
    "        _s = f\"{_company.lower()} : {' '.join(_tokens).lower()}\"\n",
    "\n",
    "        if head in d['entities'] and tail in d['entities']:\n",
    "            # Try all pos/neg relation templates, save the best template  \n",
    "            _max_pos_ev = (None, None, 0)\n",
    "            for _tmpl in _pos_templates:\n",
    "                _tmpl_filled = _tmpl.format(head, tail)\n",
    "                _entail_pred, _entail_probs = entailment_model.predict(_s, _tmpl_filled)\n",
    "                _entail_score = _entail_probs[2].item()\n",
    "                if _entail_score > _max_pos_ev[-1]:\n",
    "                    _max_pos_ev = (_s, _tmpl_filled, _entail_score)\n",
    "\n",
    "            _max_neg_ev = (None, None, 0)\n",
    "            for _tmpl in _neg_templates:\n",
    "                _tmpl_filled = _tmpl.format(head, tail)\n",
    "                _entail_pred, _entail_probs = entailment_model.predict(_s, _tmpl_filled)\n",
    "                _entail_score = _entail_probs[2].item()\n",
    "                if _entail_score > _max_neg_ev[-1]:\n",
    "                    _max_neg_ev = (_s, _tmpl_filled, _entail_score)\n",
    "\n",
    "            _pos_evidences.append(_max_pos_ev)\n",
    "            _neg_evidences.append(_max_neg_ev)\n",
    "    \n",
    "    _pos_evidences.sort(key=lambda p : p[-1], reverse=True)\n",
    "    _neg_evidences.sort(key=lambda p : p[-1], reverse=True)\n",
    "    \n",
    "    return _pos_evidences, _neg_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_pos_evidences, _neg_evidences = find_evidences('walmart', 'black jeans')\n",
    "'POS:', _pos_evidences[:10], 'NEG:', _neg_evidences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def find_evidences_RE(df_relations, corpus=sent_dicts, p_thres=0.7):\n",
    "    ## TODO: to script \n",
    "    \n",
    "    # Dict[Tuple(head, rel, tail): List[Tuple(s1(evid), s2(tmpl), score)]]\n",
    "    pos_evidences = defaultdict(list)\n",
    "    neg_evidences = defaultdict(list)\n",
    "    \n",
    "    # collect all relations \n",
    "    rels = []\n",
    "    head2rels = defaultdict(list)\n",
    "    tail2rels = defaultdict(list)\n",
    "    for i, row in df_relations.iterrows():\n",
    "        _h = row['head']\n",
    "        _t = row['tail']\n",
    "        _r = 'has_dress_code'\n",
    "        rels.append((_h, _r, _t))\n",
    "        if row['base'] == 'HEAD':\n",
    "            head2rels[_h].append((_h, _r, _t))\n",
    "        else:\n",
    "            tail2rels[_t].append((_h, _r, _t))\n",
    "\n",
    "    # collect sents for each entity \n",
    "    entity2sents = defaultdict(set)\n",
    "    for i, d in enumerate(sent_dicts):\n",
    "        _s = f\"{d['company']} : {' '.join(d['tokens'])}\".lower()\n",
    "        for _e in d['entities']:\n",
    "            entity2sents[_e].add(_s)\n",
    "    \n",
    "    for _h, _r, _t in tqdm(rels[200::200]):\n",
    "        # assure key existence\n",
    "        _ = pos_evidences[(_h, _r, _t)]\n",
    "        _ = neg_evidences[(_h, _r, _t)]\n",
    "        \n",
    "        h_sents = entity2sents[_h]\n",
    "        t_sents = entity2sents[_t]\n",
    "        intersect_sents = h_sents & t_sents\n",
    "        \n",
    "        for _s in intersect_sents:\n",
    "            _ss = _s.strip()\n",
    "\n",
    "            # Try all pos/neg relation templates, save the best template  \n",
    "            _max_pos_ev = (None, None, 0)\n",
    "            for _tmpl in _pos_templates:\n",
    "                _tmpl_filled = _tmpl.format(_h, _t)\n",
    "                _entail_pred, _entail_probs = entailment_model.predict(_ss, _tmpl_filled)\n",
    "                _entail_score = _entail_probs[2].item()\n",
    "                if _entail_score > _max_pos_ev[-1]:\n",
    "                    _max_pos_ev = (_ss, _tmpl_filled, _entail_score)\n",
    "\n",
    "            _max_neg_ev = (None, None, 0)\n",
    "            for _tmpl in _neg_templates:\n",
    "                _tmpl_filled = _tmpl.format(_h, _t)\n",
    "                _entail_pred, _entail_probs = entailment_model.predict(_ss, _tmpl_filled)\n",
    "                _entail_score = _entail_probs[2].item()\n",
    "                if _entail_score > _max_neg_ev[-1]:\n",
    "                    _max_neg_ev = (_ss, _tmpl_filled, _entail_score)\n",
    "\n",
    "            if _max_pos_ev[-1] > p_thres:\n",
    "                pos_evidences[(_h, _r, _t)].append(_max_pos_ev)\n",
    "            if _max_neg_ev[-1] > p_thres:\n",
    "                neg_evidences[(_h, _r, _t)].append(_max_neg_ev)\n",
    "    \n",
    "    \n",
    "#     # Head-base\n",
    "#     for _h, _rels in rel_head_index.items():\n",
    "#         # First find sentences with _h\n",
    "#         _h_sents = [] \n",
    "#         for i, d in enumerate(sent_dicts):\n",
    "#             _company = d['company']\n",
    "#             _tokens = d['tokens']\n",
    "#             _s = f\" {_company.lower()} : {' '.join(_tokens).lower()} \"\n",
    "#             if f' {_h} ' in _s:\n",
    "#                 _h_sents.append(_s)\n",
    "        \n",
    "#         # KV: check for _t only; entail\n",
    "#         for _h, _r, _t in tqdm(_rels):\n",
    "#             for _s in _h_sents:\n",
    "#                 if f' {_t} ' in _s:\n",
    "#                     _ss = _s.strip()\n",
    "                    \n",
    "#                     # Try all pos/neg relation templates, save the best template  \n",
    "#                     _max_pos_ev = (None, None, 0)\n",
    "#                     for _tmpl in _pos_templates:\n",
    "#                         _tmpl_filled = _tmpl.format(_h, _t)\n",
    "#                         _entail_pred, _entail_probs = entailment_model.predict(_ss, _tmpl_filled)\n",
    "#                         _entail_score = _entail_probs[2].item()\n",
    "#                         if _entail_score > _max_pos_ev[-1]:\n",
    "#                             _max_pos_ev = (_ss, _tmpl_filled, _entail_score)\n",
    "\n",
    "#                     _max_neg_ev = (None, None, 0)\n",
    "#                     for _tmpl in _neg_templates:\n",
    "#                         _tmpl_filled = _tmpl.format(_h, _t)\n",
    "#                         _entail_pred, _entail_probs = entailment_model.predict(_ss, _tmpl_filled)\n",
    "#                         _entail_score = _entail_probs[2].item()\n",
    "#                         if _entail_score > _max_neg_ev[-1]:\n",
    "#                             _max_neg_ev = (_ss, _tmpl_filled, _entail_score)\n",
    "\n",
    "#                     if _max_pos_ev[-1] > p_thres:\n",
    "#                         pos_evidences[(_h, _r, _t)].append(_max_pos_ev)\n",
    "#                     if _max_neg_ev[-1] > p_thres:\n",
    "#                         neg_evidences[(_h, _r, _t)].append(_max_neg_ev)\n",
    "\n",
    "#     # Tail-base\n",
    "#     for _t, _rels in tqdm(rel_tail_index.items(), total=len(rel_tail_index)):\n",
    "#         # First find sentences with _t\n",
    "#         _t_sents = [] \n",
    "#         for i, d in enumerate(sent_dicts):\n",
    "#             _company = d['company']\n",
    "#             _tokens = d['tokens']\n",
    "#             _s = f\" {_company.lower()} : {' '.join(_tokens).lower()} \"\n",
    "#             if f' {_t} ' in _s:\n",
    "#                 _t_sents.append(_s)\n",
    "        \n",
    "#         # KV: check for _h only; entail\n",
    "#         for _h, _r, _t in tqdm(_rels):\n",
    "#             if (_h, _r, _t) in pos_evidences or (_h, _r, _t) in neg_evidences:\n",
    "#                 # already computed \n",
    "#                 continue\n",
    "#             for _s in _t_sents:\n",
    "#                 if f' {_h} ' in _s:\n",
    "#                     _ss = _s.strip()\n",
    "                    \n",
    "#                     # Try all pos/neg relation templates, save the best template  \n",
    "#                     _max_pos_ev = (None, None, 0)\n",
    "#                     for _tmpl in _pos_templates:\n",
    "#                         _tmpl_filled = _tmpl.format(head, tail)\n",
    "#                         _entail_pred, _entail_probs = entailment_model.predict(_ss, _tmpl_filled)\n",
    "#                         _entail_score = _entail_probs[2].item()\n",
    "#                         if _entail_score > _max_pos_ev[-1]:\n",
    "#                             _max_pos_ev = (_ss, _tmpl_filled, _entail_score)\n",
    "\n",
    "#                     _max_neg_ev = (None, None, 0)\n",
    "#                     for _tmpl in _neg_templates:\n",
    "#                         _tmpl_filled = _tmpl.format(head, tail)\n",
    "#                         _entail_pred, _entail_probs = entailment_model.predict(_ss, _tmpl_filled)\n",
    "#                         _entail_score = _entail_probs[2].item()\n",
    "#                         if _entail_score > _max_neg_ev[-1]:\n",
    "#                             _max_neg_ev = (_ss, _tmpl_filled, _entail_score)\n",
    "\n",
    "#                     if _max_pos_ev[-1] > p_thres:\n",
    "#                         pos_evidences[(_h, _r, _t)].append(_max_pos_ev)\n",
    "#                     if _max_neg_ev[-1] > p_thres:\n",
    "#                         neg_evidences[(_h, _r, _t)].append(_max_neg_ev)\n",
    "\n",
    "    for _rel, _evidences in pos_evidences.items():\n",
    "        _evidences.sort(key=lambda p : p[-1], reverse=True)\n",
    "    for _rel, _evidences in neg_evidences.items():\n",
    "        _evidences.sort(key=lambda p : p[-1], reverse=True)\n",
    "    \n",
    "    return pos_evidences, neg_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = find_evidences_RE(df_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use script \n",
    "!python knowledge_verification_entail.py \\\n",
    "-d $base_dir/data/$data_ac/intermediate \\\n",
    "-in $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE.csv \\\n",
    "-o_kv $base_dir/data/$data_ac/intermediate/kv_evidences-has_benefits-RE.json \\\n",
    "-o_re $base_dir/data/$data_ac/intermediate/rel_extraction-has_benefits-RE+KV_0.9.csv \\\n",
    "-r $yutong_base_dir/models/roberta-large \\\n",
    "-rs $yutong_base_dir/repos/Roberta_SES/checkpoints/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt \\\n",
    "-p_kv 0.7 \\\n",
    "-p_re 0.9 \\\n",
    "--fast_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate \n",
    "benchmark_path = os.path.join(base_dir, f'data/indeed-benchmark/benchmark_evidence_clean.csv')\n",
    "\n",
    "benchmark_relations_list = load_benchmark_relations(benchmark_path)\n",
    "len(benchmark_relations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_evidences_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/kv_evidences.json')\n",
    "\n",
    "with open(kv_evidences_path, 'r') as f:\n",
    "    kv_evidences = [json.loads(l) for l in f.readlines()]\n",
    "len(kv_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_thres = 0.9\n",
    "\n",
    "kv_filtered_rels = []\n",
    "for d in kv_evidences:\n",
    "    _h, _r, _t = d['relation']\n",
    "    _pos_evs = d['pos_evidences']\n",
    "    _neg_evs = d['neg_evidences']\n",
    "    if (len(_pos_evs) > 0 and _pos_evs[0][-1] > p_thres):\n",
    "        kv_filtered_rels.append((_h, _t))\n",
    "    elif (len(_neg_evs) > 0 and _neg_evs[0][-1] > p_thres):\n",
    "        kv_filtered_rels.append((_h, _t))\n",
    "\n",
    "len(kv_filtered_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 665, 6)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_relations_set = set([tuple(d.values()) for d in benchmark_relations_list])\n",
    "kv_filtered_rels_set = set(kv_filtered_rels)\n",
    "\n",
    "intersection = benchmark_relations_set & kv_filtered_rels_set\n",
    "\n",
    "len(benchmark_relations_set), len(kv_filtered_rels_set), len(intersection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('best buy', 'uniform'),\n",
       " ('costco', 'hair color'),\n",
       " ('dd', 'facial hair'),\n",
       " ('dollar tree', 'uniform'),\n",
       " ('family dollar', 'facial hair'),\n",
       " ('walmart', 'uniform')}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in kv_evidences:\n",
    "    _h, _r, _t = d['relation']\n",
    "    _pos_evs = d['pos_evidences']\n",
    "    _neg_evs = d['neg_evidences']\n",
    "    \n",
    "    if (_h, _t) in intersection:\n",
    "        print(_h, _t)\n",
    "        _max = sorted(_pos_evs + _neg_evs, key=lambda p : p[-1], reverse=True)[0]\n",
    "        print(_max)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kv_evidences = list(kv_evidences)\n",
    "\n",
    "for d in test_kv_evidences:\n",
    "    _h, _r, _t = d['relation']\n",
    "    _pos_evs = d['pos_evidences'][:5]\n",
    "    _neg_evs = d['neg_evidences'][:5]\n",
    "    \n",
    "    _max = sorted(_pos_evs + _neg_evs, key=lambda p : p[-1], reverse=True)[0]\n",
    "    d['pos_evidences'] = _pos_evs\n",
    "    d['neg_evidences'] = _neg_evs\n",
    "    d['max_ev'] = _max\n",
    "\n",
    "sorted(test_kv_evidences, key=lambda d : d['max_ev'][-1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussions:\n",
    "# coherence clustering / ensemble models?\n",
    "# trying for other relations or entities\n",
    "# using entities in sub-categories\n",
    "# fine-tuning\n",
    "# ambiguous samples (high for pos and neg)\n",
    "# quantitative-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore various techniques\n",
    "# Get prompts \"between\" entities\n",
    "# Get prompts by syntactic parsing\n",
    "# Get prompts by paraphrasing\n",
    "# Get prompts uisng AutoPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit here: /meg-kb/src/analysis/pattern_mining.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Amazon []\n",
      "1 is [Amazon, place, .]\n",
      "2 a []\n",
      "3 good []\n",
      "4 place [a, good, for]\n",
      "5 for [working]\n",
      "6 working [as]\n",
      "7 as [time]\n",
      "8 a []\n",
      "9 part []\n",
      "10 - []\n",
      "11 time [a, part, -]\n",
      "12 . []\n"
     ]
    }
   ],
   "source": [
    "_sent = \"Amazon is a good place for working as a part-time.\"\n",
    "_doc = _nlp(_sent)\n",
    "for _t in _doc:\n",
    "    print(_t.i, _t, list(_t.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7fbf27eb0430>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = []\n",
    "for _t in _doc:\n",
    "    for child in _t.children:\n",
    "        edges.append(('{}-{}'.format(_t.lower_,_t.i), '{}-{}'.format(child.lower_,child.i))) \n",
    "\n",
    "graph = nx.Graph(edges)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeView([('is-1', 'amazon-0'), ('is-1', 'place-4'), ('is-1', '.-12'), ('place-4', 'a-2'), ('place-4', 'good-3'), ('place-4', 'for-5'), ('for-5', 'working-6'), ('working-6', 'as-7'), ('as-7', 'time-11'), ('time-11', 'a-8'), ('time-11', 'part-9'), ('time-11', '--10')])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon-0', 'is-1', 'place-4', 'for-5', 'working-6', 'as-7', 'time-11']\n"
     ]
    }
   ],
   "source": [
    "_src = 'amazon-0'\n",
    "_tgt = 'time-11'\n",
    "if nx.has_path(graph, source=_src, target=_tgt):\n",
    "    path = nx.shortest_path(graph, source=_src, target=_tgt)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'amazon-0' in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('is-1', 'amazon-0') in graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using prompts to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences_with_company.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413232"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(corpus_path, 'r') as f:\n",
    "    corpus = [json.loads(l) for l in f]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prompt_retrieve(prompt):\n",
    "    ret = []\n",
    "    \n",
    "    for d in corpus:\n",
    "        _entities = d['entities']\n",
    "        _tokens = d['tokens']\n",
    "        if len(_entities) < 2:\n",
    "            continue\n",
    "\n",
    "        _sent = ' '.join(_tokens).lower()\n",
    "        if f' {prompt} ' not in _sent:\n",
    "            continue\n",
    "        \n",
    "        _p_idx = _sent.index(f' {prompt} ') + 1\n",
    "        _e_indices = [_sent.find(e) for e in _entities]\n",
    "        if max(_e_indices) < 0:\n",
    "            continue\n",
    "        _l_indices = [(i, e) for i, e in zip(_e_indices, _entities) if 0 <= i < _p_idx]\n",
    "        _r_indices = [(i, e) for i, e in zip(_e_indices, _entities) if i > _p_idx]\n",
    "        if len(_l_indices) == 0 or len(_r_indices) == 0:\n",
    "            continue\n",
    "        _l_idx, _l_ent = max(_l_indices, key=lambda p: p[0])\n",
    "        _r_idx, _r_ent = min(_r_indices, key=lambda p: p[0])\n",
    "#         _r_idx += len(_r_ent)\n",
    "\n",
    "        ## Extracting dep path \n",
    "        doc = _nlp(_sent)\n",
    "        src_matcher = Matcher(nlp.vocab)\n",
    "        src_pattern = [{\"LOWER\": t} for t in _l_ent.split(' ')]\n",
    "        src_matcher.add(\"src\", [src_pattern])\n",
    "        \n",
    "        prompt_matcher = Matcher(nlp.vocab)\n",
    "        prompt_pattern = [{\"LOWER\": t} for t in prompt.split(' ')]\n",
    "        prompt_matcher.add(\"prompt\", [prompt_pattern])\n",
    "\n",
    "        tgt_matcher = Matcher(nlp.vocab)\n",
    "        tgt_pattern = [{\"LOWER\": t} for t in _r_ent.split(' ')]\n",
    "        tgt_matcher.add(\"tgt\", [tgt_pattern])\n",
    "        \n",
    "        src_matches = src_matcher(doc)\n",
    "        if len(src_matches) == 0:\n",
    "            print('src not matched')\n",
    "            continue\n",
    "        tgt_matches = tgt_matcher(doc)\n",
    "        if len(tgt_matches) == 0:\n",
    "            print('tgt not matched')\n",
    "            continue\n",
    "        prompt_matches = prompt_matcher(doc)\n",
    "        if len(prompt_matches) == 0:\n",
    "            print('prompt not matched')\n",
    "            continue\n",
    "        \n",
    "        src_match = None\n",
    "        src_span = None\n",
    "        for _m in src_matches:\n",
    "            if doc[_m[1]].idx == _l_idx:\n",
    "                src_match = _m[0]\n",
    "                src_span = doc[_m[1]: _m[2]]\n",
    "                break\n",
    "        \n",
    "        prompt_match = None\n",
    "        prompt_span = None\n",
    "        for _m in prompt_matches:\n",
    "            if doc[_m[1]].idx == _p_idx:\n",
    "                prompt_match = _m[0]\n",
    "                prompt_span = doc[_m[1]: _m[2]]\n",
    "                break\n",
    "        \n",
    "        tgt_match = None\n",
    "        tgt_span = None\n",
    "        for _m in tgt_matches:\n",
    "            if doc[_m[1]].idx == _r_idx:\n",
    "                tgt_match = _m[0]\n",
    "                tgt_span = doc[_m[1]: _m[2]]\n",
    "                break\n",
    "        \n",
    "        if None in (src_match, prompt_match, tgt_match):\n",
    "            print('_sent:', _sent)\n",
    "            print(_l_ent, prompt, _r_ent)\n",
    "            print(src_match, prompt_match, tgt_match)\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        if len(spacy.util.filter_spans([src_span, prompt_span, tgt_span])) != 3: # distinct_spans\n",
    "            print('overlapping spans')\n",
    "            continue\n",
    "        \n",
    "        src_root = src_span.root\n",
    "        prompt_root = prompt_span.root\n",
    "        tgt_root = tgt_span.root\n",
    "        \n",
    "        edges = []\n",
    "        for token in doc:\n",
    "            for child in token.children:\n",
    "                edges.append(('{}-{}'.format(token.lower_,token.i), '{}-{}'.format(child.lower_,child.i)))\n",
    "#         print(edges)\n",
    "        \n",
    "        graph = nx.Graph(edges)\n",
    "        path = None\n",
    "        source = '{}-{}'.format(src_root.lower_, src_root.i)\n",
    "        middle = '{}-{}'.format(prompt_root.lower_, prompt_root.i)\n",
    "        target = '{}-{}'.format(tgt_root.lower_, tgt_root.i)\n",
    "        if any([n not in graph for n in (source, middle, target)]):\n",
    "            continue\n",
    "        if nx.has_path(graph, source=source, target=middle) and nx.has_path(graph, source=middle, target=target):\n",
    "            path1 = nx.shortest_path(graph, source=source, target=middle)\n",
    "            path2 = nx.shortest_path(graph, source=middle, target=target)\n",
    "            path = path1 + path2[1:]\n",
    "        \n",
    "        if path is not None:\n",
    "            for t in src_span:\n",
    "                n = '{}-{}'.format(t.lower_, t.i)  \n",
    "                if n not in path:\n",
    "                    path.append(n)\n",
    "            for t in prompt_span:\n",
    "                n = '{}-{}'.format(t.lower_, t.i)\n",
    "                if n not in path:\n",
    "                    path.append(n)\n",
    "            for t in tgt_span:\n",
    "                n = '{}-{}'.format(t.lower_, t.i)\n",
    "                if n not in path:\n",
    "                    path.append(n)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        path_nodes = dict()\n",
    "        for p in path:\n",
    "            t, i = p.rsplit('-', 1)\n",
    "            i = int(i)\n",
    "            path_nodes[i] = t\n",
    "        path_nodes = sorted(path_nodes.items(), key=lambda x: x[0])\n",
    "        pattern = ' '.join([p[1] for p in path_nodes])\n",
    "\n",
    "        print('_sent:', _sent)\n",
    "        print(path)\n",
    "        print(pattern, (_l_ent, _r_ent))\n",
    "        print()\n",
    "        ret.append(pattern)\n",
    "        if len(ret) >= 100:\n",
    "            break\n",
    "        \n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: this company has a great lp !\n",
      "['company-1', 'has-2', 'lp-5']\n",
      "company has lp ('company', 'lp')\n",
      "\n",
      "_sent: at&t has many ways to receive higher education .\n",
      "['at&t-0', 'has-1', 'ways-3', 'receive-5', 'education-7', 'higher-6']\n",
      "at&t has ways receive higher education ('at&t', 'higher education')\n",
      "\n",
      "_sent: will at&t hire someone who has a pending felony charge but has not been convicted of it from over three years ago ?\n",
      "['at&t-1', 'hire-2', 'someone-3', 'has-5', 'charge-9', 'pending-7', 'felony-8']\n",
      "at&t hire someone has pending felony charge ('at&t', 'pending felony')\n",
      "\n",
      "_sent: yes , at&t has a generous telecommuting policy .\n",
      "['at&t-2', 'has-3', 'policy-7', 'telecommuting-6']\n",
      "at&t has telecommuting policy ('at&t', 'telecommuting')\n",
      "\n",
      "_sent: management has their own issues because upper management keeps pressure on lower management for unattainable results .\n",
      "['management-0', 'has-1', 'keeps-8', 'management-7', 'upper-6']\n",
      "management has upper management keeps ('management', 'upper')\n",
      "\n",
      "_sent: at&t has all company policy and procedures .\n",
      "['at&t-0', 'has-1', 'policy-4', 'company-3']\n",
      "at&t has company policy ('at&t', 'company')\n",
      "\n",
      "_sent: like all call centers , at&t has a strict attendance policy .\n",
      "['at&t-5', 'has-6', 'policy-10', 'attendance-9']\n",
      "at&t has attendance policy ('at&t', 'attendance policy')\n",
      "\n",
      "_sent: i started right out of high school and had been my only career , but the last three to four years has been the ultimate worst side of our company i have ever seen .\n",
      "['school-6', 'of-4', 'out-3', 'started-1', 'been-22', 'has-21', 'been-22', 'side-26', 'of-27', 'company-29']\n",
      "started out of school has been side of company ('school', 'company')\n",
      "\n",
      "_sent: management has been made to micromanage on the hour , every hour to monitor results , and so much focus on call flow , makes the customers feel like we are n't listening to them when we follow it .\n",
      "['management-0', 'made-3', 'has-1', 'made-3', 'micromanage-5', 'hour-11', 'monitor-13', 'results-14']\n",
      "management has made micromanage hour monitor results ('management', 'results')\n",
      "\n",
      "_sent:   company has put an emphasis in contracting and outsourcing to overseas .\n",
      "['company-1', 'put-3', 'has-2', 'put-3', 'in-6', 'contracting-7', 'outsourcing-9']\n",
      "company has put in contracting outsourcing ('company', 'outsourcing')\n",
      "\n",
      "_sent: my personal experience , yes , at&t , has the right and they exercise that right , to randomly drug test you , any given time of any day , when you arrive at work or during that day of work you can be notified to go get tested , you do so ..... asap\n",
      "['at&t-6', 'experience-2', 'has-8', 'exercise-13', 'randomly-18', 'drug-19', 'test-20']\n",
      "experience at&t has exercise randomly drug test ('at&t', 'randomly drug test')\n",
      "\n",
      "_sent: i would imagine that at&t , like many companies that operate call centers , probably has positions that allow for telecommuting , though i did not work in such a capacity for them .\n",
      "['centers-12', 'operate-10', 'companies-8', 'like-6', 'has-15', 'positions-16', 'allow-18', 'for-19', 'telecommuting-20', 'call-11']\n",
      "like companies operate call centers has positions allow for telecommuting ('call centers', 'telecommuting')\n",
      "\n",
      "_sent: at&t has a very strict code of business conduct and have a zero tolerance attitude towards violating any of the policies .\n",
      "['at&t-0', 'has-1', 'code-5', 'of-6', 'conduct-8', 'business-7']\n",
      "at&t has code of business conduct ('at&t', 'business')\n",
      "\n",
      "_sent: at&t as far as i know has n't ever done a credit check on me or any other employee .\n",
      "['at&t-0', 'far-2', 'done-9', 'has-6', 'done-9', 'check-12', 'credit-11']\n",
      "at&t far has done credit check ('at&t', 'credit check')\n",
      "\n",
      "_sent: no , at&t has a no marijuana policy .\n",
      "['at&t-2', 'has-3', 'policy-7', 'marijuana-6']\n",
      "at&t has marijuana policy ('at&t', 'marijuana')\n",
      "\n",
      "_sent:   the company has very good benefits and will reward you for hard work .\n",
      "['company-2', 'has-3', 'benefits-6']\n",
      "company has benefits ('company', 'benefits')\n",
      "\n",
      "_sent: at&t has slow internal decision processes , have patience .\n",
      "['at&t-0', 'has-1', 'have-7', 'patience-8']\n",
      "at&t has have patience ('at&t', 'patience')\n",
      "\n",
      "_sent: skechers has non slip , they are the most comfortable , durable , work shoes i 've ever bought .\n",
      "['skechers-0', 'has-1', 'slip-3', 'non-2']\n",
      "skechers has non slip ('skechers', 'non slip')\n",
      "\n",
      "_sent: oh we had a guy that fell his background check for  \n",
      "  impostering police and firemen brought it to our manager 's attention it 's on the \n",
      " internet that he 's a liar and a scammer scammer and has done time in prison nothing got done so it really does n't matter about the background check so yeah you got hired congratulations\n",
      "['internet-28', 'on-25', \"'s-24\", \"'s-31\", 'done-40', 'has-39', 'done-40', 'in-42', 'prison-43']\n",
      "'s on internet 's has done in prison ('internet', 'prison')\n",
      "\n",
      "_sent: can go through cb to get oxfords for roughly $ 10 a piece ) \n",
      "\n",
      " apron ( depending upon position can have decorative pins ) \n",
      "\n",
      " pants must reach the ankle and be of either cotton or polyester material in navy blue , black , or khaki ( if it has belt loops a belt has to be worn -- i think brown leather ) .\n",
      "['khaki-46', 'black-43', 'blue-41', 'in-39', 'cotton-35', 'of-33', 'be-32', 'reach-28', 'has-55', 'has-50', 'loops-52', 'belt-51']\n",
      "reach be of cotton in blue black khaki has belt loops has ('khaki', 'belt loops')\n",
      "\n",
      "_sent: cracker barrel has a website where you can order uniforms\n",
      "['barrel-1', 'has-2', 'website-4', 'cracker-0']\n",
      "cracker barrel has website ('cracker barrel', 'website')\n",
      "\n",
      "_sent: their daily volume of guests and the need to step into the age of technology has caused them to update their hiring process .\n",
      "['technology-14', 'of-13', 'age-12', 'into-10', 'step-9', 'need-7', 'volume-2', 'caused-16', 'has-15', 'caused-16', 'update-19', 'process-22', 'hiring-21']\n",
      "volume need step into age of technology has caused update hiring process ('technology', 'hiring process')\n",
      "\n",
      "_sent: raises are given out frequently but it should be known retail has probably the lowest pay scale in the store and they keep adding to the responsibilities .\n",
      "['retail-10', 'has-11', 'scale-16', 'pay-15']\n",
      "retail has pay scale ('retail', 'pay scale')\n",
      "\n",
      "_sent: i believe that cracker barrel should pay for drug tests , especially if the test comes back negative and if that person has no history of drug use .\n",
      "test has history\n",
      "None 10540441779542591554 14930118741968279164\n",
      "\n",
      "_sent: nail polish / acrylic is allowed only if you do not handle food products and it has to be a color that matches your skin tone\n",
      "['products-13', 'handle-11', 'allowed-5', 'has-16', 'be-18', 'color-20', 'matches-22', 'tone-25', 'skin-24']\n",
      "allowed handle products has be color matches skin tone ('products', 'skin')\n",
      "\n",
      "_sent: yes the company has lay offs\n",
      "['company-2', 'lay-4', 'has-3', 'lay-4', 'offs-5']\n",
      "company has lay offs ('company', 'lay offs')\n",
      "\n",
      "_sent: tattoos has to be covered because it is a corporate setting .\n",
      "['tattoos-0', 'has-1', 'covered-4']\n",
      "tattoos has covered ('tattoos', 'covered')\n",
      "\n",
      "_sent: my hiring process was atypical , however , generally once a \" panel \" interview has been completed , the hiring manager makes a decision and either the manager , or possibly the administrative assistant , will make an offer .\n",
      "['process-2', 'was-3', 'completed-17', 'has-15', 'completed-17', 'was-3', 'makes-22', 'manager-21', 'hiring-1', 'hiring-20']\n",
      "hiring process was has completed hiring manager makes ('hiring process', 'hiring manager')\n",
      "\n",
      "_sent: wells fargo has their mortgage officers separately that periodically visit the banks when personal bankers give them leads .\n",
      "['fargo-1', 'has-2', 'officers-5', 'mortgage-4']\n",
      "fargo has mortgage officers ('fargo', 'mortgage')\n",
      "\n",
      "_sent:   wells fargo has a very thorough background check procedure .\n",
      "['fargo-2', 'has-3', 'procedure-9', 'check-8', 'background-7']\n",
      "fargo has background check procedure ('fargo', 'background check')\n",
      "\n",
      "_sent: wells fargo has an on - site cafeteria at most locations .\n",
      "['fargo-1', 'has-2', 'cafeteria-7']\n",
      "fargo has cafeteria ('fargo', 'cafeteria')\n",
      "\n",
      "_sent: yes , retail has trained you in customer service , they will continue your trading to ensure you understand why you are requesting additional documents .\n",
      "['retail-2', 'trained-4', 'has-3', 'trained-4', 'in-6', 'service-8', 'customer-7']\n",
      "retail has trained in customer service ('retail', 'customer service')\n",
      "\n",
      "_sent: i am not sure my job description is not a chasier or has nothing to do with that area\n",
      "['description-6', 'is-7', 'has-12', 'nothing-13', 'do-15', 'with-16', 'area-18', 'job-5']\n",
      "job description is has nothing do with area ('job description', 'area')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: this starting salary is the same as the guy i work with who 's been there for 5 years that has only gotten a dollar raise since he started .\n",
      "['salary-2', 'is-3', 'same-5', 'as-6', 'guy-8', 'work-10', 'with-11', 'been-14', 'for-16', 'years-18', 'gotten-22', 'has-20', 'gotten-22', 'raise-25', 'starting-1', 'dollar-24']\n",
      "starting salary is same as guy work with been for years has gotten dollar raise ('starting salary', 'dollar raise')\n",
      "\n",
      "_sent: yes ,   the company has a mixture of gender and races .\n",
      "['company-4', 'has-5', 'mixture-7', 'of-8', 'gender-9']\n",
      "company has mixture of gender ('company', 'gender')\n",
      "\n",
      "_sent: home depot has there own shipping and receiving dept .\n",
      "['depot-1', 'has-2', 'own-4', 'shipping-5', 'home-0', 'and-6', 'receiving-7']\n",
      "home depot has own shipping and receiving ('home depot', 'shipping and receiving')\n",
      "\n",
      "_sent:   home depot has good training methods , i 've learned a lot , more than i 've ever known & it feels good to be knowledgeable about retail .\n",
      "['depot-2', 'has-3', 'methods-6', 'training-5', 'home-1']\n",
      "home depot has training methods ('home depot', 'training')\n",
      "\n",
      "_sent: home depot has a lack of management .\n",
      "['depot-1', 'has-2', 'lack-4', 'of-5', 'management-6', 'home-0']\n",
      "home depot has lack of management ('home depot', 'management')\n",
      "\n",
      "_sent: management has clearly shown favoritism , and exclusion of employees they do n’t like from company gifts and promotions .\n",
      "['management-0', 'shown-3', 'has-1', 'shown-3', 'exclusion-7', 'do-11', 'like-13', 'from-14', 'gifts-16', 'company-15']\n",
      "management has shown exclusion do like from company gifts ('management', 'company')\n",
      "\n",
      "_sent: fyi - all of the entries below depict a fair assessment of how this conglomerate company has achieved it 's market share .\n",
      "['company-15', 'achieved-17', 'has-16', 'achieved-17', 'of-11', 'assessment-10', 'depict-7', \"'s-19\", 'share-21', 'market-20']\n",
      "depict assessment of company has achieved 's market share ('company', 'market share')\n",
      "\n",
      "_sent: home depot has a team in place for asset protection .\n",
      "['depot-1', 'has-2', 'team-4', 'in-5', 'place-6', 'for-7', 'protection-9', 'home-0', 'asset-8']\n",
      "home depot has team in place for asset protection ('home depot', 'asset protection')\n",
      "\n",
      "_sent: work was without any on the job training whatsoever and the employee has to ask questions in order to perform their duties !\n",
      "['training-7', 'on-4', 'was-1', 'has-12', 'ask-14', 'in-16', 'order-17']\n",
      "was on training has ask in order ('training', 'order')\n",
      "\n",
      "_sent: my home depot experience has been in rock hill , sc and i do not know how many years back this company goes on a background check .\n",
      "['depot-2', 'experience-3', 'been-5', 'has-4', 'been-5', 'in-6', 'hill-8', 'rock-7', 'home-1']\n",
      "home depot experience has been in rock hill ('home depot', 'rock')\n",
      "\n",
      "_sent: the company has many new types of flooring technology and hardware equipment that make our lives easier .\n",
      "['company-1', 'has-2', 'types-5', 'of-6', 'equipment-11', 'technology-8']\n",
      "company has types of technology equipment ('company', 'technology')\n",
      "\n",
      "_sent: you can wear a hat as long as it has a home depot logo on it .\n",
      "['long-6', 'has-9', 'logo-13', 'depot-12', 'home-11']\n",
      "long has home depot logo ('long', 'home depot')\n",
      "\n",
      "_sent: not sure if home depot has 3rd hour night shifts .\n",
      "['depot-4', 'has-5', 'shifts-9', 'home-3', 'night-8']\n",
      "home depot has night shifts ('home depot', 'night shifts')\n",
      "\n",
      "_sent: no my pay rate has not changed from new york to north carolina\n",
      "['rate-3', 'changed-6', 'has-4', 'changed-6', 'from-7', 'york-9', 'pay-2', 'new-8']\n",
      "pay rate has changed from new york ('pay rate', 'new york')\n",
      "\n",
      "_sent: so if you live in a state that has legal weed , they will still turn you down for weed usage- despite having medical use cards and all the such .\n",
      "['state-6', 'has-8', 'weed-10']\n",
      "state has weed ('state', 'weed')\n",
      "\n",
      "_sent: thd has the right to drug test at any time .\n",
      "['thd-0', 'has-1', 'right-3', 'to-4', 'test-6', 'drug-5']\n",
      "thd has right to drug test ('thd', 'drug')\n",
      "\n",
      "_sent: do n't know about washington but california home depot does run a drug test as a part of the application process and the company is & has a no tolerance for drugs & alcohol .\n",
      "['company-23', 'is-24', 'has-26', 'tolerance-29', 'for-30', 'drugs-31']\n",
      "company is has tolerance for drugs ('company', 'drugs')\n",
      "\n",
      "_sent: no the company has a dress code .\n",
      "['company-2', 'has-3', 'code-6', 'dress-5']\n",
      "company has dress code ('company', 'dress code')\n",
      "\n",
      "_sent: yes home depot has a company wide drug testing policy .\n",
      "['depot-2', 'has-3', 'policy-9', 'company-5', 'home-1']\n",
      "home depot has company policy ('home depot', 'company')\n",
      "\n",
      "_sent: the home depot has a policy to test for drugs upon injury .\n",
      "['depot-2', 'has-3', 'policy-5', 'test-7', 'home-1']\n",
      "home depot has policy test ('home depot', 'test')\n",
      "\n",
      "_sent: yes , i 've had purple hair before , two of my coworkers has had pink hair , and a former coworker had blue , purple , and red hair .\n",
      "['hair-7', 'had-5', 'had-15', 'has-14', 'had-15', 'hair-17', 'pink-16']\n",
      "had hair has had pink hair ('hair', 'pink')\n",
      "\n",
      "_sent: home depot has their own loss prevention which works for them\n",
      "['depot-1', 'has-2', 'prevention-6', 'home-0', 'loss-5']\n",
      "home depot has loss prevention ('home depot', 'loss prevention')\n",
      "\n",
      "_sent: the company is huge and has locations all over the united states .\n",
      "['company-1', 'is-2', 'has-5', 'locations-6', 'over-8', 'states-11', 'united-10']\n",
      "company is has locations over united states ('company', 'united states')\n",
      "\n",
      "_sent: no , the home depot has a place for anyone no matter the age .\n",
      "['depot-4', 'has-5', 'matter-11', 'home-3']\n",
      "home depot has matter ('home depot', 'matter')\n",
      "\n",
      "_sent: \n",
      " full time has better coverage with aetna but not very good deductibles .\n",
      "['time-2', 'has-3', 'coverage-5', 'with-6', 'aetna-7', 'full-1']\n",
      "full time has coverage with aetna ('full time', 'aetna')\n",
      "\n",
      "_sent: yes the home depot has healthcare\n",
      "['depot-3', 'has-4', 'healthcare-5', 'home-2']\n",
      "home depot has healthcare ('home depot', 'healthcare')\n",
      "\n",
      "_sent: medical coverage has different tiers , so you can pick one that works within your budget , but is really excellent insurance .\n",
      "['medical-0', 'coverage-1', 'has-2', 'pick-9', 'is-18', 'insurance-21']\n",
      "medical coverage has pick is insurance ('medical', 'insurance')\n",
      "\n",
      "_sent: geico has employees ranging from all ages and walks of life .\n",
      "['geico-0', 'has-1', 'employees-2', 'ranging-3', 'from-4', 'ages-6', 'walks-8']\n",
      "geico has employees ranging from ages walks ('geico', 'walks')\n",
      "\n",
      "_sent: they make you jump through an endless cycle of hoops like almost thousands , you would think that you were interviewing for ceo of a company , not a location that has 10 or 11 pages of nothing but one - star reviews on yelp for their claims being wrongfully denied ( poway , ca ) .\n",
      "['company-25', 'of-23', 'ceo-22', 'location-29', 'has-31', 'pages-35', 'of-36', 'nothing-37', 'but-38', 'reviews-42', 'star-41']\n",
      "ceo of company location has pages of nothing but star reviews ('company', 'star')\n",
      "\n",
      "_sent: they will find a reason to fire someone with a disability because they do n't want to keep someone with the company that has special needs or has to take time off for doctors appointments\n",
      "['company-21', 'has-23', 'needs-25', 'special-24']\n",
      "company has special needs ('company', 'special needs')\n",
      "\n",
      "_sent:   any training that i have had done has been within geico .\n",
      "['training-2', 'been-9', 'has-8', 'been-9', 'within-10', 'geico-11']\n",
      "training has been within geico ('training', 'geico')\n",
      "\n",
      "_sent:   they spend a great deal of time , money and effort on training for the employees to ensure that everyone has the correct information and is comfortable with what they have learned before they need to \" go out into the real world \" to use it .\n",
      "['training-13', 'ensure-18', 'has-21', 'information-24']\n",
      "training ensure has information ('training', 'information')\n",
      "\n",
      "_sent: the company has since changed the hours for certain positions due to different needs in different locations , but sales and customer service are day hours .\n",
      "['company-1', 'changed-4', 'has-2', 'changed-4', 'are-23', 'sales-19', 'service-22', 'customer-21']\n",
      "company has changed sales customer service are ('company', 'customer service')\n",
      "\n",
      "_sent: so i was punished because i was given incorrect information by someone that has no business in that position .\n",
      "['information-9', 'by-10', 'someone-11', 'has-13', 'business-15']\n",
      "information by someone has business ('information', 'business')\n",
      "\n",
      "_sent: i work at a fulfilment center and while each loaction has different ways of operations , everything comes down to are you willing to stand on your feet for extended period of time ?\n",
      "['center-5', 'at-2', 'work-1', 'comes-17', 'has-10', 'comes-17', 'are-20', 'willing-22', 'stand-24', 'for-28', 'period-30']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work at center has comes are willing stand for period ('center', 'period')\n",
      "\n",
      "_sent: amazon has a half million employees worldwide , nobody cares about your work / life balance .\n",
      "['amazon-0', 'has-1', 'cares-9', 'about-10', 'balance-15', 'work-12', '/-13', 'life-14']\n",
      "amazon has cares about work / life balance ('amazon', 'work / life balance')\n",
      "\n",
      "_sent: well as an amazon employee ( waiting for my start date ) and as an amazon prime member and a human who has this very issue of an off balanced work / life balance i just want to focus on learning from my past mistakes and fixing them .\n",
      "['human-20', 'has-22', 'issue-25', 'of-26', 'balance-33', 'work-30', '/-31', 'life-32']\n",
      "human has issue of work / life balance ('human', 'work / life balance')\n",
      "\n",
      "_sent: amazon has a bunch of unqualified am 's and pa 's who do n't know nothing .\n",
      "['amazon-0', 'has-1', 'bunch-3', 'of-4', 'am-6', 'pa-9']\n",
      "amazon has bunch of am pa ('amazon', 'pa')\n",
      "\n",
      "_sent: it ’s disgusting that jeff bezos has become the richest man off the literal blood and sweat of his slaves .\n",
      "['bezos-5', 'become-7', 'has-6', 'become-7', 'man-10', 'off-11', 'blood-14', 'jeff-4']\n",
      "jeff bezos has become man off blood ('jeff bezos', 'blood')\n",
      "\n",
      "_sent: have read at least fifty job requirements and amazon has stated that they are a eeo employer and they use all standards except for disability and age related people .\n",
      "['amazon-8', 'requirements-6', 'stated-10', 'has-9', 'stated-10', 'are-13', 'use-19', 'except-22', 'for-23', 'disability-24']\n",
      "requirements amazon has stated are use except for disability ('amazon', 'disability')\n",
      "\n",
      "_sent:   management has its clicks and base all information on gossip .\n",
      "['management-1', 'has-2', 'base-6', 'information-8']\n",
      "management has base information ('management', 'information')\n",
      "\n",
      "_sent: \n",
      "\n",
      " amazon really has no bias for promoting the company is focused on hiring but never on promotion .\n",
      "['amazon-1', 'has-3', 'bias-5', 'for-6', 'promoting-7', 'company-9']\n",
      "amazon has bias for promoting company ('amazon', 'company')\n",
      "\n",
      "_sent: this company has reached a point where it simply needs warm bodies .\n",
      "['company-1', 'reached-3', 'has-2', 'reached-3', 'point-5', 'needs-9', 'bodies-11', 'warm-10']\n",
      "company has reached point needs warm bodies ('company', 'warm bodies')\n",
      "\n",
      "_sent: it is easier to get a job in a bank , than to be able to finish an applicatiuon on line , nochance to choose shift hours , lack of website information , people from other countries answering to the online chat , amazon has become a s ... hole to even apply to , it might be because they announced $ 15 an hour , and now they are all trying to minimize new hires ? ? ?\n",
      "['amazon-43', 'become-45', 'has-44', 'become-45', 'be-57', 'announced-60', 'trying-71', 'minimize-73', 'hires-75', 'new-74']\n",
      "amazon has become be announced trying minimize new hires ('amazon', 'new hires')\n",
      "\n",
      "_sent: my college education has spread out over the years with moving from state to state , employment , relocations and   family .\n",
      "['education-2', 'spread-4', 'has-3', 'spread-4', 'out-5']\n",
      "education has spread out ('education', 'spread out')\n",
      "\n",
      "_sent: amazon prime now has one of the worst hiring practices that i 've ever experienced .\n",
      "['prime-1', 'has-3', 'one-4', 'of-5', 'practices-9', 'now-2', 'hiring-8']\n",
      "prime now has one of hiring practices ('prime now', 'hiring practices')\n",
      "\n",
      "_sent: 6am-6pm.the company has many different shifts esp during seasonal /holidays .\n",
      "['company-1', 'has-2', 'during-7', 'seasonal-8']\n",
      "company has during seasonal ('company', 'seasonal')\n",
      "\n",
      "_sent: dallas has one inside the city but you will not be able to work at the one i am reviewing .\n",
      "['dallas-0', 'has-1', 'one-2', 'inside-3', 'city-5']\n",
      "dallas has one inside city ('dallas', 'city')\n",
      "\n",
      "_sent: amazon has a lot of great benefits to reward their associates because they know how hard they will work .\n",
      "['amazon-0', 'has-1', 'lot-3', 'of-4', 'benefits-6', 'great-5']\n",
      "amazon has lot of great benefits ('amazon', 'great benefits')\n",
      "\n",
      "_sent: amazon has never requested a urine drug test\n",
      "['amazon-0', 'requested-3', 'has-1', 'requested-3', 'test-7', 'urine-5']\n",
      "amazon has requested urine test ('amazon', 'urine')\n",
      "\n",
      "_sent: every corporation has it problems but , with opening communication and respect for others goes a long way .. i believe in right person for the right sit .\n",
      "['corporation-1', 'has-2', 'goes-14', 'with-7', 'opening-8', 'communication-9']\n",
      "corporation has with opening communication goes ('corporation', 'communication')\n",
      "\n",
      "_sent: they terminated me , and the letter said because i executed a confidentiality and invention assignment agreement , i do not see how that has anything to do with being absent and sick with a doctor 's note ?\n",
      "['invention-14', 'confidentiality-12', 'assignment-15', 'agreement-16', 'executed-10', 'see-21', 'has-24', 'anything-25', 'do-27', 'with-28', 'being-29', 'absent-30', 'sick-32', 'with-33', 'note-37', 'doctor-35', \"'s-36\"]\n",
      "executed confidentiality invention assignment agreement see has anything do with being absent sick with doctor 's note ('invention', \"doctor 's note\")\n",
      "\n",
      "_sent: i work at a brand new sort facility smf5 , my morale has dropped due to earning so little money in the last month .\n",
      "['morale-11', 'dropped-13', 'has-12', 'dropped-13', 'earning-16', 'money-19']\n",
      "morale has dropped earning money ('morale', 'money')\n",
      "\n",
      "_sent: i live in a suburb of cleveland and wondering if amazon has work from home positions .\n",
      "['amazon-10', 'has-11', 'work-12', 'from-13', 'positions-15', 'home-14']\n",
      "amazon has work from home positions ('amazon', 'home positions')\n",
      "\n",
      "_sent:   if an individual was given an invitation to apply for ft position and they have not met their quota by the time amazon fulfills their ft and pt company needs and the individual still has n't   been contacted by hr or a manager , may not have been selected to continue employment and should have received an email or letter in the mail .\n",
      "['company-29', 'needs-30', 'fulfills-24', 'time-22', 'by-20', 'met-17', 'contacted-39', 'has-35', 'contacted-39', 'by-40', 'hr-41', 'manager-44']\n",
      "met by time fulfills company needs has contacted by hr manager ('company', 'manager')\n",
      "\n",
      "_sent: yes , amazon has hired felons .\n",
      "['amazon-2', 'hired-4', 'has-3', 'hired-4', 'felons-5']\n",
      "amazon has hired felons ('amazon', 'felons')\n",
      "\n",
      "_sent: the company has gotten bad press about these measures as many people would like to see a union perhaps with this never happening more than likely because of the effect on wages and safety being paramount as with product .\n",
      "['company-1', 'gotten-3', 'has-2', 'gotten-3', 'like-13', 'see-15', 'union-17']\n",
      "company has gotten like see union ('company', 'union')\n",
      "\n",
      "_sent: for these reasons management has to step up as though people are important just as the bottom line is and , the catharsis lay in the middle without a union which are for far more dangerous professions e.g. aircraft assembly\n",
      "['management-3', 'has-4', 'step-6', 'is-18', 'line-17', 'bottom-16']\n",
      "management has step bottom line is ('management', 'bottom line')\n",
      "\n",
      "_sent: management has to talk with them and also collect all the reasons enclind with the matter and solve with accepting all losical matter .\n",
      "['management-0', 'has-1', 'talk-3']\n",
      "management has talk ('management', 'talk')\n",
      "\n",
      "_sent: never had a problem yet they have been great management has been nice and very helpful ... i am definitely happy with my job ...\n",
      "['management-9', 'been-7', 'had-1', 'been-11', 'has-10', 'been-11', 'nice-12']\n",
      "had been management has been nice ('management', 'nice')\n",
      "\n",
      "_sent: treat people the way the law has order it .\n",
      "['law-5', 'has-6', 'way-3', 'order-7']\n",
      "way law has order ('law', 'order')\n",
      "\n",
      "_sent: for few ( not couple of them ) employees watch video clips on you - tube whole day , \" pa \" enjoying free drinks at bar after work hours by certain employees so they do n't have to work as hard as other employees who has to and are forced to work till last minute while others can stop processing atleast 30 minutes shift ends .\n",
      "['free-23', 'drinks-24', 'enjoying-22', 'have-37', 'work-39', 'hard-41', 'as-42', 'employees-44', 'has-46', 'forced-50', 'work-52', 'till-53']\n",
      "enjoying free drinks have work hard as employees has forced work till ('free', 'till')\n",
      "\n",
      "_sent: as you know , shannon , the president or ceo of this company is a brutal guy and at least for now is keeping his mouth closed in the role he has with trump .\n",
      "['mouth-25', 'keeping-23', 'closed-26', 'in-27', 'role-29', 'has-31', 'role-29', 'in-27', 'closed-26', 'with-32', 'trump-33']\n",
      "keeping mouth closed in role has with trump ('mouth', 'trump')\n",
      "\n",
      "_sent: make sure the products each one receives is awesome and has good reviews and working order before shipping to anybody makes a lasting customer relationship\n",
      "['products-3', 'is-7', 'has-10', 'reviews-12', 'order-15']\n",
      "products is has reviews order ('products', 'order')\n",
      "\n",
      "_sent: i was shocked to be terminated suddenly by an email stating that “ as everyone has heard that the packages were coming in later ” and “ we did communicate during our all - hands meeting that we needed volunteers from our associates to switch their schedules to nit sorts ” .\n",
      "['email-9', 'stating-10', 'that-11', '“-12', 'heard-16', 'has-15', 'heard-16', 'communicate-29', 'during-30', 'meeting-35', 'needed-38', 'switch-44', 'to-47', 'sorts-49', 'nit-48']\n",
      "email stating that “ has heard communicate during meeting needed switch to nit sorts ('email', 'nit')\n",
      "\n",
      "_sent: i live about 40 minutes away and my route has a lot of construction which can cause me to have to leave the house an hour to an hour and a half early in the morning to make sure i get to work on time .\n",
      "['live-1', 'has-9', 'lot-11', 'of-12', 'construction-13']\n",
      "live has lot of construction ('live', 'construction')\n",
      "\n",
      "_sent: amazon provides a livable wage and has been at the forefront of global innovation in retail .\n",
      "['wage-4', 'provides-1', 'been-7', 'has-6', 'been-7', 'at-8', 'forefront-10', 'of-11', 'innovation-13', 'livable-3']\n",
      "provides livable wage has been at forefront of innovation ('livable wage', 'innovation')\n",
      "\n",
      "_sent: \n",
      "\n",
      " it is so disheartening that such a great company has such a poor organization .\n",
      "['company-9', 'has-10', 'organization-14']\n",
      "company has organization ('company', 'organization')\n",
      "\n",
      "company has lp\n",
      "at&t has ways receive higher education\n",
      "at&t hire someone has pending felony charge\n",
      "at&t has telecommuting policy\n",
      "management has upper management keeps\n",
      "at&t has company policy\n",
      "at&t has attendance policy\n",
      "started out of school has been side of company\n",
      "management has made micromanage hour monitor results\n",
      "company has put in contracting outsourcing\n",
      "experience at&t has exercise randomly drug test\n",
      "like companies operate call centers has positions allow for telecommuting\n",
      "at&t has code of business conduct\n",
      "at&t far has done credit check\n",
      "at&t has marijuana policy\n",
      "company has benefits\n",
      "at&t has have patience\n",
      "skechers has non slip\n",
      "'s on internet 's has done in prison\n",
      "reach be of cotton in blue black khaki has belt loops has\n",
      "cracker barrel has website\n",
      "volume need step into age of technology has caused update hiring process\n",
      "retail has pay scale\n",
      "allowed handle products has be color matches skin tone\n",
      "company has lay offs\n",
      "tattoos has covered\n",
      "hiring process was has completed hiring manager makes\n",
      "fargo has mortgage officers\n",
      "fargo has background check procedure\n",
      "fargo has cafeteria\n",
      "retail has trained in customer service\n",
      "job description is has nothing do with area\n",
      "starting salary is same as guy work with been for years has gotten dollar raise\n",
      "company has mixture of gender\n",
      "home depot has own shipping and receiving\n",
      "home depot has training methods\n",
      "home depot has lack of management\n",
      "management has shown exclusion do like from company gifts\n",
      "depict assessment of company has achieved 's market share\n",
      "home depot has team in place for asset protection\n",
      "was on training has ask in order\n",
      "home depot experience has been in rock hill\n",
      "company has types of technology equipment\n",
      "long has home depot logo\n",
      "home depot has night shifts\n",
      "pay rate has changed from new york\n",
      "state has weed\n",
      "thd has right to drug test\n",
      "company is has tolerance for drugs\n",
      "company has dress code\n",
      "home depot has company policy\n",
      "home depot has policy test\n",
      "had hair has had pink hair\n",
      "home depot has loss prevention\n",
      "company is has locations over united states\n",
      "home depot has matter\n",
      "full time has coverage with aetna\n",
      "home depot has healthcare\n",
      "medical coverage has pick is insurance\n",
      "geico has employees ranging from ages walks\n",
      "ceo of company location has pages of nothing but star reviews\n",
      "company has special needs\n",
      "training has been within geico\n",
      "training ensure has information\n",
      "company has changed sales customer service are\n",
      "information by someone has business\n",
      "work at center has comes are willing stand for period\n",
      "amazon has cares about work / life balance\n",
      "human has issue of work / life balance\n",
      "amazon has bunch of am pa\n",
      "jeff bezos has become man off blood\n",
      "requirements amazon has stated are use except for disability\n",
      "management has base information\n",
      "amazon has bias for promoting company\n",
      "company has reached point needs warm bodies\n",
      "amazon has become be announced trying minimize new hires\n",
      "education has spread out\n",
      "prime now has one of hiring practices\n",
      "company has during seasonal\n",
      "dallas has one inside city\n",
      "amazon has lot of great benefits\n",
      "amazon has requested urine test\n",
      "corporation has with opening communication goes\n",
      "executed confidentiality invention assignment agreement see has anything do with being absent sick with doctor 's note\n",
      "morale has dropped earning money\n",
      "amazon has work from home positions\n",
      "met by time fulfills company needs has contacted by hr manager\n",
      "amazon has hired felons\n",
      "company has gotten like see union\n",
      "management has step bottom line is\n",
      "management has talk\n",
      "had been management has been nice\n",
      "way law has order\n",
      "enjoying free drinks have work hard as employees has forced work till\n",
      "keeping mouth closed in role has with trump\n",
      "products is has reviews order\n",
      "email stating that “ has heard communicate during meeting needed switch to nit sorts\n",
      "live has lot of construction\n",
      "provides livable wage has been at forefront of innovation\n",
      "company has organization\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(_prompt_retrieve('has')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: \n",
      " and the company does n't hire your if you do n't have a tam card ..\n",
      "['company-3', 'hire-6', 'have-12', 'do-10', 'have-12', 'card-15']\n",
      "company hire do have card ('company', 'card')\n",
      "\n",
      "_sent: yes that s what i lke about rhis company is that you can be yourself they do nt judge or makes you wear clothes or shoes do nt like\n",
      "['company-8', 'about-6', 'lke-5', 's-2', 'is-9', 'be-13', 'yourself-14', 'judge-18', 'do-16', 'judge-18']\n",
      "s lke about company is be yourself do judge ('company', 'judge')\n",
      "\n",
      "_sent: no marshalls does not do any drug test .\n",
      "['marshalls-1', 'do-4', 'test-7', 'drug-6']\n",
      "marshalls do drug test ('marshalls', 'drug')\n",
      "\n",
      "_sent: how much is the pay in durham nc and do they pay every week or bi weekly\n",
      "['nc-7', 'in-5', 'pay-4', 'is-2', 'pay-11', 'do-9', 'pay-11', 'week-13', 'bi-15', 'weekly-16']\n",
      "is pay in nc do pay week bi weekly ('nc', 'bi weekly')\n",
      "\n",
      "_sent: marshalls do not allow leggings to be worn at any time .\n",
      "['marshalls-0', 'allow-3', 'do-1', 'allow-3', 'worn-7', 'leggings-4']\n",
      "marshalls do allow leggings worn ('marshalls', 'leggings')\n",
      "\n",
      "_sent: the drug test they would do is urine .\n",
      "['test-2', 'do-5', 'test-2', 'is-6', 'urine-7']\n",
      "test do is urine ('test', 'urine')\n",
      "\n",
      "_sent: i did n't mind it at first for a part time job to have me do something while i was in community college , but now that i left i 'd say that this is a great first job if you 're wanting to experience what retail is like .\n",
      "['time-10', 'job-11', 'have-13', 'do-15', 'was-19', 'in-20', 'college-22', 'community-21', 'part-9']\n",
      "part time job have do was in community college ('part time', 'community')\n",
      "\n",
      "_sent: the company will do drug testing and will ask for background checks .\n",
      "['company-1', 'do-3', 'testing-5', 'drug-4']\n",
      "company do drug testing ('company', 'drug')\n",
      "\n",
      "_sent: they allow you to dress freely as long as you do n't wear big logos or ripped jeans .\n",
      "['long-7', 'wear-12', 'do-10', 'wear-12', 'logos-14']\n",
      "long do wear logos ('long', 'logos')\n",
      "\n",
      "_sent: depending on when do you apply to a position , and for how long that position would be posted , it could take a minimum of 3 weeks ( for existing employees ) to 5 weeks for new hired employees .\n",
      "['depending-0', 'apply-5', 'do-3', 'apply-5', 'take-22', 'for-11', 'posted-18', 'long-13', 'on-1']\n",
      "depending on do apply for long posted take ('depending on', 'long')\n",
      "\n",
      "_sent: i sent my application may 11th 2019 on may 13th they sent me an email to do the video interview .\n",
      "['email-14', 'do-16', 'interview-19', 'video-18']\n",
      "email do video interview ('email', 'video')\n",
      "\n",
      "_sent: at&t do take drug test .\n",
      "['at&t-0', 'take-2', 'do-1', 'take-2', 'test-4', 'drug-3']\n",
      "at&t do take drug test ('at&t', 'drug')\n",
      "\n",
      "_sent:   i would think with those working in non - management positions such as outside plant and installers , they probably do have drug testing .\n",
      "['plant-15', 'as-13', 'positions-11', 'in-7', 'working-6', 'those-5', 'with-4', 'think-3', 'have-22', 'do-21', 'have-22', 'testing-24', 'drug-23']\n",
      "think with those working in positions as plant do have drug testing ('plant', 'drug')\n",
      "\n",
      "_sent: how much knowledge do you have regarding new cell phones , accessories and tablets .\n",
      "['knowledge-2', 'have-5', 'do-3', 'have-5', 'regarding-6', 'phones-9', 'cell-8']\n",
      "knowledge do have regarding cell phones ('knowledge', 'cell phones')\n",
      "\n",
      "_sent: like working for hitler they do not care about employees as long as they can make themselves look good .\n",
      "['hitler-3', 'for-2', 'working-1', 'like-0', 'care-7', 'do-5', 'care-7', 'long-11']\n",
      "like working for hitler do care long ('hitler', 'long')\n",
      "\n",
      "_sent: your life is over and they do not care about you or your family .\n",
      "['life-1', 'is-2', 'care-8', 'do-6', 'care-8', 'about-9', 'you-10', 'family-13']\n",
      "life is do care about you family ('life', 'family')\n",
      "\n",
      "_sent: i went to college , they do not work with school\n",
      "['college-3', 'to-2', 'went-1', 'work-8', 'do-6', 'work-8', 'with-9', 'school-10']\n",
      "went to college do work with school ('college', 'school')\n",
      "\n",
      "_sent: management thinks you should sleep with them , and if you do nt your life will be misery , and good luck reporting it , i tried that route and was told oh i 'm so sorry and nothing was even done about it or even looked at even though i had concrete proof and other agents had reported the same manager harassing them as well .\n",
      "['sleep-4', 'thinks-1', 'be-16', 'do-11', 'be-16', 'life-14']\n",
      "thinks sleep do life be ('sleep', 'life')\n",
      "\n",
      "_sent: so far major lipservice is all you have managed to do in this area .\n",
      "['major-2', 'lipservice-3', 'is-4', 'all-5', 'managed-8', 'do-10', 'in-11', 'area-13']\n",
      "major lipservice is all managed do in area ('major', 'area')\n",
      "\n",
      "_sent: lack of support from management and the fact that i do n't count .\n",
      "['management-4', 'from-3', 'support-2', 'of-1', 'lack-0', 'fact-7', 'count-12', 'do-10', 'count-12']\n",
      "lack of support from management fact do count ('management', 'count')\n",
      "\n",
      "_sent: i had weekends off most of my career with at&t and there were lots of employees you could do a shift trade with .\n",
      "['at&t-9', 'with-8', 'weekends-2', 'had-1', 'do-18', 'trade-21']\n",
      "had weekends with at&t do trade ('at&t', 'trade')\n",
      "\n",
      "_sent: yes , there are flexible schedules that do allow for training etc .\n",
      "['schedules-5', 'allow-8', 'do-7', 'allow-8', 'for-9', 'etc-11', 'training-10', 'flexible-4']\n",
      "flexible schedules do allow for training etc ('flexible schedules', 'training')\n",
      "\n",
      "_sent:   i am a retiree and would love to do something on contract with at&t.   i started in the call center and from there promoted up to 1st and then 2nd level in 12 years .\n",
      "['love-7', 'do-9', 'something-10', 'on-11', 'contract-12']\n",
      "love do something on contract ('love', 'contract')\n",
      "\n",
      "_sent: how long do you work there until the health insurance kicks in\n",
      "['long-1', 'work-4', 'do-2', 'work-4', 'until-6', 'kicks-10', 'insurance-9', 'health-8']\n",
      "long do work until health insurance kicks ('long', 'health')\n",
      "\n",
      "_sent: i have no idea as my position with at&t had nothing to do with hiring decisions .\n",
      "['at&t-8', 'with-7', 'position-6', 'had-9', 'nothing-10', 'do-12', 'with-13', 'hiring-14', 'decisions-15']\n",
      "position with at&t had nothing do with hiring decisions ('at&t', 'hiring decisions')\n",
      "\n",
      "_sent: i 'm sure if you 're going to be working in a retail environment with at&t they do a pretty extensive background check .\n",
      "['at&t-16', 'with-15', 'working-10', 'going-7', 'do-18', 'check-23', 'extensive-21', 'background-22']\n",
      "going working with at&t do extensive background check ('at&t', 'extensive background check')\n",
      "\n",
      "_sent: nothing in particular as management employees do not get top notch \" paid benefits \"\n",
      "['management-4', 'employees-5', 'get-8', 'do-6', 'get-8', 'benefits-13']\n",
      "management employees do get benefits ('management', 'benefits')\n",
      "\n",
      "_sent: make sure there is a contact number or email in order to do a follow up .\n",
      "['order-10', 'do-12', 'follow-14', 'up-15']\n",
      "order do follow up ('order', 'follow up')\n",
      "\n",
      "_sent: what is your ambition in life and where do you see yourself in 5 years with at&t ?\n",
      "['life-5', 'in-4', 'ambition-3', 'is-1', 'see-10', 'do-8', 'see-10', 'with-15', 'at&t-16']\n",
      "is ambition in life do see with at&t ('life', 'at&t')\n",
      "\n",
      "_sent: no if your part time they do not extend benefits and the union acts like your invisible\n",
      "['time-4', 'extend-8', 'do-6', 'extend-8', 'benefits-9', 'part-3']\n",
      "part time do extend benefits ('part time', 'benefits')\n",
      "\n",
      "_sent:  \n",
      "\n",
      " answer : it cost less to maintain at&t system in texas rather than in california , do to personnel and the needs of the business .\n",
      "['california-15', 'in-14', 'maintain-7', 'cost-4', 'do-17', 'to-18', 'personnel-19', 'needs-22', 'of-23', 'business-25']\n",
      "cost maintain in california do to personnel needs of business ('california', 'business')\n",
      "\n",
      "_sent: not long at all , you will do a few days of training and you will start making phone calls after a few training sessions via online .\n",
      "['long-1', 'all-3', 'do-7', 'days-10', 'of-11', 'training-12']\n",
      "long all do days of training ('long', 'training')\n",
      "\n",
      "_sent: i was not associated or involved in the hr department and do not know at&t pay rates .\n",
      "['department-9', 'in-6', 'involved-5', 'associated-3', 'know-13', 'do-11', 'know-13', 'rates-16', 'at&t-14']\n",
      "associated involved in department do know at&t rates ('department', 'at&t')\n",
      "\n",
      "_sent: i was not associated or involved in the hr department and do not know at&t hiring / substance use policies .\n",
      "['department-9', 'in-6', 'involved-5', 'associated-3', 'know-13', 'do-11', 'know-13', 'use-18', 'substance-17', 'at&t-14']\n",
      "associated involved in department do know at&t substance use ('department', 'at&t')\n",
      "\n",
      "_sent: drugs are a non factor , there is so much to do with such short time frames , those that use drugs do so far from the job and to their own detriment .\n",
      "['drugs-0', 'are-1', 'is-7', 'much-9', 'do-11', 'with-12', 'frames-16', 'time-15', 'short-14']\n",
      "drugs are is much do with short time frames ('drugs', 'short')\n",
      "\n",
      "_sent: since hostess shifts are generally 6 hours long with no one to cover , i do not get time to have an employee meal .\n",
      "['cover-12', 'one-10', 'with-8', 'are-3', 'get-17', 'do-15', 'get-17', 'time-18', 'have-20', 'meal-23', 'employee-22']\n",
      "are with one cover do get time have employee meal ('cover', 'employee meal')\n",
      "\n",
      "_sent: does cracker barrel do direct deposit\n",
      "['barrel-2', 'do-3', 'deposit-5', 'cracker-1', 'direct-4']\n",
      "cracker barrel do direct deposit ('cracker barrel', 'direct deposit')\n",
      "\n",
      "_sent: what web site do i go under to check the cracker barrel background check .\n",
      "['site-2', 'go-5', 'do-3', 'go-5', 'check-8', 'check-13', 'barrel-11', 'web-1', 'cracker-10']\n",
      "web site do go check cracker barrel check ('web site', 'cracker barrel')\n",
      "\n",
      "_sent: how long do it take for a background check to come back\n",
      "['long-1', 'take-4', 'do-2', 'take-4', 'for-5', 'check-8', 'background-7']\n",
      "long do take for background check ('long', 'background check')\n",
      "\n",
      "_sent: the only one suffering is the guest ; get in the business of retaining   ( you are exhausting yourself ) when all you do is train .\n",
      "['business-11', 'in-9', 'get-8', 'exhausting-18', 'is-25', 'all-22', 'do-24', 'all-22', 'is-25', 'train-26']\n",
      "get in business exhausting all do is train ('business', 'train')\n",
      "\n",
      "_sent: i am not very familiar with the hiring process of the company , but i know they do a background check .\n",
      "['company-11', 'of-9', 'process-8', 'with-5', 'familiar-4', 'am-1', 'know-15', 'do-17', 'check-20', 'background-19']\n",
      "am familiar with process of company know do background check ('company', 'background check')\n",
      "\n",
      "_sent: but in my store the district manager and store manager do not mind very small nose piercings\n",
      "['manager-9', 'mind-12', 'do-10', 'mind-12', 'piercings-16', 'store-8', 'nose-15']\n",
      "store manager do mind nose piercings ('store manager', 'nose piercings')\n",
      "\n",
      "_sent: i got to watch a video and do paper work then they made me come back and do an online course which took at 3 hours .\n",
      "['video-5', 'watch-3', 'do-7', 'work-9', 'paper-8']\n",
      "watch video do paper work ('video', 'paper')\n",
      "\n",
      "_sent: usually they only want natural hair colors however they do n't always make a big deal about it .\n",
      "['hair-5', 'colors-6', 'want-3', 'make-12', 'do-9', 'make-12', 'deal-15', 'big-14']\n",
      "want hair colors do make big deal ('hair', 'big deal')\n",
      "\n",
      "_sent: yea they will hire you as long as you do n't have violent crime\n",
      "['long-6', 'have-11', 'do-9', 'have-11', 'crime-13', 'violent-12']\n",
      "long do have violent crime ('long', 'violent crime')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: ask if they can undergo high stress environment with extreme heat some can do one or the other but doing both is a big deal while working at cracker barrel\n",
      "['heat-10', 'with-8', 'undergo-4', 'do-13', 'is-21', 'deal-24', 'big-23']\n",
      "undergo with heat do is big deal ('heat', 'big deal')\n",
      "\n",
      "_sent: does cracker barrel do direct deposit\n",
      "['barrel-2', 'do-3', 'deposit-5', 'cracker-1', 'direct-4']\n",
      "cracker barrel do direct deposit ('cracker barrel', 'direct deposit')\n",
      "\n",
      "_sent: nail polish / acrylic is allowed only if you do not handle food products and it has to be a color that matches your skin tone\n",
      "['polish-1', 'acrylic-3', 'allowed-5', 'handle-11', 'do-9', 'handle-11', 'products-13', 'food-12']\n",
      "polish acrylic allowed do handle food products ('polish', 'food')\n",
      "\n",
      "src not matched\n",
      "_sent: classes normally ran from 8:00 am - 4:30 pm , however , i would normally be an hour early and stay at least a half hour late to do all of my daily emails and voice messages .\n",
      "['hour-25', 'late-26', 'stay-20', 'do-28', 'all-29', 'of-30', 'emails-33', 'daily-32', 'half-24']\n",
      "stay half hour late do all of daily emails ('half hour', 'daily')\n",
      "\n",
      "_sent: some jobs at the bank do not require dealing with counting large amounts of money and much is done electronically .\n",
      "['bank-4', 'at-2', 'jobs-1', 'require-7', 'do-5', 'require-7', 'dealing-8', 'with-9', 'counting-10', 'amounts-12', 'of-13', 'money-14']\n",
      "jobs at bank do require dealing with counting amounts of money ('bank', 'money')\n",
      "\n",
      "_sent: yes , as most banks do require a credit check , backround check - fingerprinting .\n",
      "['banks-4', 'require-6', 'do-5', 'require-6', 'check-9', 'credit-8']\n",
      "banks do require credit check ('banks', 'credit check')\n",
      "\n",
      "_sent: during my 2nd interview i was asked if i could do a third interview with the hiring manager so i had my 3rd interview in the same day .\n",
      "['interview-3', 'during-0', 'asked-6', 'do-10', 'interview-13', 'with-14', 'manager-17', '2nd-2', 'hiring-16']\n",
      "during 2nd interview asked do interview with hiring manager ('2nd interview', 'hiring manager')\n",
      "\n",
      "_sent: i think wells fargo needs to do a mental health screening on all their current managers .\n",
      "['fargo-3', 'needs-4', 'do-6', 'screening-10', 'mental-8', 'health-9']\n",
      "fargo needs do mental health screening ('fargo', 'mental health')\n",
      "\n",
      "_sent: in our department we did not do credit checks , but they may have done so at different departments .\n",
      "['department-2', 'in-0', 'do-6', 'checks-8', 'credit-7']\n",
      "in department do credit checks ('department', 'credit checks')\n",
      "\n",
      "_sent: it was really quick , i got a phone call to do a one on one after the group interview about three or four hours after the end of the group interview .\n",
      "['call-9', 'do-11', 'interview-19', 'phone-8', 'group-18']\n",
      "phone call do group interview ('phone call', 'group interview')\n",
      "\n",
      "_sent: yes , part timers do get benefits just the same as a full timer .\n",
      "['timers-3', 'get-5', 'do-4', 'get-5', 'benefits-6', 'part-2']\n",
      "part timers do get benefits ('part timers', 'benefits')\n",
      "\n",
      "_sent: just a general do not reply email to confirm my 2nd interview appointment .\n",
      "['general-2', 'reply-5', 'do-3', 'reply-5', 'email-6']\n",
      "general do reply email ('general', 'email')\n",
      "\n",
      "_sent: teller does not that the same password and does not have to do as much as lead .\n",
      "['password-6', 'does-1', 'have-10', 'do-12', 'much-14', 'as-15', 'lead-16']\n",
      "does password have do much as lead ('password', 'lead')\n",
      "\n",
      "_sent: working remotely out of the san antonio area , but i do believe when i first started with world savings there was random drug testing involved during the hiring process .\n",
      "['area-7', 'of-3', 'out-2', 'working-0', 'believe-12', 'do-11', 'believe-12', 'was-21', 'testing-24', 'random-22', 'drug-23']\n",
      "working out of area do believe was random drug testing ('area', 'random drug testing')\n",
      "\n",
      "_sent: yes take finger prints and do drug test .\n",
      "['prints-3', 'take-1', 'do-5', 'test-7', 'drug-6', 'finger-2']\n",
      "take finger prints do drug test ('finger prints', 'drug')\n",
      "\n",
      "_sent: wells fargo does do a background check before you are eligible to start work there .\n",
      "['fargo-1', 'do-3', 'check-6', 'background-5']\n",
      "fargo do background check ('fargo', 'background check')\n",
      "\n",
      "_sent: yes all job with this company do drug test\n",
      "['company-5', 'with-3', 'job-2', 'do-6', 'test-8', 'drug-7']\n",
      "job with company do drug test ('company', 'drug')\n",
      "\n",
      "_sent: i am not sure what a phone banker is , nor do i know roanoke va .\n",
      "['banker-7', 'is-8', 'sure-3', 'am-1', 'know-13', 'do-11', 'know-13', 'va-15', 'roanoke-14', 'phone-6']\n",
      "am sure phone banker is do know roanoke va ('phone banker', 'roanoke')\n",
      "\n",
      "_sent: criminal history , i believe they do a credit check too\n",
      "['history-1', 'believe-4', 'do-6', 'check-9', 'credit-8']\n",
      "history believe do credit check ('history', 'credit check')\n",
      "\n",
      "_sent: does anybody know if wells fargo background check have alot to do with education ?\n",
      "['check-7', 'have-8', 'alot-9', 'do-11', 'with-12', 'education-13', 'background-6']\n",
      "background check have alot do with education ('background check', 'education')\n",
      "\n",
      "_sent: have to take a phone interview and then come in to do a group interview\n",
      "['interview-5', 'take-2', 'come-8', 'do-11', 'interview-14', 'phone-4', 'group-13']\n",
      "take phone interview come do group interview ('phone interview', 'group interview')\n",
      "\n",
      "_sent:   they will fingerprint you but do not administer any drug tests .\n",
      "['fingerprint-3', 'administer-8', 'do-6', 'administer-8', 'tests-11', 'drug-10']\n",
      "fingerprint do administer drug tests ('fingerprint', 'drug')\n",
      "\n",
      "_sent: part time employees do not receive benefits that i am aware of .\n",
      "['time-1', 'receive-5', 'do-3', 'receive-5', 'benefits-6', 'part-0']\n",
      "part time do receive benefits ('part time', 'benefits')\n",
      "\n",
      "_sent: yes the management of home depot do not discriminate in any way towards those with a criminal record , it anything they are always willing to help people get back on their feet .\n",
      "['depot-5', 'of-3', 'management-2', 'discriminate-8', 'do-6', 'discriminate-8', 'in-9', 'way-11', 'towards-12', 'those-13', 'with-14', 'record-17', 'home-4', 'criminal-16']\n",
      "management of home depot do discriminate in way towards those with criminal record ('home depot', 'criminal record')\n",
      "\n",
      "_sent: i did a drug screening for employment at home depot how do i give list of prescription drugs i 'm on ? ?\n",
      "['depot-9', 'at-7', 'did-1', 'give-13', 'do-11', 'give-13', 'list-14', 'of-15', 'drugs-17', 'home-8', 'prescription-16']\n",
      "did at home depot do give list of prescription drugs ('home depot', 'prescription drugs')\n",
      "\n",
      "_sent: the home depot on thompson lane in nashville tn does drug test and they do mouth swab\n",
      "['test-11', 'does-9', 'do-14', 'swab-16', 'mouth-15']\n",
      "does test do mouth swab ('test', 'mouth')\n",
      "\n",
      "_sent: how does home depot   do the pre drug test is it swab or urine\n",
      "['depot-3', 'do-5', 'is-10', 'test-9', 'drug-8', 'home-2']\n",
      "home depot do drug test is ('home depot', 'drug')\n",
      "\n",
      "_sent: for asheville nc   home depot how do they do the test swab or urine\n",
      "['depot-5', 'do-9', 'do-7', 'do-9', 'swab-12', 'test-11', 'home-4']\n",
      "home depot do do test swab ('home depot', 'test')\n",
      "\n",
      "_sent: part time workers do not get benefits besides some vacation days and sick days .\n",
      "['time-1', 'workers-2', 'get-5', 'do-3', 'get-5', 'benefits-6', 'part-0']\n",
      "part time workers do get benefits ('part time', 'benefits')\n",
      "\n",
      "_sent: i am not sure my job description is not a chasier or has nothing to do with that area\n",
      "['description-6', 'is-7', 'has-12', 'nothing-13', 'do-15', 'with-16', 'area-18', 'job-5']\n",
      "job description is has nothing do with area ('job description', 'area')\n",
      "\n",
      "_sent: department managers do not work set schedules\n",
      "['department-0', 'managers-1', 'work-4', 'do-2', 'work-4', 'set-5', 'schedules-6']\n",
      "department managers do work set schedules ('department', 'set schedules')\n",
      "\n",
      "_sent: are you a friendly outgoing person do you like working with the public how are your customer service skills\n",
      "['friendly-3', 'outgoing-4', 'person-5', 'are-0', 'like-8', 'do-6', 'like-8', 'working-9', 'with-10', 'public-12']\n",
      "are friendly outgoing person do like working with public ('friendly', 'public')\n",
      "\n",
      "_sent: no the home depot do not hire felons\n",
      "['depot-3', 'hire-6', 'do-4', 'hire-6', 'home-2', 'felons-7']\n",
      "home depot do hire felons ('home depot', 'hire felons')\n",
      "\n",
      "_sent: management only cares about the numbers and if you do n't get a customer to apply for a credit card you will receive a note in the system and then terminated .\n",
      "['management-0', 'cares-2', 'get-11', 'do-9', 'get-11', 'apply-15', 'for-16', 'card-19', 'credit-18']\n",
      "management cares do get apply for credit card ('management', 'credit card')\n",
      "\n",
      "_sent: department heads are rude and do n't know how to talk to their workers , then they expect you to do they work while they take credit for it unless it is done incorrectly .\n",
      "['department-0', 'heads-1', 'are-2', 'know-7', 'do-5', 'know-7', 'talk-10']\n",
      "department heads are do know talk ('department', 'talk')\n",
      "\n",
      "_sent: company hires some of the most incompetent managers , which is your corporate district people , they absolutely do not   have a good work life balance ,   do n't plan on making it a career because if you work your way up to a decent wage and your in your 40 ' s   you can be assured they will shove you around till you leave , home depot is famous for getting rid of older people making good money , but very careful how they do it ,   look up history on a law suite in co. any good people they suck dry ,   the negetives you read are true\n",
      "['district-13', 'people-14', 'is-10', 'managers-7', 'of-3', 'some-2', 'hires-1', 'have-21', 'do-18', 'have-21', 'balance-26', 'work-24', 'life-25']\n",
      "hires some of managers is district people do have work life balance ('district', 'work life balance')\n",
      "\n",
      "_sent: new district manager who spent thousands of dollars to make things pretty , removed anti - fatigue mats , self - checkout registers do n't work , bad working conditions , threaten to take away your breaks and lunches because no coverage .\n",
      "['anti-14', '--15', 'fatigue-16', 'mats-17', 'manager-2', 'work-25', 'do-23', 'work-25', 'conditions-29', 'working-28']\n",
      "manager anti - fatigue mats do work working conditions ('anti', 'working conditions')\n",
      "\n",
      "_sent: butt off get all your funky reeards you hand out and do e fluzy thier only 1year get promoted to full time .\n",
      "e do full time\n",
      "None 10540441779542591554 14930118741968279164\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: management has clearly shown favoritism , and exclusion of employees they do n’t like from company gifts and promotions .\n",
      "['management-0', 'shown-3', 'exclusion-7', 'do-11', 'like-13', 'from-14', 'gifts-16', 'company-15']\n",
      "management shown exclusion do like from company gifts ('management', 'company')\n",
      "\n",
      "_sent: if part time you do n't get a weekend off , ever , unless you request it .\n",
      "['time-2', 'get-6', 'do-4', 'get-6', 'weekend-8', 'part-1', 'off-9']\n",
      "part time do get weekend off ('part time', 'weekend off')\n",
      "\n",
      "_sent: what products home depot sells , what 's my work experience like , anything to do with customer relations .\n",
      "['depot-3', 'sells-4', \"'s-7\", 'anything-13', 'do-15', 'with-16', 'relations-18', 'home-2', 'customer-17']\n",
      "home depot sells 's anything do with customer relations ('home depot', 'customer relations')\n",
      "\n",
      "_sent: as long as you wear a collared shirt or polo and do not show any rude logos they do not care .\n",
      "['polo-9', 'shirt-7', 'wear-4', 'show-13', 'do-11', 'show-13', 'logos-16']\n",
      "wear shirt polo do show logos ('polo', 'logos')\n",
      "\n",
      "_sent: \n",
      " how long do you think you would stay @ home depot ?\n",
      "['long-2', 'think-5', 'do-3', 'think-5', 'stay-8', '@-9', 'depot-11', 'home-10']\n",
      "long do think stay @ home depot ('long', 'home depot')\n",
      "\n",
      "_sent: urine , i do n't do drugs so not really a concern .\n",
      "['urine-0', 'do-5', 'do-3', 'do-5', 'drugs-6']\n",
      "urine do do drugs ('urine', 'drugs')\n",
      "\n",
      "_sent: not sure , but as long as you do n't operate heavy machinery , i think you 'll be fine .\n",
      "['long-5', 'operate-10', 'do-8', 'operate-10', 'machinery-12', 'heavy-11']\n",
      "long do operate heavy machinery ('long', 'heavy machinery')\n",
      "\n",
      "_sent: worst company ever if your in houston texas jake   mcord is a lying weasel and snake , promise to pay but never do , worthless excuse of a marine\n",
      "['snake-16', 'weasel-14', 'is-11', 'promise-18', 'pay-20', 'do-23', 'excuse-26', 'of-27', 'marine-29']\n",
      "is weasel snake promise pay do excuse of marine ('snake', 'marine')\n",
      "\n",
      "_sent: home depot will do their best to work with you through your disability .\n",
      "['depot-1', 'do-3', 'work-7', 'through-10', 'disability-12', 'home-0']\n",
      "home depot do work through disability ('home depot', 'disability')\n",
      "\n",
      "_sent: i started with the company 16 years ago and i do not recall if there was   a background check .\n",
      "['company-4', 'with-2', 'started-1', 'recall-12', 'do-10', 'recall-12', 'was-15', 'check-19', 'background-18']\n",
      "started with company do recall was background check ('company', 'background check')\n",
      "\n",
      "_sent: easy to pass , as long as you do n't do anything for 24 - 72 hours before your test !\n",
      "['long-5', 'do-10', 'do-8', 'do-10', 'before-17', 'test-19']\n",
      "long do do before test ('long', 'test')\n",
      "\n",
      "_sent: i did n't work in this location , but all home depot stores do individual criminal background checks i believe , but not sure .\n",
      "['depot-11', 'stores-12', 'do-13', 'checks-17', 'background-16', 'home-10', 'criminal-15']\n",
      "home depot stores do criminal background checks ('home depot', 'criminal background')\n",
      "\n",
      "src not matched\n",
      "_sent: no idea is this location does drug testing or not i do n't know why they chose wind him connecticut because i worked at a home depot in mount pleasant texas\n",
      "['drug-6', 'testing-7', 'does-5', 'know-13', 'do-11', 'know-13', 'chose-16', 'wind-17']\n",
      "does drug testing do know chose wind ('drug', 'wind')\n",
      "\n",
      "_sent: as long as you do n't have a felony you should be ok .\n",
      "['long-1', 'have-6', 'do-4', 'have-6', 'felony-8']\n",
      "long do have felony ('long', 'felony')\n",
      "\n",
      "_sent: these associates are considered home depot associates and do receive home depot benefits\n",
      "['depot-5', 'associates-6', 'considered-3', 'receive-9', 'do-8', 'receive-9', 'benefits-12', 'home-4']\n",
      "considered home depot associates do receive benefits ('home depot', 'benefits')\n",
      "\n",
      "_sent: in orlando , florida , i was required to do the saliva swab drug test .\n",
      "['florida-3', 'orlando-1', 'in-0', 'required-7', 'do-9', 'test-14', 'saliva-11']\n",
      "in orlando florida required do saliva test ('florida', 'saliva')\n",
      "\n",
      "_sent: if you are offered a position with the home depot you do have a drug test administered on - site .\n",
      "['depot-9', 'have-12', 'do-11', 'have-12', 'test-15', 'drug-14', 'home-8']\n",
      "home depot do have drug test ('home depot', 'drug')\n",
      "\n",
      "_sent: i never whent to hawaii to do the drug screening i did it in new jersey\n",
      "['hawaii-4', 'do-6', 'screening-9', 'drug-8']\n",
      "hawaii do drug screening ('hawaii', 'drug')\n",
      "\n",
      "company hire do have card\n",
      "s lke about company is be yourself do judge\n",
      "marshalls do drug test\n",
      "is pay in nc do pay week bi weekly\n",
      "marshalls do allow leggings worn\n",
      "test do is urine\n",
      "part time job have do was in community college\n",
      "company do drug testing\n",
      "long do wear logos\n",
      "depending on do apply for long posted take\n",
      "email do video interview\n",
      "at&t do take drug test\n",
      "think with those working in positions as plant do have drug testing\n",
      "knowledge do have regarding cell phones\n",
      "like working for hitler do care long\n",
      "life is do care about you family\n",
      "went to college do work with school\n",
      "thinks sleep do life be\n",
      "major lipservice is all managed do in area\n",
      "lack of support from management fact do count\n",
      "had weekends with at&t do trade\n",
      "flexible schedules do allow for training etc\n",
      "love do something on contract\n",
      "long do work until health insurance kicks\n",
      "position with at&t had nothing do with hiring decisions\n",
      "going working with at&t do extensive background check\n",
      "management employees do get benefits\n",
      "order do follow up\n",
      "is ambition in life do see with at&t\n",
      "part time do extend benefits\n",
      "cost maintain in california do to personnel needs of business\n",
      "long all do days of training\n",
      "associated involved in department do know at&t rates\n",
      "associated involved in department do know at&t substance use\n",
      "drugs are is much do with short time frames\n",
      "are with one cover do get time have employee meal\n",
      "cracker barrel do direct deposit\n",
      "web site do go check cracker barrel check\n",
      "long do take for background check\n",
      "get in business exhausting all do is train\n",
      "am familiar with process of company know do background check\n",
      "store manager do mind nose piercings\n",
      "watch video do paper work\n",
      "want hair colors do make big deal\n",
      "long do have violent crime\n",
      "undergo with heat do is big deal\n",
      "cracker barrel do direct deposit\n",
      "polish acrylic allowed do handle food products\n",
      "stay half hour late do all of daily emails\n",
      "jobs at bank do require dealing with counting amounts of money\n",
      "banks do require credit check\n",
      "during 2nd interview asked do interview with hiring manager\n",
      "fargo needs do mental health screening\n",
      "in department do credit checks\n",
      "phone call do group interview\n",
      "part timers do get benefits\n",
      "general do reply email\n",
      "does password have do much as lead\n",
      "working out of area do believe was random drug testing\n",
      "take finger prints do drug test\n",
      "fargo do background check\n",
      "job with company do drug test\n",
      "am sure phone banker is do know roanoke va\n",
      "history believe do credit check\n",
      "background check have alot do with education\n",
      "take phone interview come do group interview\n",
      "fingerprint do administer drug tests\n",
      "part time do receive benefits\n",
      "management of home depot do discriminate in way towards those with criminal record\n",
      "did at home depot do give list of prescription drugs\n",
      "does test do mouth swab\n",
      "home depot do drug test is\n",
      "home depot do do test swab\n",
      "part time workers do get benefits\n",
      "job description is has nothing do with area\n",
      "department managers do work set schedules\n",
      "are friendly outgoing person do like working with public\n",
      "home depot do hire felons\n",
      "management cares do get apply for credit card\n",
      "department heads are do know talk\n",
      "hires some of managers is district people do have work life balance\n",
      "manager anti - fatigue mats do work working conditions\n",
      "management shown exclusion do like from company gifts\n",
      "part time do get weekend off\n",
      "home depot sells 's anything do with customer relations\n",
      "wear shirt polo do show logos\n",
      "long do think stay @ home depot\n",
      "urine do do drugs\n",
      "long do operate heavy machinery\n",
      "is weasel snake promise pay do excuse of marine\n",
      "home depot do work through disability\n",
      "started with company do recall was background check\n",
      "long do do before test\n",
      "home depot stores do criminal background checks\n",
      "does drug testing do know chose wind\n",
      "long do have felony\n",
      "considered home depot associates do receive benefits\n",
      "in orlando florida required do saliva test\n",
      "home depot do have drug test\n",
      "hawaii do drug screening\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(_prompt_retrieve('do')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: marshalls pays employees every other week , or biweekly .\n",
      "['marshalls-0', 'pays-1', 'biweekly-8']\n",
      "marshalls pays biweekly ('marshalls', 'biweekly')\n",
      "\n",
      "_sent: marshalls pays weekly\n",
      "['marshalls-0', 'pays-1', 'weekly-2']\n",
      "marshalls pays weekly ('marshalls', 'weekly')\n",
      "\n",
      "_sent: marshalls pays weekly , i myself have never had any problems with my paycheck while working here .\n",
      "['marshalls-0', 'pays-1', 'weekly-2']\n",
      "marshalls pays weekly ('marshalls', 'weekly')\n",
      "\n",
      "_sent: the marshall 's division pays associates on a weekly basis .\n",
      "['division-3', 'pays-4', 'on-6', 'basis-9', 'weekly-8']\n",
      "division pays on weekly basis ('division', 'weekly basis')\n",
      "\n",
      "_sent: the company pays weekly\n",
      "['company-1', 'pays-2', 'weekly-3']\n",
      "company pays weekly ('company', 'weekly')\n",
      "\n",
      "_sent: wells fargo pays biweekly pay periods for base pay and monthly for commission .\n",
      "['fargo-1', 'pays-2', 'pay-4', 'biweekly-3']\n",
      "fargo pays biweekly pay ('fargo', 'biweekly')\n",
      "\n",
      "_sent: i stayed with wells fargo because they are the only bank where i live that pays enough to support a family .\n",
      "['live-13', 'bank-10', 'pays-15', 'support-18', 'family-20']\n",
      "bank live pays support family ('live', 'family')\n",
      "\n",
      "_sent: yes i would be able to work with college courses as well , i could take morning or night classes , whatever fits in with my work schedule , i want a job that pays decent money so i can afford my funds at school along with extra money for stuff i need\n",
      "['college-8', 'courses-9', 'with-7', 'work-6', 'able-4', 'be-3', 'take-15', 'want-30', 'job-32', 'pays-34', 'money-36', 'decent-35']\n",
      "be able work with college courses take want job pays decent money ('college', 'decent money')\n",
      "\n",
      "_sent: home depot pays biweekly .\n",
      "['depot-1', 'pays-2', 'biweekly-3', 'home-0']\n",
      "home depot pays biweekly ('home depot', 'biweekly')\n",
      "\n",
      "_sent: home depot pays you biweekly\n",
      "['depot-1', 'pays-2', 'biweekly-4', 'home-0']\n",
      "home depot pays biweekly ('home depot', 'biweekly')\n",
      "\n",
      "_sent: a full time associate roughly pays $ 200 for medical insurance .\n",
      "['time-2', 'associate-3', 'pays-5', '200-7', 'for-8', 'insurance-10', 'medical-9', 'full-1']\n",
      "full time associate pays 200 for medical insurance ('full time', 'medical')\n",
      "\n",
      "_sent: home depot pays bi - weekly\n",
      "['depot-1', 'pays-2', 'weekly-5', 'home-0']\n",
      "home depot pays weekly ('home depot', 'weekly')\n",
      "\n",
      "_sent: home depot pays biweekly\n",
      "['depot-1', 'pays-2', 'biweekly-3', 'home-0']\n",
      "home depot pays biweekly ('home depot', 'biweekly')\n",
      "\n",
      "_sent: i m currently still working my full time job monday through friday , that pays the bills .\n",
      "['friday-11', 'through-10', 'working-4', 'pays-14', 'bills-16']\n",
      "working through friday pays bills ('friday', 'bills')\n",
      "\n",
      "_sent: amazon pays weekly\n",
      "['amazon-0', 'pays-1', 'weekly-2']\n",
      "amazon pays weekly ('amazon', 'weekly')\n",
      "\n",
      "_sent: 20 - 30 full time pays better but night shift make more lmoney lesson .\n",
      "['time-4', 'pays-5', 'make-10', 'shift-9', 'full-3', 'night-8']\n",
      "full time pays night shift make ('full time', 'night shift')\n",
      "\n",
      "_sent: dollar tree warehouse pays their employees more thn amazon and that s sad ! !\n",
      "['warehouse-2', 'pays-3', 'amazon-8']\n",
      "warehouse pays amazon ('warehouse', 'amazon')\n",
      "\n",
      "_sent: would mainly be because everywhere else has a pay increase or pays much more for less the stress of worrying about being terminated because you could nt make rate due to amazon system .\n",
      "['increase-9', 'has-6', 'pays-11', 'make-27', 'due-29', 'to-30', 'system-32', 'amazon-31', 'pay-8']\n",
      "has pay increase pays make due to amazon system ('pay increase', 'amazon')\n",
      "\n",
      "_sent: favoritism ... straight from the top , managers are instructed not to disipline the people from neopaul or any person from a foreign country ... these people are why amazon pays no taxes,,they make their production rate @460 items processed , then stop working .\n",
      "['amazon-29', 'pays-30', 'make-33', 'rate-36', 'production-35']\n",
      "amazon pays make production rate ('amazon', 'production')\n",
      "\n",
      "_sent: tampa pays seasonal $ 10.50 , full time starts at $ 11.00\n",
      "['tampa-0', 'pays-1', 'seasonal-2']\n",
      "tampa pays seasonal ('tampa', 'seasonal')\n",
      "\n",
      "_sent: tjmaxx distribuition center pays 1.25 as shift differential .\n",
      "['center-2', 'pays-3', 'as-5', 'differential-7', 'shift-6']\n",
      "center pays as shift differential ('center', 'shift differential')\n",
      "\n",
      "_sent: hope that answered your question but always ask because i know they only have that shift in columbus and all pays jump state to state and country to etc .\n",
      "['columbus-17', 'in-16', 'shift-15', 'have-13', 'pays-20', 'state-22']\n",
      "have shift in columbus pays state ('columbus', 'state')\n",
      "\n",
      "_sent: amazon pays bi weekly\n",
      "['amazon-0', 'pays-1', 'bi-2', 'weekly-3']\n",
      "amazon pays bi weekly ('amazon', 'bi weekly')\n",
      "\n",
      "_sent: from personal experience amazon pays bi - weekly .\n",
      "['amazon-3', 'from-0', 'pays-4', 'weekly-7']\n",
      "from amazon pays weekly ('amazon', 'weekly')\n",
      "\n",
      "_sent: amazon pays on a by weekly fashion\n",
      "['amazon-0', 'pays-1', 'by-4', 'fashion-6', 'weekly-5']\n",
      "amazon pays by weekly fashion ('amazon', 'weekly')\n",
      "\n",
      "_sent: amazon pays weekly for the roll i was hired for .\n",
      "['amazon-0', 'pays-1', 'weekly-2']\n",
      "amazon pays weekly ('amazon', 'weekly')\n",
      "\n",
      "_sent: no , subway pays bi - weekly so every 2 weeks .\n",
      "['subway-2', 'pays-3', 'weekly-6']\n",
      "subway pays weekly ('subway', 'weekly')\n",
      "\n",
      "src not matched\n",
      "_sent: most people give you \" that look \" when you say you work in fast food , but it pays the bills and it 's a great work environment .\n",
      "['food-15', 'in-13', 'work-12', 'say-10', 'look-6', 'give-2', 'pays-19', 'bills-21']\n",
      "give look say work in food pays bills ('food', 'bills')\n",
      "\n",
      "_sent: working night ( 3 - 9 average ) is just the same as working morning , it pays minimum wage and is n't worth it .\n",
      "['average-6', 'night-1', 'is-8', 'pays-17', 'wage-19', 'minimum-18']\n",
      "night average is pays minimum wage ('average', 'minimum wage')\n",
      "\n",
      "_sent: subway pays biweekly\n",
      "['subway-0', 'pays-1', 'biweekly-2']\n",
      "subway pays biweekly ('subway', 'biweekly')\n",
      "\n",
      "_sent: subway pays biweekly\n",
      "['subway-0', 'pays-1', 'biweekly-2']\n",
      "subway pays biweekly ('subway', 'biweekly')\n",
      "\n",
      "_sent: subway pays weekly\n",
      "['subway-0', 'pays-1', 'weekly-2']\n",
      "subway pays weekly ('subway', 'weekly')\n",
      "\n",
      "_sent: subway pays bi weekly on wednesday\n",
      "['subway-0', 'pays-1', 'bi-2', 'weekly-3']\n",
      "subway pays bi weekly ('subway', 'bi weekly')\n",
      "\n",
      "_sent: rural retreat subway pays bi weekly .\n",
      "['subway-2', 'pays-3', 'bi-4', 'weekly-5']\n",
      "subway pays bi weekly ('subway', 'bi weekly')\n",
      "\n",
      "_sent: if subway pays on friday , where does salary start counting hours ; what day , to where it caps ; ditto ?\n",
      "['subway-1', 'pays-2', 'on-3', 'friday-4']\n",
      "subway pays on friday ('subway', 'friday')\n",
      "\n",
      "_sent: subway pays the employees bi - weekly plus the occasional small tips .\n",
      "['subway-0', 'pays-1', 'weekly-6']\n",
      "subway pays weekly ('subway', 'weekly')\n",
      "\n",
      "_sent: subway pays biweekly\n",
      "['subway-0', 'pays-1', 'biweekly-2']\n",
      "subway pays biweekly ('subway', 'biweekly')\n",
      "\n",
      "_sent: fedex home delivery pays on a weekly basis .\n",
      "['fedex-0', 'delivery-2', 'pays-3', 'on-4', 'basis-7', 'weekly-6']\n",
      "fedex delivery pays on weekly basis ('fedex', 'weekly basis')\n",
      "\n",
      "_sent: weekly it pays every friday\n",
      "['weekly-0', 'pays-2', 'friday-4']\n",
      "weekly pays friday ('weekly', 'friday')\n",
      "\n",
      "_sent: fedex pays for the costs of the drug test .\n",
      "['fedex-0', 'pays-1', 'for-2', 'costs-4', 'of-5', 'test-8', 'drug-7']\n",
      "fedex pays for costs of drug test ('fedex', 'drug')\n",
      "\n",
      "_sent: fedex pays weekly\n",
      "['fedex-0', 'pays-1', 'weekly-2']\n",
      "fedex pays weekly ('fedex', 'weekly')\n",
      "\n",
      "_sent: yes , fedex pays for orientation and training .\n",
      "['fedex-2', 'pays-3', 'for-4', 'orientation-5']\n",
      "fedex pays for orientation ('fedex', 'orientation')\n",
      "\n",
      "_sent: yes , the management pays special attention to the new hires\n",
      "['management-3', 'pays-4', 'to-7', 'hires-10', 'new-9']\n",
      "management pays to new hires ('management', 'new hires')\n",
      "\n",
      "_sent: starbucks pays retail employees bi weekly .\n",
      "['starbucks-0', 'pays-1', 'employees-3', 'retail-2']\n",
      "starbucks pays retail employees ('starbucks', 'retail')\n",
      "\n",
      "_sent: starbucks pays bi - weekly .\n",
      "['starbucks-0', 'pays-1', 'weekly-4']\n",
      "starbucks pays weekly ('starbucks', 'weekly')\n",
      "\n",
      "_sent: starbucks pays bi - weekly\n",
      "['starbucks-0', 'pays-1', 'weekly-4']\n",
      "starbucks pays weekly ('starbucks', 'weekly')\n",
      "\n",
      "_sent: starbucks pays you bi - weekly .\n",
      "['starbucks-0', 'pays-1', 'weekly-5']\n",
      "starbucks pays weekly ('starbucks', 'weekly')\n",
      "\n",
      "_sent: starbucks pays biweekly\n",
      "['starbucks-0', 'pays-1', 'biweekly-2']\n",
      "starbucks pays biweekly ('starbucks', 'biweekly')\n",
      "\n",
      "_sent: starbucks pays biweekly with no stipend however you do get tips once a week .\n",
      "['starbucks-0', 'pays-1', 'biweekly-2']\n",
      "starbucks pays biweekly ('starbucks', 'biweekly')\n",
      "\n",
      "_sent: starbucks pays biweekly .\n",
      "['starbucks-0', 'pays-1', 'biweekly-2']\n",
      "starbucks pays biweekly ('starbucks', 'biweekly')\n",
      "\n",
      "_sent: starbucks pays their employees biweekly\n",
      "['starbucks-0', 'pays-1', 'biweekly-4']\n",
      "starbucks pays biweekly ('starbucks', 'biweekly')\n",
      "\n",
      "_sent: starbucks pays biweekly\n",
      "['starbucks-0', 'pays-1', 'biweekly-2']\n",
      "starbucks pays biweekly ('starbucks', 'biweekly')\n",
      "\n",
      "_sent: starbucks pays by weekly\n",
      "['starbucks-0', 'pays-1', 'by-2', 'weekly-3']\n",
      "starbucks pays by weekly ('starbucks', 'weekly')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: starbucks pays weekly\n",
      "['starbucks-0', 'pays-1', 'weekly-2']\n",
      "starbucks pays weekly ('starbucks', 'weekly')\n",
      "\n",
      "_sent: all of harmans kfc pays weekly which is great !\n",
      "['kfc-3', 'of-1', 'all-0', 'pays-4', 'weekly-5']\n",
      "all of kfc pays weekly ('kfc', 'weekly')\n",
      "\n",
      "_sent: kfc pays biweekly .\n",
      "['kfc-0', 'pays-1', 'biweekly-2']\n",
      "kfc pays biweekly ('kfc', 'biweekly')\n",
      "\n",
      "_sent: i think mcdonalds pays more and chock - fill - a gives more benefits like for example shcolarships\n",
      "['mcdonalds-2', 'pays-3', 'think-1', 'gives-11', 'benefits-13']\n",
      "think mcdonalds pays gives benefits ('mcdonalds', 'benefits')\n",
      "\n",
      "_sent: macy 's pays weekly + commission\n",
      "['macy-0', 'pays-2', 'weekly-3', \"'s-1\"]\n",
      "macy 's pays weekly (\"macy 's\", 'weekly')\n",
      "\n",
      "_sent: in florida and idaho macy 's pays weekly .\n",
      "['macy-4', 'florida-1', 'in-0', 'weekly-7', 'pays-6', 'weekly-7', \"'s-5\"]\n",
      "in florida macy 's pays weekly (\"macy 's\", 'weekly')\n",
      "\n",
      "_sent: chipotle mexican grill pays bi weekly .\n",
      "['mexican-1', 'grill-2', 'pays-3', 'bi-4', 'weekly-5']\n",
      "mexican grill pays bi weekly ('mexican', 'bi weekly')\n",
      "\n",
      "_sent: chipotle pays biweekly .\n",
      "['chipotle-0', 'pays-1', 'biweekly-2']\n",
      "chipotle pays biweekly ('chipotle', 'biweekly')\n",
      "\n",
      "_sent: chipotle pays bi - weekly .\n",
      "['chipotle-0', 'pays-1', 'weekly-4']\n",
      "chipotle pays weekly ('chipotle', 'weekly')\n",
      "\n",
      "_sent: chipotle pays bi - weekly\n",
      "['chipotle-0', 'pays-1', 'weekly-4']\n",
      "chipotle pays weekly ('chipotle', 'weekly')\n",
      "\n",
      "_sent: i am a college student , and no one pays my bills for me .\n",
      "['student-4', 'am-1', 'pays-9', 'bills-11']\n",
      "am student pays bills ('student', 'bills')\n",
      "\n",
      "_sent: it will be three weeks since panera bread pays bi - weekly .\n",
      "['bread-7', 'pays-8', 'weekly-11']\n",
      "bread pays weekly ('bread', 'weekly')\n",
      "\n",
      "_sent: jcpenney pays on a biweekly basis .\n",
      "['jcpenney-0', 'pays-1', 'on-2', 'basis-5', 'biweekly-4']\n",
      "jcpenney pays on biweekly basis ('jcpenney', 'biweekly')\n",
      "\n",
      "_sent: jcpenney pays biweekly .\n",
      "['jcpenney-0', 'pays-1', 'biweekly-2']\n",
      "jcpenney pays biweekly ('jcpenney', 'biweekly')\n",
      "\n",
      "_sent: jcpenney pays bi - weekly\n",
      "['jcpenney-0', 'pays-1', 'weekly-4']\n",
      "jcpenney pays weekly ('jcpenney', 'weekly')\n",
      "\n",
      "_sent: target pays weekly\n",
      "['target-0', 'pays-1', 'weekly-2']\n",
      "target pays weekly ('target', 'weekly')\n",
      "\n",
      "_sent: target pays bi - weekly\n",
      "['target-0', 'pays-1', 'weekly-4']\n",
      "target pays weekly ('target', 'weekly')\n",
      "\n",
      "_sent: target pays on a biweekly rate\n",
      "['target-0', 'pays-1', 'on-2', 'rate-5', 'biweekly-4']\n",
      "target pays on biweekly rate ('target', 'biweekly')\n",
      "\n",
      "_sent: target pays biweekly .\n",
      "['target-0', 'pays-1', 'biweekly-2']\n",
      "target pays biweekly ('target', 'biweekly')\n",
      "\n",
      "_sent: target pays biweekly .\n",
      "['target-0', 'pays-1', 'biweekly-2']\n",
      "target pays biweekly ('target', 'biweekly')\n",
      "\n",
      "_sent: yes target pays while you are in orientation .\n",
      "['target-1', 'pays-2', 'are-5', 'in-6', 'orientation-7']\n",
      "target pays are in orientation ('target', 'orientation')\n",
      "\n",
      "_sent: this company pays weekly\n",
      "['company-1', 'pays-2', 'weekly-3']\n",
      "company pays weekly ('company', 'weekly')\n",
      "\n",
      "_sent: target pays biweekly .\n",
      "['target-0', 'pays-1', 'biweekly-2']\n",
      "target pays biweekly ('target', 'biweekly')\n",
      "\n",
      "_sent: target pays its workers bi - weekly\n",
      "['target-0', 'pays-1', 'weekly-6']\n",
      "target pays weekly ('target', 'weekly')\n",
      "\n",
      "_sent: target pays biweekly\n",
      "['target-0', 'pays-1', 'biweekly-2']\n",
      "target pays biweekly ('target', 'biweekly')\n",
      "\n",
      "_sent: target pays bi weekly\n",
      "['target-0', 'pays-1', 'bi-2', 'weekly-3']\n",
      "target pays bi weekly ('target', 'bi weekly')\n",
      "\n",
      "_sent: target pays biweekly\n",
      "['target-0', 'pays-1', 'biweekly-2']\n",
      "target pays biweekly ('target', 'biweekly')\n",
      "\n",
      "_sent: target pays weekly\n",
      "['target-0', 'pays-1', 'weekly-2']\n",
      "target pays weekly ('target', 'weekly')\n",
      "\n",
      "_sent: olive garden pays bi - weekly\n",
      "['garden-1', 'pays-2', 'weekly-5']\n",
      "garden pays weekly ('garden', 'weekly')\n",
      "\n",
      "_sent: olive garden pays weekly\n",
      "['garden-1', 'pays-2', 'weekly-3']\n",
      "garden pays weekly ('garden', 'weekly')\n",
      "\n",
      "_sent: olive garden pays weekly\n",
      "['garden-1', 'pays-2', 'weekly-3']\n",
      "garden pays weekly ('garden', 'weekly')\n",
      "\n",
      "_sent: olive garden pays every friday\n",
      "['garden-1', 'pays-2', 'friday-4']\n",
      "garden pays friday ('garden', 'friday')\n",
      "\n",
      "_sent: olive garden pays bi - weekly\n",
      "['garden-1', 'pays-2', 'weekly-5']\n",
      "garden pays weekly ('garden', 'weekly')\n",
      "\n",
      "_sent: taken off wiki of goodwill mission statement , really when they should be eliminating ... \n",
      "\n",
      " they underpay people , underappreciate people , undervalue people , they want to find others to rule with an iron fist alongside them and most of all they do n't care for you ... literally if you do n't meet the production quota ( why do we have quotas when this company states and claims they want to eliminate barriers in the work force yet they then go around in the shadow and say if you do n't get these numbers you 're fired ... literally in the employee handbook ( idgaf about disposing confidential information they 're putting you on a leash until you get fired then silence you with threats as always ) \n",
      "\n",
      " mcdonalds pays better and at least have a more professional management ... \n",
      "\n",
      " now if you 'll excuse me i got a month left until i leave this horrid place and then hopefully my manager gets fired or found out about how abusive she freak'n is .\n",
      "['mcdonalds-132', 'pays-133', 'have-138', 'management-142', 'professional-141']\n",
      "mcdonalds pays have professional management ('mcdonalds', 'professional')\n",
      "\n",
      "_sent: this company pays biweekly .\n",
      "['company-1', 'pays-2', 'biweekly-3']\n",
      "company pays biweekly ('company', 'biweekly')\n",
      "\n",
      "_sent: burger king pays weekly .\n",
      "['king-1', 'pays-2', 'weekly-3']\n",
      "king pays weekly ('king', 'weekly')\n",
      "\n",
      "src not matched\n",
      "_sent: burger king pays biweekly\n",
      "['king-1', 'pays-2', 'biweekly-3']\n",
      "king pays biweekly ('king', 'biweekly')\n",
      "\n",
      "_sent: michigan burger king pays weekly ... what about terre haute indiana\n",
      "['king-2', 'pays-3', 'weekly-4']\n",
      "king pays weekly ('king', 'weekly')\n",
      "\n",
      "_sent: burger king in my experience pays bi - weekly(every 2 weeks ) and paydays usually fall on a friday .\n",
      "['king-1', 'pays-5', 'fall-15', 'on-16', 'friday-18']\n",
      "king pays fall on friday ('king', 'friday')\n",
      "\n",
      "_sent: i do n't know how burger king pays in baltimore but in scott city , mo , we got paid twice a month , once on the 7th of the month and once on the 22nd of the month .\n",
      "['king-6', 'pays-7', 'in-8', 'baltimore-9']\n",
      "king pays in baltimore ('king', 'baltimore')\n",
      "\n",
      "_sent: yes , burger king pays employees before thanksgiving .\n",
      "['king-3', 'pays-4', 'before-6', 'thanksgiving-7']\n",
      "king pays before thanksgiving ('king', 'thanksgiving')\n",
      "\n",
      "_sent: no , burger king pays bi - weekly .\n",
      "['king-3', 'pays-4', 'weekly-7']\n",
      "king pays weekly ('king', 'weekly')\n",
      "\n",
      "_sent: mburger king pays bi weekly\n",
      "['king-1', 'pays-2', 'bi-3', 'weekly-4']\n",
      "king pays bi weekly ('king', 'bi weekly')\n",
      "\n",
      "_sent: yes burger king pays you for jury duty\n",
      "['king-2', 'pays-3', 'for-5', 'duty-7', 'jury-6']\n",
      "king pays for jury duty ('king', 'jury duty')\n",
      "\n",
      "_sent: t - mobile pays bi - weekly .\n",
      "['mobile-2', 'pays-3', 'weekly-6']\n",
      "mobile pays weekly ('mobile', 'weekly')\n",
      "\n",
      "_sent: chili 's pays bi - weekly .\n",
      "['chili-0', 'pays-2', 'chili-0', 'weekly-5']\n",
      "chili pays weekly ('chili', 'weekly')\n",
      "\n",
      "_sent: chilis pays biweekly\n",
      "['chilis-0', 'pays-1', 'biweekly-2']\n",
      "chilis pays biweekly ('chilis', 'biweekly')\n",
      "\n",
      "marshalls pays biweekly\n",
      "marshalls pays weekly\n",
      "marshalls pays weekly\n",
      "division pays on weekly basis\n",
      "company pays weekly\n",
      "fargo pays biweekly pay\n",
      "bank live pays support family\n",
      "be able work with college courses take want job pays decent money\n",
      "home depot pays biweekly\n",
      "home depot pays biweekly\n",
      "full time associate pays 200 for medical insurance\n",
      "home depot pays weekly\n",
      "home depot pays biweekly\n",
      "working through friday pays bills\n",
      "amazon pays weekly\n",
      "full time pays night shift make\n",
      "warehouse pays amazon\n",
      "has pay increase pays make due to amazon system\n",
      "amazon pays make production rate\n",
      "tampa pays seasonal\n",
      "center pays as shift differential\n",
      "have shift in columbus pays state\n",
      "amazon pays bi weekly\n",
      "from amazon pays weekly\n",
      "amazon pays by weekly fashion\n",
      "amazon pays weekly\n",
      "subway pays weekly\n",
      "give look say work in food pays bills\n",
      "night average is pays minimum wage\n",
      "subway pays biweekly\n",
      "subway pays biweekly\n",
      "subway pays weekly\n",
      "subway pays bi weekly\n",
      "subway pays bi weekly\n",
      "subway pays on friday\n",
      "subway pays weekly\n",
      "subway pays biweekly\n",
      "fedex delivery pays on weekly basis\n",
      "weekly pays friday\n",
      "fedex pays for costs of drug test\n",
      "fedex pays weekly\n",
      "fedex pays for orientation\n",
      "management pays to new hires\n",
      "starbucks pays retail employees\n",
      "starbucks pays weekly\n",
      "starbucks pays weekly\n",
      "starbucks pays weekly\n",
      "starbucks pays biweekly\n",
      "starbucks pays biweekly\n",
      "starbucks pays biweekly\n",
      "starbucks pays biweekly\n",
      "starbucks pays biweekly\n",
      "starbucks pays by weekly\n",
      "starbucks pays weekly\n",
      "all of kfc pays weekly\n",
      "kfc pays biweekly\n",
      "think mcdonalds pays gives benefits\n",
      "macy 's pays weekly\n",
      "in florida macy 's pays weekly\n",
      "mexican grill pays bi weekly\n",
      "chipotle pays biweekly\n",
      "chipotle pays weekly\n",
      "chipotle pays weekly\n",
      "am student pays bills\n",
      "bread pays weekly\n",
      "jcpenney pays on biweekly basis\n",
      "jcpenney pays biweekly\n",
      "jcpenney pays weekly\n",
      "target pays weekly\n",
      "target pays weekly\n",
      "target pays on biweekly rate\n",
      "target pays biweekly\n",
      "target pays biweekly\n",
      "target pays are in orientation\n",
      "company pays weekly\n",
      "target pays biweekly\n",
      "target pays weekly\n",
      "target pays biweekly\n",
      "target pays bi weekly\n",
      "target pays biweekly\n",
      "target pays weekly\n",
      "garden pays weekly\n",
      "garden pays weekly\n",
      "garden pays weekly\n",
      "garden pays friday\n",
      "garden pays weekly\n",
      "mcdonalds pays have professional management\n",
      "company pays biweekly\n",
      "king pays weekly\n",
      "king pays biweekly\n",
      "king pays weekly\n",
      "king pays fall on friday\n",
      "king pays in baltimore\n",
      "king pays before thanksgiving\n",
      "king pays weekly\n",
      "king pays bi weekly\n",
      "king pays for jury duty\n",
      "mobile pays weekly\n",
      "chili pays weekly\n",
      "chilis pays biweekly\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(_prompt_retrieve('pays')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: with the unemployment rate being so low ( and you ’re not the only company that receives all of these complaints it ’s the way our whole system / economy is set up ) all of you corporations better be careful because people are clearly getting fed up being treated like americanized sweat shop workers .\n",
      "['company-14', 'receives-16', 'company-14', '’re-10', 'way-24', 'set-31', 'economy-29']\n",
      "’re company receives way economy set ('company', 'economy')\n",
      "\n",
      "_sent: make sure the products each one receives is awesome and has good reviews and working order before shipping to anybody makes a lasting customer relationship\n",
      "['products-3', 'receives-6', 'products-3', 'is-7', 'has-10', 'reviews-12', 'order-15']\n",
      "products receives is has reviews order ('products', 'order')\n",
      "\n",
      "_sent: once the candidate receives the drug screen email from infomart , containing a link to the drug screen then there , the canada is responsible for selecting a testing facility / collection site , printing the e - passport provided and bringing the e - passport and proper identification to drug screen collection site within 2 business days not including weekends and holidays , of receipt of the drug screen email from infomart .\n",
      "e receives drug\n",
      "None 10540441779542591554 14930118741968279164\n",
      "\n",
      "_sent: you qualify for certain ones as part time , full time receives all benefits .\n",
      "['time-10', 'receives-11', 'benefits-13', 'full-9']\n",
      "full time receives benefits ('full time', 'benefits')\n",
      "\n",
      "_sent: but for all i know so far , the students who do work here receives good benefits from target .\n",
      "['students-9', 'receives-14', 'benefits-16']\n",
      "students receives benefits ('students', 'benefits')\n",
      "\n",
      "_sent: after the organization receives the background check , they may decide to do their own investigating to determine whether or not a potential candidate can be hired .\n",
      "['organization-2', 'receives-3', 'check-6', 'background-5']\n",
      "organization receives background check ('organization', 'background check')\n",
      "\n",
      "_sent: you receive a drug test form and take it to a hospital and then petsmart receives the results .\n",
      "['petsmart-14', 'receives-15', 'results-17']\n",
      "petsmart receives results ('petsmart', 'results')\n",
      "\n",
      "_sent: no we do not only management receives sick days\n",
      "['management-5', 'receives-6', 'days-8', 'sick-7']\n",
      "management receives sick days ('management', 'sick days')\n",
      "\n",
      "_sent: yes , once the management receives the results , you should be informed within a week of taking the test\n",
      "['management-4', 'receives-5', 'results-7']\n",
      "management receives results ('management', 'results')\n",
      "\n",
      "_sent: yes everyoone   that works for walmart receives health insurance\n",
      "['walmart-6', 'for-5', 'works-4', 'receives-7', 'insurance-9', 'health-8']\n",
      "works for walmart receives health insurance ('walmart', 'health')\n",
      "\n",
      "_sent: 40 hours a week unless its around the holidays then they may ask you stay and collect overtime.usually everyone with open availability receives no less than 32 hrs .\n",
      "['availability-21', 'with-19', 'everyone-18', 'receives-22', 'hrs-27', 'open-20', '32-26']\n",
      "everyone with open availability receives 32 hrs ('open availability', '32 hrs')\n",
      "\n",
      "_sent: mobile department of best buy receives 30 % bonus on sales at the end of the month .\n",
      "['buy-4', 'of-2', 'department-1', 'receives-5', 'bonus-8', 'best-3']\n",
      "department of best buy receives bonus ('best buy', 'bonus')\n",
      "\n",
      "_sent: full time receives dental , vision , medical .\n",
      "['time-1', 'receives-2', 'dental-3', 'vision-5', 'medical-7', 'full-0']\n",
      "full time receives dental vision medical ('full time', 'medical')\n",
      "\n",
      "_sent: \n",
      " for sick leave , a part time employee receives no pay ; you must have a doctor 's note to explain these days off as well .\n",
      "['time-7', 'employee-8', 'receives-9', 'have-15', 'note-19', 'part-6', 'doctor-17', \"'s-18\"]\n",
      "part time employee receives have doctor 's note ('part time', \"doctor 's note\")\n",
      "\n",
      "_sent: none for part - time and i 'm unaware if full time receives any benefits as its never been offer to me or anyof my co - workers that i know of .\n",
      "['time-12', 'receives-13', 'benefits-15', 'full-11']\n",
      "full time receives benefits ('full time', 'benefits')\n",
      "\n",
      "_sent: management also receives quarterly bonuses .\n",
      "['management-0', 'receives-2', 'bonuses-4', 'quarterly-3']\n",
      "management receives quarterly bonuses ('management', 'quarterly bonuses')\n",
      "\n",
      "_sent: the company will send you an email as soon as the store that you applied for receives your assessment test .\n",
      "['email-6', 'send-3', 'soon-8', 'as-9', 'store-11', 'applied-14', 'for-15', 'receives-16', 'test-19', 'assessment-18']\n",
      "send email soon as store applied for receives assessment test ('email', 'assessment test')\n",
      "\n",
      "_sent: only management receives benefits if they opt in .\n",
      "['management-1', 'receives-2', 'benefits-3']\n",
      "management receives benefits ('management', 'benefits')\n",
      "\n",
      "_sent:   but management receives no support from corporate , forcing them to try to find quick fixes to problems with no real thought given to the long term effect .\n",
      "['management-2', 'receives-3', 'forcing-9', 'try-12', 'find-14', 'to-17', 'problems-18', 'with-19', 'thought-22', 'given-23', 'to-24', 'effect-28', 'term-27', 'long-26']\n",
      "management receives forcing try find to problems with thought given to long term effect ('management', 'long')\n",
      "\n",
      "’re company receives way economy set\n",
      "products receives is has reviews order\n",
      "full time receives benefits\n",
      "students receives benefits\n",
      "organization receives background check\n",
      "petsmart receives results\n",
      "management receives sick days\n",
      "management receives results\n",
      "works for walmart receives health insurance\n",
      "everyone with open availability receives 32 hrs\n",
      "department of best buy receives bonus\n",
      "full time receives dental vision medical\n",
      "part time employee receives have doctor 's note\n",
      "full time receives benefits\n",
      "management receives quarterly bonuses\n",
      "send email soon as store applied for receives assessment test\n",
      "management receives benefits\n",
      "management receives forcing try find to problems with thought given to long term effect\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(_prompt_retrieve('receives')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_sent: they use mostly dress shoes or at least shoes that a professional would wear which does n't allow sandals , sneakers , or sports shoes of any kind .\n",
      "['professional-11', 'wear-13', 'allow-17', 'sandals-18', 'sneakers-20', 'shoes-24', 'sports-23', 'does-15', \"n't-16\"]\n",
      "professional wear does n't allow sandals sneakers sports shoes ('professional', 'sports')\n",
      "\n",
      "_sent: only if you 're a full time employee , which target does n't allow team members to come close to 40 hours .\n",
      "['target-10', 'allow-13', 'come-17', 'members-15', 'does-11', \"n't-12\", 'team-14']\n",
      "target does n't allow team members come ('target', 'team members')\n",
      "\n",
      "_sent: union does n't allow employees to be drug tested unless you have an accident .\n",
      "['union-0', 'allow-3', 'be-6', 'drug-7', 'does-1', \"n't-2\"]\n",
      "union does n't allow be drug ('union', 'drug')\n",
      "\n",
      "_sent: it might be the opposite because i do know that the company does n't allow hourly associates to have overtime but i 'm not sure how that works at a dc .\n",
      "['company-11', 'allow-14', 'have-18', 'associates-16', 'does-12', \"n't-13\", 'hourly-15']\n",
      "company does n't allow hourly associates have ('company', 'hourly associates')\n",
      "\n",
      "_sent: union does n't allow employees to be screened after the pre employment screening .\n",
      "['union-0', 'allow-3', 'screened-7', 'after-8', 'screening-12', 'employment-11', 'does-1', \"n't-2\", 'pre-10']\n",
      "union does n't allow screened after pre employment screening ('union', 'pre employment')\n",
      "\n",
      "_sent: stand ... management does n't allow seats unless you have an injury and doctor note for reasonable accommodation and they still give you a hard time about that .. you have to fight for decent human rights\n",
      "['management-2', 'allow-5', 'have-9', 'note-14', 'for-15', 'accommodation-17', 'does-3', \"n't-4\", 'reasonable-16']\n",
      "management does n't allow have note for reasonable accommodation ('management', 'reasonable accommodation')\n",
      "\n",
      "professional wear does n't allow sandals sneakers sports shoes\n",
      "target does n't allow team members come\n",
      "union does n't allow be drug\n",
      "company does n't allow hourly associates have\n",
      "union does n't allow screened after pre employment screening\n",
      "management does n't allow have note for reasonable accommodation\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(_prompt_retrieve('does n\\'t allow')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Prompt Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit here: /meg-kb/src/analysis/lm_probing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggest Quality Prompts"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_transformers_nikita)",
   "language": "python",
   "name": "conda_transformers_nikita"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
