{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/mnt/efs/shared/meg_shared_scripts/meg-kb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "from compute_concept_clusters import load_embeddings, knn\n",
    "\n",
    "from compute_keyphrase_embeddings import get_masked_contexts, ensure_tensor_on_device, mean_pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: text corpus\n",
    "# step 1: extract key phrases (autophrase)\n",
    "# step 2: generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/keyword_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n"
     ]
    }
   ],
   "source": [
    "#change to keyword extractor directory\n",
    "%cd $base_dir/src/keyword_extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./corpusProcess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the dataset and thread no\n",
    "data_ac = 'indeeda-meg-ac'\n",
    "data_pt = 'indeeda-meg-pt'\n",
    "thread = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction\n",
      "\u001b[32m===Corpus Name: sample-indeeda-meg-ac===\u001b[m\n",
      "\u001b[32m===Current Path: /mnt/efs/shared/meg_shared_scripts/meg-kb/src/keyword_extraction===\u001b[m\n",
      "\u001b[32m===Cleaning input corpus===\u001b[m\n",
      "\u001b[32m===Running AutoPhrase===\u001b[m\n",
      "make: Nothing to be done for 'all'.\n",
      "\u001b[32m===RAW_TRAIN: ../../../data/sample-indeeda-meg-ac/source/corpus.clean.txt===\u001b[m\n",
      "auto_phrase.sh parameters: sample-indeeda-meg-ac ../../../data/sample-indeeda-meg-ac/source/corpus.clean.txt 10 data/EN/wiki_quality.txt 8\n",
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m0.702s\n",
      "user\t0m1.668s\n",
      "sys\t0m0.100s\n",
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n",
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Disabled\n",
      "Discard Ratio = 0.050000\n",
      "Number of threads = 8\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 6553\n",
      "max word token id = 1450\n",
      "# of documents = 500\n",
      "# of distinct POS tags = 0\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 1451\n",
      "# of frequent phrases = 1483\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 192\n",
      "\tThe size of the negative pool = 1282\n",
      "# truth patterns = 1202\n",
      "Estimating Phrase Quality...\n",
      "0 32\n",
      "[ERROR] not enough training data found!\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "0 32\n",
      "[ERROR] not enough training data found!\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m1.922s\n",
      "user\t0m2.496s\n",
      "sys\t0m0.016s\n",
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "phrasal_segmentation.sh parameters: sample-indeeda-meg-ac ../../../data/sample-indeeda-meg-ac/source/corpus.clean.txt 0.5 0.9 8\n",
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m0.568s\n",
      "user\t0m1.396s\n",
      "sys\t0m0.108s\n",
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n",
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/sample-indeeda-meg-ac/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.500000\n",
      "\tQ(single-word phrases) >= 0.900000\n",
      "=======\n",
      "Length penalty model loaded.\n",
      "\tpenalty = 199.805\n",
      "# of loaded patterns = 136\n",
      "# of loaded truth patterns = 1394\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 715\n",
      "   # of total processed sentences = 828\n",
      "   avg highlights per sentence = 0.863527\n",
      "\n",
      "real\t0m0.050s\n",
      "user\t0m0.016s\n",
      "sys\t0m0.000s\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Generating Phrase Text===\u001b[m\n",
      "process_segmentation.py parameters: ../../../data/sample-indeeda-meg-ac/intermediate/ 0.5 0.9\n",
      "11.152\n",
      "\u001b[32m===Running NLP Feature Extraction===\u001b[m\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 66.53it/s]\n",
      " 71%|██████████████████████████████▌            | 47/66 [00:00<00:00, 58.53it/s]Finish NLP processing, using time 0.8209497928619385 (second)\n",
      "100%|███████████████████████████████████████████| 57/57 [00:00<00:00, 65.72it/s]\n",
      " 80%|██████████████████████████████████▌        | 53/66 [00:00<00:00, 57.33it/s]Finish NLP processing, using time 0.9230477809906006 (second)\n",
      "100%|███████████████████████████████████████████| 62/62 [00:00<00:00, 70.06it/s]\n",
      "100%|███████████████████████████████████████████| 46/46 [00:00<00:00, 57.63it/s]\n",
      "Finish NLP processing, using time 0.9408359527587891 (second)\n",
      " 95%|████████████████████████████████████████▉  | 60/63 [00:00<00:00, 74.56it/s]Finish NLP processing, using time 0.8718316555023193 (second)\n",
      "100%|███████████████████████████████████████████| 66/66 [00:01<00:00, 62.34it/s]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:00<00:00, 68.87it/s]\n",
      "Finish NLP processing, using time 1.1157433986663818 (second)\n",
      "100%|███████████████████████████████████████████| 76/76 [00:00<00:00, 80.52it/s]\n",
      "Finish NLP processing, using time 0.9821081161499023 (second)\n",
      "Finish NLP processing, using time 1.003368854522705 (second)\n",
      "100%|███████████████████████████████████████████| 79/79 [00:01<00:00, 76.95it/s]\n",
      "Finish NLP processing, using time 1.0861637592315674 (second)\n",
      "\u001b[32m===Clean unnecessary files===\u001b[m\n",
      "\u001b[32m===Key Term Extraction===\u001b[m\n",
      "Extract Key Terms from Corpus: 100%|███████| 694/694 [00:00<00:00, 24407.16it/s]\n",
      "\u001b[32m===Sentence-wise Entity Segmentation===\u001b[m\n",
      "loading corpus for word2vec training: 100%|█| 694/694 [00:00<00:00, 288117.09it/\n",
      "100%|███████████████████████████████████████| 94/94 [00:00<00:00, 431834.15it/s]\n",
      "100%|█████████████████████████████████████| 122/122 [00:00<00:00, 423947.88it/s]\n",
      "100%|█████████████████████████████████████| 118/118 [00:00<00:00, 377519.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# process corpus and generate key prhases\n",
    "!./corpusProcess.sh $data_ac $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy these results to sample-meg-pt\n",
    "!cp -r ../../data/$data_ac ../../data/$data_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd $base_dir/src/concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|█████████████████████| 694/694 [00:00<00:00, 194471.34it/s]\n",
      "computing entity-wise embedding: 100%|████████| 177/177 [00:03<00:00, 50.59it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!python compute_keyphrase_embeddings.py -m bert-base-uncased -et ac -d ../../data/$data_ac/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|█████████████████████| 694/694 [00:00<00:00, 191566.11it/s]\n",
      "computing entity-wise embedding: 100%|████████| 177/177 [00:03<00:00, 53.88it/s]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "!python compute_keyphrase_embeddings.py -m bert-base-uncased -et pt -d ../../data/$data_pt/intermediate -c 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/tools/AutoPhrase\n"
     ]
    }
   ],
   "source": [
    "# change directory to autophrase\n",
    "%cd $base_dir/src/tools/AutoPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corel = 'sample-indeeda-corel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2021-06-18 00:36:18,384 : INFO : loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2021-06-18 00:36:18,776 : INFO : loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ubuntu/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2021-06-18 00:36:18,777 : INFO : Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2021-06-18 00:36:19,108 : INFO : loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "Traceback (most recent call last):\n",
      "  File \"extractBertEmbedding.py\", line 86, in <module>\n",
      "    with open(inputFilePath, \"r\") as fin:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../../../data/sample-indeeda-corel/intermediate//sent_segmentation.txt'\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extractBertEmbedding.py ../../../data/$data_corel/intermediate/ $thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add embeddings for seed instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seed_concepts(path):\n",
    "    df = pd.read_csv(path)    \n",
    "    df[\"seedInstances\"] = df[\"seedInstances\"].map(lambda s : eval(str(s)))\n",
    "    return df\n",
    "\n",
    "def load_seed_aligned_concepts(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df[\"generalizations\"] != \"x\"]\n",
    "    df[\"seedInstances\"] = df[\"seedInstances\"].map(lambda s : eval(str(s)))\n",
    "    return df\n",
    "\n",
    "def load_seed_aligned_relations(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df[\"range\"] != \"x\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_contexts_for_entities(entities, input_file):\n",
    "    \"\"\"Return a (list of) sentence(s) with entity replaced with MASK.\"\"\"\n",
    "    \"\"\"YS: input should be sentences.json\"\"\"\n",
    "    \n",
    "    ent_freq = {ent : 0 for ent in entities}\n",
    "    ent_context = {ent : [] for ent in entities}\n",
    "    \n",
    "    with open(input_file, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in tqdm(lines, total=len(lines), desc=\"loading corpus\"):\n",
    "            json_dict = json.loads(line)\n",
    "            sent = ' ' + ' '.join(json_dict['tokens']).lower() + ' '\n",
    "            #entities = [match.group(1) for match in re.finditer(pat, line)]\n",
    "            \n",
    "            for entity in entities:\n",
    "                pat = f' {entity} '\n",
    "                if pat not in sent:\n",
    "                    continue\n",
    "\n",
    "                context = sent.replace(pat, ' [MASK] ').strip()\n",
    "                c = context.split('[MASK]')\n",
    "                if len(c) != 2:  # sanity to not have too many repeating phrases in the context\n",
    "                    continue\n",
    "\n",
    "                # ignore too short contexts\n",
    "                if len(context) < 15:\n",
    "                    continue\n",
    "\n",
    "                # print(entity)\n",
    "                # print(context)\n",
    "                \n",
    "                _freq = ent_freq.get(entity, 0)\n",
    "                ent_freq[entity] = _freq + 1\n",
    "\n",
    "                context_lst = ent_context.get(entity, [])\n",
    "                context_lst.append(context)\n",
    "                ent_context[entity] = context_lst\n",
    "\n",
    "    dedup_context = {}\n",
    "    for e, v in ent_context.items():\n",
    "        dedup_context[e] = list(set(v))\n",
    "    return ent_freq, dedup_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_avg_context_embedding_for_entities(entities, model_path, input_file, max_context_ct):\n",
    "    '''\n",
    "    mean pooling from sentence-transformers\n",
    "    :param entity: List[str], the entities to compute embeddings for\n",
    "    :param model_path:\n",
    "    :param input_file:\n",
    "    :return:\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "    ent_freq, ent_context = get_masked_contexts_for_entities(entities, input_file)\n",
    "    \n",
    "    entity_embeddings = {}\n",
    "    for entity, en_context_lst in tqdm(ent_context.items(), total=len(ent_context), desc=\"computing entity-wise embedding\"):\n",
    "#     for entity, en_context_lst in ent_context.items():\n",
    "        print(entity)\n",
    "        en_context_lst = random.sample(en_context_lst, min(len(en_context_lst), max_context_ct))\n",
    "        chunks = [en_context_lst[i:i + 100] for i in range(0, len(en_context_lst), 100)]\n",
    "        # print(entity)\n",
    "        # print(len(en_context_lst))\n",
    "        all_context_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            encoded_input = tokenizer.batch_encode_plus(chunk, return_token_type_ids=True, add_special_tokens=True, max_length=128, return_tensors='pt', padding=True, pad_to_max_length=True, truncation=True)\n",
    "            mask = encoded_input['input_ids'] != mask_token_id\n",
    "            with torch.no_grad():\n",
    "                encoded_input = ensure_tensor_on_device(device, **encoded_input)\n",
    "                model_output = model(**encoded_input)  # Compute token embeddings\n",
    "            context_embeddings = mean_pooling(model_output, mask)  # mean pooling\n",
    "            all_context_embeddings.append(context_embeddings)\n",
    "            \n",
    "        assert len(all_context_embeddings) > 0\n",
    "            \n",
    "        entity_embedding = torch.mean(torch.cat(all_context_embeddings, dim=0), dim=0).cpu().detach().numpy().tolist()\n",
    "        entity_embeddings[entity] = entity_embedding\n",
    "    \n",
    "    return entity_embeddings, ent_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences.json')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "\n",
    "orig_bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed.txt')\n",
    "orig_bert_emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum.txt')\n",
    "\n",
    "new_bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "new_bert_emb_num_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembednum+seeds.txt')\n",
    "\n",
    "orig_emb_df = load_embeddings(bert_emb_path, 768)\n",
    "emb_dict = dict(zip(orig_emb_df['entity'].to_list(), orig_emb_df['embedding'].to_list()))\n",
    "\n",
    "with open(orig_bert_emb_num_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    emb_freq_dict = dict([l.strip().rsplit(' ', 1) for l in lines])\n",
    "\n",
    "concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_instances_list = [inst for _, (_a_con, _u_con, _gnrl, _seed_instances) in concepts_df.iterrows()\n",
    "                           for inst in _seed_instances]\n",
    "\n",
    "## debug\n",
    "seed_instances_list = seed_instances_list[::10]\n",
    "\n",
    "print(seed_instances_list)\n",
    "\n",
    "entity_embeddings, ent_freq = \\\n",
    "    get_avg_context_embedding_for_entities(entities=seed_instances_list, \n",
    "                                           model_path='bert-base-uncased',\n",
    "                                           input_file=corpus_path,\n",
    "                                           max_context_ct=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inst in seed_instances_list:\n",
    "    emb = entity_embeddings[inst]\n",
    "    freq = ent_freq[inst]\n",
    "    if inst in emb_dict:\n",
    "        print(f'Already exists: {inst}')\n",
    "#         assert np.allclose(emb_dict[inst], emb)\n",
    "#         assert emb_freq_dict[inst] == freq, f'{inst}: orig {emb_freq_dict[inst]} != new {freq}'\n",
    "#         print(f'Check passed: {inst}')\n",
    "    else:\n",
    "        emb_dict[inst] = emb\n",
    "        emb_freq_dict[inst] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_bert_emb_path, 'w') as f, open(new_bert_emb_num_path, 'w') as f2:\n",
    "    for inst in seed_instances_list:\n",
    "        emb = emb_dict[inst]\n",
    "        freq = ent_freq[inst]\n",
    "        f.write(\"{} {}\\n\".format(inst, ' '.join([str(x) for x in emb])))\n",
    "        f2.write(\"{} {}\\n\".format(inst, freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed instances: ['walmart', 'amazon', 'subway', 'microsoft', 'target', 'business casual', 'uniform', 'hair color', 'tattoos', 'facial hair', 'shoes', 'piercings', 'delivery driver', 'store manager', 'cashier', 'package handler', 'sales associate', 'barista', 'dishwasher', 'weekly', 'biweekly', 'friday', 'saturday', 'health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance', 'base pay', 'stock options', 'benefits', 'overtime pay', 'bonus', 'checks', 'direct deposit', 'prepaid card', 'drug test', 'criminal background check', 'employment verification', 'felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'misdemeanor', 'pregnant', 'students', 'seniors', 'hiring age', 'bachelors degree', 'prior experience', 'working permit', 'heavy lifting', 'night shift', 'dinner shift', 'early morning shift', '8 hour shift', 'christmas eve', 'early morning', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend', 'full time', 'part time', 'seasonal', 'orientation', 'introduction', 'workstation', 'training', 'team lunch']\n",
      "New instances: ['uniform', 'hair color', 'tattoos', 'shoes', 'cashier', 'weekly', 'biweekly', 'friday', 'saturday', '401k', 'stock options', 'benefits', 'overtime pay', 'bonus', 'checks', 'employment verification', 'felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'pregnant', 'students', 'seniors', '8 hour shift', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend', 'full time', 'part time', 'seasonal', 'orientation', 'introduction', 'training', 'team lunch']\n",
      "loading corpus: 100%|████████████████| 465226/465226 [00:09<00:00, 47556.15it/s]\n",
      "computing entity-wise embedding: 100%|██████████| 36/36 [00:41<00:00,  1.14s/it]\n",
      "Saving embedding\n"
     ]
    }
   ],
   "source": [
    "# Using script\n",
    "\n",
    "!python add_seed_instances_embeddings.py -m bert-base-uncased -et ac -d $base_dir/data/$data_ac/intermediate -b $base_dir/data/indeed-benchmark -c 750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8053"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub_dir = data_ac\n",
    "bert_emb_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/BERTembed+seeds.txt')\n",
    "\n",
    "embeddings = load_embeddings(bert_emb_path, 768)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8023</th>\n",
       "      <td>biweekly</td>\n",
       "      <td>[0.06975648552179337, -0.06970633566379547, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                                          embedding\n",
       "8023  biweekly  [0.06975648552179337, -0.06970633566379547, 0...."
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[embeddings['entity'] == 'biweekly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (X) Other ways of embeddings / clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|██████████| 458/458 [00:00<00:00, 73813.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175, 175)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/sent_segmentation.txt')\n",
    "ent_freq, dedup_context = get_masked_contexts(input_file_path)\n",
    "len(ent_freq), len(dedup_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " [\"we dropped by in hopes of finding atkinson 's peanut_butter bars ( we first tried them from honey salt 's [MASK] bowl ) and after searching a few minutes , we found it .\",\n",
       "  \"if you 're searching for a [MASK] or soda_pop you grew up with and can no longer find , there 's a good chance you 'll find it here .\"])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_freq['candy'], dedup_context['candy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_context_embeddings(model_path, input_file, max_context_ct):\n",
    "    '''\n",
    "    Adapted from get_avg_context_embeddings()\n",
    "    keep all context embeddings, using max similarity for knn\n",
    "    :param model_path:\n",
    "    :param input_file:\n",
    "    :return:\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "    ent_freq, ent_context = get_masked_contexts(input_file)\n",
    "    entity_embeddings = {}\n",
    "    for entity, en_context_lst in tqdm(ent_context.items(), total=len(ent_context), desc=\"computing entity-wise embedding\"):\n",
    "        en_context_lst = random.sample(en_context_lst, min(len(en_context_lst), max_context_ct))\n",
    "        chunks = [en_context_lst[i:i + 100] for i in range(0, len(en_context_lst), 100)]\n",
    "        # print(entity)\n",
    "        # print(len(en_context_lst))\n",
    "        all_context_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            encoded_input = tokenizer.batch_encode_plus(chunk, return_token_type_ids=True, add_special_tokens=True, max_length=128, return_tensors='pt', padding=True, pad_to_max_length=True, truncation=True)\n",
    "            mask = encoded_input['input_ids'] != mask_token_id\n",
    "            with torch.no_grad():\n",
    "                encoded_input = ensure_tensor_on_device(device, **encoded_input)\n",
    "                model_output = model(**encoded_input)  # Compute token embeddings\n",
    "            context_embeddings = mean_pooling(model_output, mask)  # mean pooling\n",
    "            # print(context_embeddings.size())\n",
    "            all_context_embeddings.append(context_embeddings)\n",
    "            \n",
    "        # entity_embedding = torch.mean(torch.cat(all_context_embeddings, dim=0), dim=0).cpu().detach().numpy().tolist()\n",
    "        # entity_embeddings[entity] = entity_embedding\n",
    "        entity_embeddings[entity] = torch.cat(all_context_embeddings, dim=0).cpu().detach().numpy().tolist()\n",
    "        \n",
    "    return entity_embeddings, ent_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading corpus: 100%|██████████| 458/458 [00:00<00:00, 150194.78it/s]\n",
      "computing entity-wise embedding: 100%|██████████| 175/175 [00:04<00:00, 41.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175, 175)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'bert-base-uncased'\n",
    "input_file_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/sent_segmentation.txt')\n",
    "max_context_ct = 10\n",
    "\n",
    "entity_embeddings, ent_freq = get_all_context_embeddings(model_path, input_file_path, max_context_ct)\n",
    "len(entity_embeddings), len(ent_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_embeddings['candy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _knn(entity_embeddings, embedding_dim, cluster_size, thread_ct=None, cluster_dest=None, **kwargs):\n",
    "    # entity_embeddings = load_embeddings(embed_src, embedding_dim)\n",
    "    \n",
    "    t = AnnoyIndex(embedding_dim, 'angular')\n",
    "    # entities = entity_embeddings['entity'].tolist()\n",
    "    entities = [f'{entity}-{_i}' for entity, embs in entity_embeddings.items() for _i in range(len(embs))]\n",
    "    # print(entities)\n",
    "    # for i, row in tqdm(entity_embeddings.iterrows(), total=entity_embeddings.shape[0], desc=\"building entity index\"):\n",
    "    #     t.add_item(i, row['embedding'])\n",
    "    i = 0\n",
    "    for entity, embs in tqdm(entity_embeddings.items(), total=len(entity_embeddings)):\n",
    "        for emb in embs:\n",
    "            t.add_item(i, emb)\n",
    "            i += 1\n",
    "    assert i == len(entities)\n",
    "    \n",
    "    t.build(100)\n",
    "    \n",
    "    neighbors = []\n",
    "    for i, entity in enumerate(tqdm(entities, desc=\"finding nearest neighbors by entity\")):\n",
    "        # print(i, entity)\n",
    "        nns, dists = t.get_nns_by_item(i, cluster_size + 1, include_distances=True)\n",
    "        cos_sim_scores = [(2 - d ** 2) / 2 for d in dists]  # convert angular distance to cosine similarity\n",
    "        zipped = list(zip(nns, cos_sim_scores))\n",
    "        sorted_nns = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "        if len(sorted_nns) > 0:\n",
    "            for nn_idx, d in sorted_nns:\n",
    "                neighbor_entity = entities[nn_idx]\n",
    "                if neighbor_entity == entity:\n",
    "                    continue\n",
    "                neighbors.append({\"entity\": entity, \"neighbor\": neighbor_entity, \"sim\": d})\n",
    "    c_df = pd.DataFrame(neighbors)\n",
    "    return c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:00<00:00, 24854.50it/s]\n",
      "finding nearest neighbors by entity: 100%|██████████| 269/269 [00:00<00:00, 6006.44it/s]\n"
     ]
    }
   ],
   "source": [
    "knn_results = _knn(entity_embeddings, 768, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'meat'\n",
    "\n",
    "df = knn_results\n",
    "\n",
    "n_embs = len(entity_embeddings[query])\n",
    "sub_frames = []\n",
    "for _i in range(n_embs):\n",
    "    ent_name = f'{query}-{_i}'\n",
    "    sub_frames.append(df[df['entity'] == ent_name])\n",
    "\n",
    "pd.concat(sub_frames).sort_values('sim', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original avg context knn \n",
    "knn_path = os.path.join(base_dir, f'data/{data_sub_dir}/intermediate/knn_100.csv')\n",
    "\n",
    "knn_results = pd.read_csv(knn_path)\n",
    "df = knn_results\n",
    "\n",
    "query = 'walmart'\n",
    "sub_frame = df[df['entity'] == query]\n",
    "sub_frame.sort_values('sim', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Seed Entities (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details here: https://github.com/rit-git/meg-kb/tree/main/src/concept_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/efs/shared/meg_shared_scripts/meg-kb/src/concept_learning\n"
     ]
    }
   ],
   "source": [
    "#change to concept learning directory\n",
    "%cd ../../concept_learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn sentence-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 100\n",
    "output = '../../data/'+data_ac+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 177/177 [00:00<00:00, 5435.26it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 177/177 [00:00<00:00, 2001.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_ac/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building entity index: 100%|████████████████| 177/177 [00:00<00:00, 3661.18it/s]\n",
      "finding nearest neighbors by entity: 100%|██| 177/177 [00:00<00:00, 4052.00it/s]\n"
     ]
    }
   ],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_pt/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20\n",
    "output = '../../data/'+data_pt+'/intermediate/knn_'+str(clusters)+'.csv'\n",
    "dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compute_concept_clusters.py -d ../../data/$data_corel/intermediate/ -ca knn -s $clusters -dim $dim -o $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit here: /meg_shared_scripts/meg-kb/src/analysis/concept_learning-test.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed instances clustering\n",
    "(using all seed instances of a concept to find neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_knn(embed_src, embedding_dim, seed_aligned_concept_src, cluster_size, thread_ct, cluster_dest, **kwargs):\n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concept_src)\n",
    "    \n",
    "    entity_embeddings = load_embeddings(embed_src, embedding_dim)\n",
    "    t = AnnoyIndex(embedding_dim, 'angular')\n",
    "    entities = entity_embeddings['entity'].tolist()\n",
    "    for i, row in tqdm(entity_embeddings.iterrows(), total=entity_embeddings.shape[0], desc=\"building entity index\"):\n",
    "        t.add_item(i, row['embedding'])\n",
    "    t.build(100)\n",
    "    \n",
    "    entity_emb_dict = dict(zip(entities, entity_embeddings['embedding'].tolist()))\n",
    "\n",
    "    neighbors = []\n",
    "    for i, (a_concept, u_concept, gnrl, seed_instances) in tqdm(seed_concepts_df.iterrows(), desc=\"finding nearest neighbors by concept\"):\n",
    "        embs = []\n",
    "        for inst in seed_instances:\n",
    "            try:\n",
    "                embs.append(entity_emb_dict[inst])\n",
    "            except KeyError:\n",
    "                print(f\"{inst} not found in entity_emb_dict??\")\n",
    "                continue\n",
    "        if len(embs) == 0:\n",
    "            continue\n",
    "        concept_emb = np.mean(embs, axis=0)\n",
    "        \n",
    "        nns, dists = t.get_nns_by_vector(concept_emb, cluster_size + 1, include_distances=True)\n",
    "        cos_sim_scores = [(2 - d ** 2) / 2 for d in dists]  # convert angular distance to cosine similarity\n",
    "        zipped = list(zip(nns, cos_sim_scores))\n",
    "        sorted_nns = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "        if len(sorted_nns) > 0:\n",
    "            for nn_idx, d in sorted_nns:\n",
    "                neighbor_entity = entities[nn_idx]\n",
    "                if neighbor_entity in seed_instances:\n",
    "                    continue\n",
    "                neighbors.append({\"concept\": a_concept, \"neighbor\": neighbor_entity, \"sim\": d})\n",
    "    c_df = pd.DataFrame(neighbors)\n",
    "    c_df.to_csv(cluster_dest, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b59a428fff4fbeb235fa3885d4ffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building entity index', max=8053.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c35b2790c454bb0942b2642a3b06859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='finding nearest neighbors by concept', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_size = 1000\n",
    "\n",
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_{cluster_size}.csv')\n",
    "\n",
    "get_concept_knn(embed_src=bert_emb_path,\n",
    "            embedding_dim=768,\n",
    "            seed_aligned_concept_src=seed_aligned_concepts_path,\n",
    "            cluster_size=1000,\n",
    "            thread_ct=1,\n",
    "            cluster_dest=concept_knn_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>company</td>\n",
       "      <td>wal mart</td>\n",
       "      <td>0.997038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>company</td>\n",
       "      <td>costco</td>\n",
       "      <td>0.997013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>company</td>\n",
       "      <td>publix</td>\n",
       "      <td>0.996753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>walgreens</td>\n",
       "      <td>0.996623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>company</td>\n",
       "      <td>kroger</td>\n",
       "      <td>0.996477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>company</td>\n",
       "      <td>home depot</td>\n",
       "      <td>0.996124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>company</td>\n",
       "      <td>sam 's club</td>\n",
       "      <td>0.995978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>company</td>\n",
       "      <td>dollar general</td>\n",
       "      <td>0.995846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>company</td>\n",
       "      <td>family dollar</td>\n",
       "      <td>0.995645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>company</td>\n",
       "      <td>jcpenney</td>\n",
       "      <td>0.995513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   concept        neighbor       sim\n",
       "0  company        wal mart  0.997038\n",
       "1  company          costco  0.997013\n",
       "2  company          publix  0.996753\n",
       "3  company       walgreens  0.996623\n",
       "4  company          kroger  0.996477\n",
       "5  company      home depot  0.996124\n",
       "6  company     sam 's club  0.995978\n",
       "7  company  dollar general  0.995846\n",
       "8  company   family dollar  0.995645\n",
       "9  company        jcpenney  0.995513"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'company'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>sunday</td>\n",
       "      <td>0.989753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>weekend</td>\n",
       "      <td>0.989047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>7 days</td>\n",
       "      <td>0.984987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>part time</td>\n",
       "      <td>0.984129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>8 hour shift</td>\n",
       "      <td>0.982685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>bonus</td>\n",
       "      <td>0.981976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>full time</td>\n",
       "      <td>0.981485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>seasonal</td>\n",
       "      <td>0.979928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>training</td>\n",
       "      <td>0.978852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>pay_schedule</td>\n",
       "      <td>orientation</td>\n",
       "      <td>0.978359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           concept      neighbor       sim\n",
       "2985  pay_schedule        sunday  0.989753\n",
       "2986  pay_schedule       weekend  0.989047\n",
       "2987  pay_schedule        7 days  0.984987\n",
       "2988  pay_schedule     part time  0.984129\n",
       "2989  pay_schedule  8 hour shift  0.982685\n",
       "2990  pay_schedule         bonus  0.981976\n",
       "2991  pay_schedule     full time  0.981485\n",
       "2992  pay_schedule      seasonal  0.979928\n",
       "2993  pay_schedule      training  0.978852\n",
       "2994  pay_schedule   orientation  0.978359"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(concept_knn_path)\n",
    "df[df['concept'] == 'pay_schedule'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity expansion evaluation\n",
    "Now using benchmark entities, mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'dress_code', 'job_position', 'pay_schedule', 'benefits', 'compensation', 'payment_option', 'background_screening', 'person', 'hire_prerequisite', 'shifts', 'schedule', 'employee_type', 'onboarding_steps']\n",
      "['has_pay_schedule', 'has_pay_schedule', 'has_dress_code', 'has_dress_code', 'has_background_screening', 'has_benefits', 'has_benefits', 'hires_person', 'has_compensation', 'has_compensation', 'has_hire_prerequisite', 'operates_on', 'hires_employee_type', 'has_onboarding_steps', 'has_shifts', 'has_shifts', 'has_job_position', 'has_hiring_policy', 'has_payment_option']\n",
      "{'background_screening', 'compensation', 'schedule', 'dress_code', 'benefits', 'job_position', 'payment_option', 'hire_prerequisite', 'employee_type', 'shifts', 'pay_schedule', 'onboarding_steps', 'person', 'company'}\n",
      "(706, 16)\n"
     ]
    }
   ],
   "source": [
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')\n",
    "benchmark_path = os.path.join(base_dir, f'data/indeed-benchmark/benchmark.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_1000.csv')\n",
    "\n",
    "seed_aligned_concepts = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "seed_aligned_relations = load_seed_aligned_relations(seed_aligned_relations_path)\n",
    "benchmark = pd.read_csv(benchmark_path)\n",
    "concept_knn = pd.read_csv(concept_knn_path)\n",
    "\n",
    "print(seed_aligned_concepts['alignedCategoryName'].tolist())\n",
    "print(seed_aligned_relations['alignedRelationName'].tolist())\n",
    "print(set(concept_knn['concept'].tolist()))\n",
    "print(benchmark.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept: company / company\n",
      "seeds: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\n",
      "{\n",
      "    \"chipotle\": 44,\n",
      "    \"olive garden\": 28,\n",
      "    \"planet fitness\": 52,\n",
      "    \"chilis\": 88,\n",
      "    \"dunkin donuts\": 43,\n",
      "    \"subways\": NaN,\n",
      "    \"lowes\": NaN,\n",
      "    \"mcdonalds\": 24,\n",
      "    \"taco bell\": 25,\n",
      "    \"dollar tree\": 15,\n",
      "    \"kroger\": 5,\n",
      "    \"primark\": 121,\n",
      "    \"amazon.com\": NaN,\n",
      "    \"pizza hut\": 11,\n",
      "    \"bonus\": NaN,\n",
      "    \"safeway\": 17,\n",
      "    \"subway\": -1,\n",
      "    \"walgreens\": 4,\n",
      "    \"burger king\": 32,\n",
      "    \"hobby lobby\": 18,\n",
      "    \"training\": NaN,\n",
      "    \"frito lay\": 42,\n",
      "    \"walmart\": -1,\n",
      "    \"tj maxx\": 45,\n",
      "    \"frito\": NaN,\n",
      "    \"company\": 54,\n",
      "    \"starbucks\": 12,\n",
      "    \"petsmart\": 33,\n",
      "    \"extensive background check\": NaN,\n",
      "    \"electric\": NaN,\n",
      "    \"cvs\": 29,\n",
      "    \"home depot\": 6,\n",
      "    \"g4s\": 48,\n",
      "    \"heb\": NaN,\n",
      "    \"instacart\": NaN,\n",
      "    \"ihop\": 39,\n",
      "    \"costco\": 2,\n",
      "    \"marshalls\": 26,\n",
      "    \"pepsi\": 35,\n",
      "    \"pepsico\": 30,\n",
      "    \"extensive background checks\": 606,\n",
      "    \"dd\": 61,\n",
      "    \"target\": -1,\n",
      "    \"tim hortons\": 97,\n",
      "    \"amazon\": -1,\n",
      "    \"fedex\": 40,\n",
      "    \"ups\": NaN,\n",
      "    \"panera\": NaN,\n",
      "    \"family dollar\": 9,\n",
      "    \"whataburger\": NaN,\n",
      "    \"dollar general\": 8,\n",
      "    \"mcdonald\": 55,\n",
      "    \"wendys\": NaN\n",
      "}\n",
      "MRR: 0.046719393430748074\n",
      "\n",
      "Concept: dress_code / dress code\n",
      "seeds: ['business casual', 'uniform', 'hair color', 'tattoos', 'facial hair', 'shoes', 'piercings']\n",
      "{\n",
      "    \"hats\": NaN,\n",
      "    \"strict dress code\": 14,\n",
      "    \"tattoos\": -1,\n",
      "    \"scrubs\": 64,\n",
      "    \"uniform shirts\": 129,\n",
      "    \"polo shirts\": NaN,\n",
      "    \"center\": 719,\n",
      "    \"acrylic nails\": 5,\n",
      "    \"color hair\": 6,\n",
      "    \"colorful hair\": 7,\n",
      "    \"jewelry\": NaN,\n",
      "    \"brown pants\": NaN,\n",
      "    \"hair colors\": NaN,\n",
      "    \"resistant shoes\": NaN,\n",
      "    \"piercings\": -1,\n",
      "    \"jeans\": NaN,\n",
      "    \"unnatural hair colors\": NaN,\n",
      "    \"facial hair\": -1,\n",
      "    \"uniform\": -1,\n",
      "    \"mustaches\": NaN,\n",
      "    \"hair net\": NaN,\n",
      "    \"wear shorts\": NaN,\n",
      "    \"black slacks\": NaN,\n",
      "    \"skirts\": NaN,\n",
      "    \"hairnets\": NaN,\n",
      "    \"face tattoos\": 40,\n",
      "    \"shirts\": NaN,\n",
      "    \"unnatural hair color\": NaN,\n",
      "    \"hat\": NaN,\n",
      "    \"business casual\": -1,\n",
      "    \"attire\": NaN,\n",
      "    \"uniforms\": NaN,\n",
      "    \"steel toe\": 152,\n",
      "    \"pants\": NaN,\n",
      "    \"professional\": 18,\n",
      "    \"unnatural colored hair\": 182,\n",
      "    \"red shirts\": NaN,\n",
      "    \"facial piercings\": 10,\n",
      "    \"uniform policy\": 28,\n",
      "    \"shorts\": NaN,\n",
      "    \"shirt\": NaN,\n",
      "    \"casual dress code\": NaN,\n",
      "    \"non slip shoes\": NaN,\n",
      "    \"wear fake nails\": NaN,\n",
      "    \"dyed hair\": 45,\n",
      "    \"natural colors\": 44,\n",
      "    \"lab coats\": 242,\n",
      "    \"t shirt\": NaN,\n",
      "    \"natural colored hair\": 57,\n",
      "    \"dress pants\": NaN,\n",
      "    \"tag\": NaN,\n",
      "    \"casual\": NaN,\n",
      "    \"blue collar\": NaN,\n",
      "    \"fake nails\": 25,\n",
      "    \"hair color\": -1,\n",
      "    \"dress shirts\": NaN,\n",
      "    \"black pants\": NaN,\n",
      "    \"tags\": NaN,\n",
      "    \"ponytail\": NaN,\n",
      "    \"steel toed boots\": NaN,\n",
      "    \"nose rings\": 36,\n",
      "    \"dress code\": 1,\n",
      "    \"wear jeans\": NaN,\n",
      "    \"black jeans\": NaN,\n",
      "    \"dress codes\": 47,\n",
      "    \"vest\": NaN,\n",
      "    \"shoes\": -1\n",
      "}\n",
      "MRR: 0.033162389886330405\n",
      "\n",
      "Concept: job_position / job position\n",
      "seeds: ['delivery driver', 'store manager', 'cashier', 'package handler', 'sales associate', 'barista', 'dishwasher']\n",
      "{\n",
      "    \"manager\": 16,\n",
      "    \"servers\": NaN,\n",
      "    \"store manager\": -1,\n",
      "    \"cashiers\": NaN,\n",
      "    \"management\": 31,\n",
      "    \"walkers\": NaN,\n",
      "    \"truck drivers\": NaN,\n",
      "    \"assistant manager\": 4,\n",
      "    \"server\": NaN,\n",
      "    \"package handler\": -1,\n",
      "    \"company\": 89,\n",
      "    \"cashier\": -1,\n",
      "    \"associates\": NaN,\n",
      "    \"entry level positions\": NaN,\n",
      "    \"crew members\": 29,\n",
      "    \"delivery driver\": -1,\n",
      "    \"managers\": NaN,\n",
      "    \"job\": NaN,\n",
      "    \"shift leader\": 11,\n",
      "    \"contractor\": NaN,\n",
      "    \"district manager\": 85,\n",
      "    \"housekeeper\": NaN,\n",
      "    \"position\": NaN\n",
      "}\n",
      "MRR: 0.02595529342023381\n",
      "\n",
      "Concept: pay_schedule / pay period\n",
      "seeds: ['weekly', 'biweekly', 'friday', 'saturday']\n",
      "{\n",
      "    \"sundays\": NaN,\n",
      "    \"payed biweekly\": NaN,\n",
      "    \"friday\": -1,\n",
      "    \"paid bi weekly\": NaN,\n",
      "    \"paid biweekly\": NaN,\n",
      "    \"pay periods\": 57,\n",
      "    \"weekly\": -1,\n",
      "    \"week\": NaN,\n",
      "    \"pay period\": 47,\n",
      "    \"tuesdays\": NaN,\n",
      "    \"bi weekly\": 33,\n",
      "    \"fridays\": NaN,\n",
      "    \"weeks\": NaN,\n",
      "    \"pay\": NaN,\n",
      "    \"payday\": 62,\n",
      "    \"period\": 88,\n",
      "    \"biweekly\": -1,\n",
      "    \"paychecks\": NaN,\n",
      "    \"paydays\": NaN,\n",
      "    \"paid weekly\": 752,\n",
      "    \"tuesday\": NaN,\n",
      "    \"payment\": NaN\n",
      "}\n",
      "MRR: 0.00515504955539881\n",
      "\n",
      "Concept: benefits / benefits\n",
      "seeds: ['health insurance', 'flexible schedule', '401k', 'paid vacation', 'sick leave', 'vision insurance']\n",
      "{\n",
      "    \"discounts\": NaN,\n",
      "    \"sick days\": 8,\n",
      "    \"prescription drugs\": 538,\n",
      "    \"401 k\": 59,\n",
      "    \"schooling\": NaN,\n",
      "    \"vacations\": NaN,\n",
      "    \"vacation\": NaN,\n",
      "    \"health benefits\": NaN,\n",
      "    \"retirement\": NaN,\n",
      "    \"health coverage\": NaN,\n",
      "    \"bonus\": NaN,\n",
      "    \"free lunch\": NaN,\n",
      "    \"relocate\": NaN,\n",
      "    \"discount\": NaN,\n",
      "    \"sick leave\": -1,\n",
      "    \"stock option\": NaN,\n",
      "    \"stock options\": NaN,\n",
      "    \"health care\": 4,\n",
      "    \"tuition reimbursement\": 15,\n",
      "    \"401k plan\": 12,\n",
      "    \"healthcare\": 2,\n",
      "    \"paid vacations\": 7,\n",
      "    \"great benefits\": 22,\n",
      "    \"pto\": NaN,\n",
      "    \"breakfast\": NaN,\n",
      "    \"monthly bonus\": 456,\n",
      "    \"tuition\": NaN,\n",
      "    \"health\": 5,\n",
      "    \"tuition assistance\": 26,\n",
      "    \"bonuses\": NaN,\n",
      "    \"401k\": -1,\n",
      "    \"retirement plan\": 21,\n",
      "    \"relocation\": NaN,\n",
      "    \"tips\": NaN,\n",
      "    \"benefits\": NaN,\n",
      "    \"pension\": 11,\n",
      "    \"life insurance\": 9,\n",
      "    \"health insurance\": -1,\n",
      "    \"health plans\": NaN\n",
      "}\n",
      "MRR: 0.047844815207513026\n",
      "\n",
      "Concept: compensation / compensation\n",
      "seeds: ['base pay', 'stock options', 'benefits', 'overtime pay', 'bonus']\n",
      "{\n",
      "    \"compensation\": NaN,\n",
      "    \"benefits\": -1,\n",
      "    \"benfits\": NaN\n",
      "}\n",
      "MRR: 0.0\n",
      "\n",
      "Concept: payment_option / nan\n",
      "seeds: ['checks', 'direct deposit', 'prepaid card']\n",
      "{\n",
      "    \"checks\": -1,\n",
      "    \"paper checks\": NaN,\n",
      "    \"prepaid card\": -1,\n",
      "    \"check\": NaN,\n",
      "    \"direct deposit\": -1,\n",
      "    \"direct deposits\": 9,\n",
      "    \"paycheck\": 8\n",
      "}\n",
      "MRR: 0.059027777777777776\n",
      "\n",
      "Concept: background_screening / background screening\n",
      "seeds: ['drug test', 'criminal background check', 'employment verification']\n",
      "{\n",
      "    \"drug screened\": NaN,\n",
      "    \"drug text\": NaN,\n",
      "    \"social media\": 85,\n",
      "    \"drug\": 45,\n",
      "    \"criminal background checks\": NaN,\n",
      "    \"previous employment\": 36,\n",
      "    \"previous jobs\": 114,\n",
      "    \"pre employment drug screening\": NaN,\n",
      "    \"screen\": NaN,\n",
      "    \"swab test\": 21,\n",
      "    \"backround\": NaN,\n",
      "    \"backround checks\": 322,\n",
      "    \"criminal records\": 33,\n",
      "    \"drug testing\": NaN,\n",
      "    \"drug test\": -1,\n",
      "    \"background check\": 2,\n",
      "    \"previous employer\": 360,\n",
      "    \"screening process\": 103,\n",
      "    \"previously worked\": NaN,\n",
      "    \"background checks\": 19,\n",
      "    \"mouth\": NaN,\n",
      "    \"alcohol\": 411,\n",
      "    \"drug tested\": NaN,\n",
      "    \"drugged tested\": NaN,\n",
      "    \"hair follicle test\": 66,\n",
      "    \"social security number\": 72,\n",
      "    \"credit report\": 35,\n",
      "    \"screening\": NaN,\n",
      "    \"swab\": NaN,\n",
      "    \"backround check\": 16,\n",
      "    \"urinalysis\": 10,\n",
      "    \"urine sample\": NaN,\n",
      "    \"drug tests\": 18,\n",
      "    \"criminal background check\": -1,\n",
      "    \"background report\": 481,\n",
      "    \"mouth swap\": NaN,\n",
      "    \"mouth swab\": 824,\n",
      "    \"hair sample\": NaN,\n",
      "    \"saliva test\": 47,\n",
      "    \"drug screening\": NaN,\n",
      "    \"drug screen\": NaN,\n",
      "    \"test\": 9,\n",
      "    \"driving record\": 13,\n",
      "    \"random drug testing\": 15,\n",
      "    \"criminal background\": 6,\n",
      "    \"urine test\": 5,\n",
      "    \"urine drug test\": 11,\n",
      "    \"cannabis\": 326,\n",
      "    \"random drug test\": 25,\n",
      "    \"backgrounds\": NaN,\n",
      "    \"credit score\": 30,\n",
      "    \"criminal record\": 29,\n",
      "    \"credit check\": 4,\n",
      "    \"blood test\": 595,\n",
      "    \"previous employers\": 323,\n",
      "    \"urine testing\": NaN,\n",
      "    \"dui\": 88,\n",
      "    \"cheek swab\": 91,\n",
      "    \"testing\": NaN,\n",
      "    \"mouth swabs\": NaN,\n",
      "    \"drug screens\": NaN,\n",
      "    \"random drug tests\": 28,\n",
      "    \"random tests\": 56,\n",
      "    \"saliva\": NaN,\n",
      "    \"follicle test\": NaN,\n",
      "    \"urine drug screen\": NaN,\n",
      "    \"urine tests\": 48,\n",
      "    \"saliva drug test\": 208,\n",
      "    \"criminal backgrounds\": 328,\n",
      "    \"urine\": 138,\n",
      "    \"criminal history\": 3,\n",
      "    \"credit checks\": 79,\n",
      "    \"drugs\": 84,\n",
      "    \"credit history\": 212,\n",
      "    \"social security\": 185\n",
      "}\n",
      "MRR: 0.03530366947138679\n",
      "\n",
      "Concept: person / nan\n",
      "seeds: ['felons', 'criminals', 'disabled', 'drug addicts', 'high schoolers', 'misdemeanor', 'pregnant', 'students', 'seniors']\n",
      "{\n",
      "    \"seniors\": -1,\n",
      "    \"ex felons\": 191,\n",
      "    \"school students\": NaN,\n",
      "    \"felony record\": NaN,\n",
      "    \"pregnant women\": 75,\n",
      "    \"high school graduate\": NaN,\n",
      "    \"senior citizens\": 25,\n",
      "    \"disabled\": -1,\n",
      "    \"drug addicts\": -1,\n",
      "    \"sex offenders\": 869,\n",
      "    \"felony\": 642,\n",
      "    \"misdemeanor charges\": NaN,\n",
      "    \"misdemeanor theft\": NaN,\n",
      "    \"disabilities\": NaN,\n",
      "    \"schoolers\": NaN,\n",
      "    \"felonies\": 289,\n",
      "    \"seniority\": NaN,\n",
      "    \"misdemeanor\": -1,\n",
      "    \"criminals\": -1,\n",
      "    \"felonys\": NaN,\n",
      "    \"convicted felons\": 168,\n",
      "    \"high school\": 309,\n",
      "    \"felons\": -1,\n",
      "    \"high school students\": NaN,\n",
      "    \"high schoolers\": -1,\n",
      "    \"pregnant\": -1\n",
      "}\n",
      "MRR: 0.004107008351780519\n",
      "\n",
      "Concept: hire_prerequisite / qualification\n",
      "seeds: ['hiring age', 'bachelors degree', 'prior experience', 'working permit', 'heavy lifting']\n",
      "{\n",
      "    \"ged\": 537,\n",
      "    \"workers permit\": 775,\n",
      "    \"bachelor degree\": 106,\n",
      "    \"working permit\": -1,\n",
      "    \"degrees\": NaN,\n",
      "    \"high school education\": 6,\n",
      "    \"diploma\": 34,\n",
      "    \"gpa\": NaN,\n",
      "    \"high school diploma\": 182,\n",
      "    \"hs diploma\": NaN,\n",
      "    \"birth certificate\": NaN,\n",
      "    \"college degree\": 3\n",
      "}\n",
      "MRR: 0.04977206840346433\n",
      "\n",
      "Concept: shifts / work shift\n",
      "seeds: ['night shift', 'dinner shift', 'early morning shift', '8 hour shift']\n",
      "{\n",
      "    \"night shifts\": 9,\n",
      "    \"morning shifts\": 5,\n",
      "    \"morning shift\": 1,\n",
      "    \"weekend shift\": 37,\n",
      "    \"open 24 hours\": NaN,\n",
      "    \"opening shifts\": 39,\n",
      "    \"3rd shift\": 3,\n",
      "    \"12 hour shifts\": 18\n",
      "}\n",
      "MRR: 0.21908350658350656\n",
      "\n",
      "Concept: schedule / nan\n",
      "seeds: ['christmas eve', 'early morning', 'hoilday', '7 days', 'saturday', 'sunday', 'weekend']\n",
      "{\n",
      "    \"hoildays\": NaN,\n",
      "    \"saturday\": -1,\n",
      "    \"saturdays\": NaN,\n",
      "    \"early morning\": -1,\n",
      "    \"weekends\": NaN,\n",
      "    \"christmas eve\": -1,\n",
      "    \"sunday\": -1,\n",
      "    \"weekend\": -1,\n",
      "    \"open 7 days\": NaN,\n",
      "    \"fridays\": NaN,\n",
      "    \"federal holidays\": NaN\n",
      "}\n",
      "MRR: 0.0\n",
      "\n",
      "Concept: employee_type / nan\n",
      "seeds: ['full time', 'part time', 'seasonal']\n",
      "{\n",
      "    \"season\": 175,\n",
      "    \"seasonal employees\": 38,\n",
      "    \"fulltime\": NaN,\n",
      "    \"parttime\": NaN,\n",
      "    \"seasonal positions\": 156,\n",
      "    \"seasons\": NaN,\n",
      "    \"seasonal\": -1,\n",
      "    \"seasonals\": NaN,\n",
      "    \"ft\": NaN,\n",
      "    \"seasonal workers\": 53\n",
      "}\n",
      "MRR: 0.006367584014058691\n",
      "\n",
      "Concept: onboarding_steps / onboarding process steps\n",
      "seeds: ['orientation', 'introduction', 'workstation', 'training', 'team lunch']\n",
      "{\n",
      "    \"training classes\": 63,\n",
      "    \"orientation\": -1,\n",
      "    \"training program\": 46,\n",
      "    \"training\": -1\n",
      "}\n",
      "MRR: 0.01880607315389924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, d in seed_aligned_concepts.iterrows():\n",
    "    a_concept = d[\"alignedCategoryName\"]\n",
    "    u_concept = d[\"unalignedCategoryName\"]\n",
    "    seed_instances = d[\"seedInstances\"]\n",
    "\n",
    "    concept_knn_instances = concept_knn[concept_knn[\"concept\"] == a_concept][\"neighbor\"].to_list()\n",
    "    \n",
    "    _b_head_instances = benchmark[benchmark[\"n_head_category\"] == a_concept][\"n_head\"].to_list()\n",
    "    _b_tail_instances = benchmark[benchmark[\"n_tail_category\"] == a_concept][\"n_tail\"].to_list()\n",
    "    benchmark_instances = list(set(_b_head_instances + _b_tail_instances))\n",
    "    \n",
    "    print(f'Concept: {a_concept} / {u_concept}')\n",
    "    print(f'seeds: {seed_instances}')\n",
    "#     print(f'expanded (concept_knn_instances): {concept_knn_instances}')\n",
    "#     print(f'benchmark_instances: {benchmark_instances}')\n",
    "    b_inst_ranks = dict()\n",
    "    recip_ranks = []\n",
    "    for _inst in benchmark_instances:\n",
    "        if _inst in seed_instances:\n",
    "            b_inst_ranks[_inst] = -1\n",
    "        elif _inst in concept_knn_instances:\n",
    "            _rank = concept_knn_instances.index(_inst) + 1\n",
    "            b_inst_ranks[_inst] = _rank\n",
    "            recip_ranks.append(1.0 / _rank)\n",
    "        else:\n",
    "            b_inst_ranks[_inst] = float('nan')\n",
    "            recip_ranks.append(0.0)\n",
    "        \n",
    "    print(json.dumps(b_inst_ranks, indent=4))\n",
    "    print('MRR:', np.mean(recip_ranks))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction Baselines\n",
    "Currently only for has_dress_code. TODO: include all other relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Imported from lm_probing.ipynb \n",
    "# TODO: for scoring purpose, maybe better to use GPT-2\n",
    "\n",
    "class LMProbe(object):\n",
    "    def __init__(self, model_name='bert-base-uncased', use_gpu=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForMaskedLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.mask_token = self.tokenizer.mask_token\n",
    "\n",
    "    def fill_multi_mask(self, input_txt, topk=3):\n",
    "        if not (input_txt.startswith('[CLS]') and input_txt.endswith('[SEP]')):\n",
    "            raise Exception('Input string must start with [CLS] and end with [SEP]')\n",
    "        if not '[MASK]' in input_txt:\n",
    "            raise Exception('Input string must have at least one mask token')\n",
    "        tokenized_txt = self.tokenizer.tokenize(input_txt)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        mask_indices = [i for i, x in enumerate(tokenized_txt) if x == \"[MASK]\"]\n",
    "        segment_idx = tokens_tensor * 0\n",
    "        tokens_tensor = tokens_tensor.to(self.device)\n",
    "        segments_tensors = segment_idx.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        probs = torch.softmax(predictions, dim=-1)[0]\n",
    "        sorted_probs, sorted_idx = probs.sort(dim=-1, descending=True)\n",
    "        sorted_probs = sorted_probs.detach().cpu().numpy()\n",
    "        sorted_idx = sorted_idx.detach().cpu().numpy()\n",
    "\n",
    "        masked_cands = []\n",
    "        for k in range(topk):\n",
    "            predicted_indices = [sorted_idx[i, k].item() for i in mask_indices]\n",
    "            predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_indices)\n",
    "            predicted_probs = [sorted_probs[i, k].item() for i in mask_indices]\n",
    "            seq = []\n",
    "            for token_id, token, prob, masked_index in zip(predicted_indices, predicted_tokens, predicted_probs,\n",
    "                                                           mask_indices):\n",
    "                seq.append({\"token\": token_id, \"token_str\": token, \"prob\": prob, \"masked_pos\": masked_index})\n",
    "            masked_cands.append(seq)\n",
    "\n",
    "        return masked_cands\n",
    "    \n",
    "    def score_candidates(self, input_txt, cands):\n",
    "        # cands: List[List[str]], list of tokenized candidates \n",
    "        tokenized_txt = self.tokenizer.tokenize(input_txt)\n",
    "        \n",
    "        if tokenized_txt[0] != \"[CLS]\" or tokenized_txt[-1] != \"[SEP]\":\n",
    "            raise Exception(f'Input string must start with [CLS] and end with [SEP], got {input_txt}')\n",
    "        if \"[MASK]\" not in tokenized_txt:\n",
    "            raise Exception(f'Input string must have at least one mask token, got {input_txt}')\n",
    "        \n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        mask_indices = [i for i, x in enumerate(tokenized_txt) if x == \"[MASK]\"]\n",
    "        segment_idx = tokens_tensor * 0\n",
    "        tokens_tensor = tokens_tensor.to(self.device)\n",
    "        segments_tensors = segment_idx.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        probs = torch.softmax(predictions, dim=-1)[0]\n",
    "        # sorted_probs, sorted_idx = probs.sort(dim=-1, descending=True)\n",
    "        # sorted_probs = sorted_probs.detach().cpu().numpy()\n",
    "        # sorted_idx = sorted_idx.detach().cpu().numpy()\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "\n",
    "        cand_scores = []\n",
    "        for c in cands:\n",
    "            assert len(c) == len(mask_indices), f'cand {c}; len(mask_indices) = {len(mask_indices)}'\n",
    "            \n",
    "            # predicted_indices = [sorted_idx[i, k].item() for i in mask_indices]\n",
    "            # predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_indices)\n",
    "            # predicted_probs = [sorted_probs[i, k].item() for i in mask_indices]\n",
    "            _scores = []\n",
    "            c_token_ids = self.tokenizer.convert_tokens_to_ids(c)\n",
    "            for i, token_id in zip(mask_indices, c_token_ids):\n",
    "                _scores.append(probs[i, token_id].item())\n",
    "            score = np.prod(_scores)\n",
    "            cand_scores.append({\"cand\": c, \"score\": score})\n",
    "\n",
    "        cand_scores.sort(key=lambda d : d[\"score\"], reverse=True)\n",
    "        return cand_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "lm_probe = LMProbe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Hand-designed. \n",
    "# TODO: mining \n",
    "# TODO: put to a file\n",
    "\n",
    "offers_job_position_templates = [\n",
    "    '{0} hires {1} .',\n",
    "    '{0} is hiring {1} .',\n",
    "    '{0} can hire you as a {1} .',\n",
    "#     'You can get a {1} job at {0} .',\n",
    "#     'Double check with the {1} at {0} .'\n",
    "]\n",
    "\n",
    "has_benefits_templates = [\n",
    "    '{0} offer {1} for their employees.',\n",
    "    '{0} provide {1} for employees.',\n",
    "    '{0} have {1} for their employees.',\n",
    "]\n",
    "\n",
    "has_pay_schedule_templates = [\n",
    "    '{0} pay their employees every {1}',\n",
    "    '{0} has a pay schedule of {1}',\n",
    "    '{0} employees get paid {1}',\n",
    "]\n",
    "\n",
    "has_dress_code_templates = [\n",
    "    '{0} don\\'t allow workers to wear {1}',\n",
    "    '{0} allow workers to wear {1}',\n",
    "    '{0} has a dress code of {1}',\n",
    "    '{0} require employees to wear {1}',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_direct_probing_candidates(templates,\n",
    "                                  lm_probe=None,\n",
    "                                  head_entity=None,\n",
    "                                  tail_entity=None,\n",
    "                                  context=None,\n",
    "                                  topk=10):\n",
    "    '''\n",
    "    Direct probing: let BERT propose possible entities  \n",
    "    :param templates: List[str]: each have 2 slots, {0} for head, {1} for tail \n",
    "    :return: Dict[str, float]: proposed entities and scores \n",
    "    '''\n",
    "    \n",
    "    # ensure given one and propose one \n",
    "    assert (head_entity is None) != (tail_entity is None), f'{head_entity} {tail_entity}'\n",
    "\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "    \n",
    "    names_scores = {}\n",
    "    for template in templates:\n",
    "        if head_entity is not None:\n",
    "            # head -> tail \n",
    "            _unigram_template = template.format(head_entity, '[MASK]')\n",
    "            _bigram_template = template.format(head_entity, '[MASK] [MASK]')\n",
    "        else:\n",
    "            # tail -> head \n",
    "            _unigram_template = template.format('[MASK]', tail_entity)\n",
    "            _bigram_template = template.format('[MASK] [MASK]', tail_entity)\n",
    "        \n",
    "        for _template in [_unigram_template, _bigram_template]:\n",
    "            if context:\n",
    "                query = '[CLS] ' + _template + '[SEP]' + context + '[SEP]'\n",
    "            else:\n",
    "                query = '[CLS] ' + _template + '[SEP]'\n",
    "            preds = lm_probe.fill_multi_mask(query, topk=topk)\n",
    "            for pred in preds:\n",
    "                name = ' '.join([p['token_str'] for p in pred])\n",
    "                name = name.replace(' ##', '')\n",
    "                score = np.prod([p['prob'] for p in pred])\n",
    "                scores = names_scores.get(name, [])\n",
    "                scores.append(score)\n",
    "                names_scores[name] = scores\n",
    "                \n",
    "    names_avg_scores = {k: float(sum(v))/ len(v) for k,v in names_scores.items()}\n",
    "    names_avg_scores = {k: v for k, v in sorted(names_avg_scores.items(), reverse=True, key=lambda item: item[1])[:topk]}\n",
    "    return names_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def direct_probing_RE(seed_concepts_path,\n",
    "                      seed_relations_path,\n",
    "                      knn_result_path,\n",
    "                      lm_probe=None,\n",
    "                      topk=10):\n",
    "    \n",
    "    seed_concepts_df = pd.read_csv(seed_concepts_path)\n",
    "    seed_relations_df = pd.read_csv(seed_relations_path)\n",
    "    seed_relations_df = seed_relations_df.iloc[1]  ## Only using offer_job_position for now \n",
    "    knn_results = pd.read_csv(knn_path)\n",
    "\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "    \n",
    "    head_type = seed_relations_df['domain']\n",
    "    tail_type = seed_relations_df['range']\n",
    "    print(head_type, '\\t', tail_type)\n",
    "    seed_heads = seed_concepts_df[seed_concepts_df['categoryName'] == head_type]['seedInstances']\n",
    "    seed_heads = eval(list(seed_heads)[0])\n",
    "    seed_tails = seed_concepts_df[seed_concepts_df['categoryName'] == tail_type]['seedInstances']\n",
    "    seed_tails = eval(list(seed_tails)[0])\n",
    "    \n",
    "    # print(seed_heads)\n",
    "    # print(seed_tails)\n",
    "    \n",
    "    extraction_results = []\n",
    "    \n",
    "    # head -> tail \n",
    "    for seed_head in seed_heads:\n",
    "        lm_cands = get_direct_probing_candidates(offers_job_position_templates,\n",
    "                              lm_probe=lm_probe,\n",
    "                              head_entity=seed_head,\n",
    "                              topk=topk)\n",
    "        neighbors = knn_results[knn_results['entity'] == seed_head]['neighbor']\n",
    "        \n",
    "        extracted_tails = list(set(lm_cands.keys()) & set(neighbors))\n",
    "        \n",
    "        print(f'seed_head: {seed_head}')\n",
    "        print(f'extr_tails: {extracted_tails}')\n",
    "        \n",
    "        for _e in extracted_tails:\n",
    "            extraction_results.append({'head': seed_head, 'tail': _e, 'new': 'TAIL'})\n",
    "        \n",
    "    # tail -> head \n",
    "    for seed_tail in seed_tails:\n",
    "        lm_cands = get_direct_probing_candidates(offers_job_position_templates,\n",
    "                              lm_probe=lm_probe,\n",
    "                              tail_entity=seed_tail,\n",
    "                              topk=topk)\n",
    "        neighbors = knn_results[knn_results['entity'] == seed_tail]['neighbor']\n",
    "        \n",
    "        extracted_heads = list(set(lm_cands.keys()) & set(neighbors))\n",
    "        \n",
    "        print(f'seed_tail: {seed_tail}')\n",
    "        print(f'extr_heads: {extracted_heads}')\n",
    "        \n",
    "        for _e in extracted_heads:\n",
    "            extraction_results.append({'head': _e, 'tail': seed_tail, 'new': 'HEAD'})\n",
    "    \n",
    "    return pd.DataFrame(extraction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def direct_probing_RE_v2(seed_concepts_path,\n",
    "                         seed_relations_path,\n",
    "                         knn_result_path,\n",
    "                         templates,\n",
    "                         lm_probe=None,\n",
    "                         topk=10):\n",
    "    \n",
    "    seed_concepts_df = pd.read_csv(seed_concepts_path)\n",
    "    seed_relations_df = pd.read_csv(seed_relations_path)\n",
    "    seed_relations_df = seed_relations_df.iloc[1]  ## Only using offer_job_position for now \n",
    "    knn_results = pd.read_csv(knn_path)\n",
    "\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "    \n",
    "    head_type = seed_relations_df['domain']\n",
    "    tail_type = seed_relations_df['range']\n",
    "    print(head_type, '\\t', tail_type)\n",
    "    seed_heads = seed_concepts_df[seed_concepts_df['categoryName'] == head_type]['seedInstances']\n",
    "    seed_heads = eval(list(seed_heads)[0])\n",
    "    seed_tails = seed_concepts_df[seed_concepts_df['categoryName'] == tail_type]['seedInstances']\n",
    "    seed_tails = eval(list(seed_tails)[0])\n",
    "    \n",
    "    # print(seed_heads)\n",
    "    # print(seed_tails)\n",
    "    \n",
    "    all_extraction_results = []\n",
    "    \n",
    "    # head -> tail \n",
    "    for seed_head in seed_heads:\n",
    "        print(f'seed_head: {seed_head}')\n",
    "        extraction_results = []\n",
    "        \n",
    "        _df = knn_results[knn_results['entity'] == seed_head]\n",
    "        cand_tails = set(_df['neighbor'])\n",
    "        \n",
    "        print(cand_tails)\n",
    "        \n",
    "        cand_bins = {1: [], 2: []}\n",
    "        for c in cand_tails:\n",
    "            c_tokenized = lm_probe.tokenizer.tokenize(c)\n",
    "            if len(c_tokenized) in [1, 2]:\n",
    "                cand_bins[len(c_tokenized)].append(c_tokenized)\n",
    "        \n",
    "        cand_scores_per_template = []\n",
    "        for template in templates:\n",
    "            _unigram_template = '[CLS] ' + template.format(seed_head, '[MASK]') + '[SEP]'\n",
    "            _bigram_template = '[CLS] ' + template.format(seed_head, '[MASK] [MASK]') + '[SEP]'\n",
    "\n",
    "            _cand_scores_1 = lm_probe.score_candidates(_unigram_template, cand_bins[1])\n",
    "            _cand_scores_2 = lm_probe.score_candidates(_bigram_template, cand_bins[2])\n",
    "            _cand_scores = sorted(_cand_scores_1 + _cand_scores_2, key=lambda d : d[\"cand\"])\n",
    "            # List[Dict[\"cand\", \"score\"]]\n",
    "            cand_scores_per_template.append(_cand_scores)\n",
    "    \n",
    "        cand_scores = []  # List[Dict[\"cand\", \"score\"]], for each \"cand\" the average score \n",
    "        for _cand_score_lst in zip(*cand_scores_per_template):\n",
    "            # _cand_score_lst: List[Dict[\"cand\", \"score\"]], for the same \"cand\" and different template \n",
    "            _cand = _cand_score_lst[0][\"cand\"]\n",
    "            assert all(d[\"cand\"] == _cand for d in _cand_score_lst), _cand_score_lst\n",
    "            _score = np.mean([d[\"score\"] for d in _cand_score_lst])\n",
    "            cand_scores.append({\"cand\": _cand, \"score\": _score})\n",
    "        cand_scores.sort(key = lambda d : d[\"score\"], reverse=True)\n",
    "    \n",
    "#         extracted_tails = [d[\"cand\"] for d in cand_scores[:topk]]\n",
    "#         print(f'seed_head: {seed_head}')\n",
    "#         print(f'extr_tails: {extracted_tails}')\n",
    "\n",
    "        for d in cand_scores[:topk]:\n",
    "            e_tail = ' '.join(d[\"cand\"]).replace(' ##', '')\n",
    "            if e_tail not in cand_tails:\n",
    "                continue\n",
    "            emb_score = _df[_df[\"neighbor\"] == e_tail][\"sim\"].item()\n",
    "            lm_score = d[\"score\"]\n",
    "            extraction_results.append({'head': seed_head, 'tail': e_tail, 'new': 'TAIL',\n",
    "                                       'emb_score': emb_score, 'lm_score': lm_score})\n",
    "        \n",
    "        all_extraction_results.extend(extraction_results)\n",
    "        \n",
    "    # tail -> head \n",
    "    for seed_tail in seed_tails:\n",
    "        print(f'seed_tail: {seed_tail}')\n",
    "        extraction_results = []\n",
    "        \n",
    "        _df = knn_results[knn_results['entity'] == seed_tail]\n",
    "        cand_heads = set(_df['neighbor'])\n",
    "        \n",
    "        print(cand_heads)\n",
    "        \n",
    "        cand_bins = {1: [], 2: []}\n",
    "        for c in cand_heads:\n",
    "            c_tokenized = lm_probe.tokenizer.tokenize(c)\n",
    "            if len(c_tokenized) in [1, 2]:\n",
    "                cand_bins[len(c_tokenized)].append(c_tokenized)\n",
    "        \n",
    "        cand_scores_per_template = []\n",
    "        for template in templates:\n",
    "            _unigram_template = '[CLS] ' + template.format('[MASK]', seed_tail) + '[SEP]'\n",
    "            _bigram_template = '[CLS] ' + template.format('[MASK] [MASK]', seed_tail) + '[SEP]'\n",
    "\n",
    "            _cand_scores_1 = lm_probe.score_candidates(_unigram_template, cand_bins[1])\n",
    "            _cand_scores_2 = lm_probe.score_candidates(_bigram_template, cand_bins[2])\n",
    "            _cand_scores = sorted(_cand_scores_1 + _cand_scores_2, key=lambda d : d[\"cand\"])\n",
    "            # List[Dict[\"cand\", \"score\"]]\n",
    "            cand_scores_per_template.append(_cand_scores)\n",
    "            \n",
    "        print(cand_scores_per_template)\n",
    "    \n",
    "        cand_scores = []  # List[Dict[\"cand\", \"score\"]], for each \"cand\" the average score \n",
    "        for _cand_score_lst in zip(*cand_scores_per_template):\n",
    "            # _cand_score_lst: List[Dict[\"cand\", \"score\"]], for the same \"cand\" and different template \n",
    "            _cand = _cand_score_lst[0][\"cand\"]\n",
    "            assert all(d[\"cand\"] == _cand for d in _cand_score_lst), _cand_score_lst\n",
    "            _score = np.mean([d[\"score\"] for d in _cand_score_lst])\n",
    "            cand_scores.append({\"cand\": _cand, \"score\": _score})\n",
    "        cand_scores.sort(key = lambda d : d[\"score\"], reverse=True)\n",
    "        \n",
    "        print(cand_scores)\n",
    "\n",
    "        for d in cand_scores[:topk]:\n",
    "            e_head = ' '.join(d[\"cand\"]).replace(' ##', '')\n",
    "            if e_head not in cand_heads:\n",
    "                continue\n",
    "            emb_score = _df[_df[\"neighbor\"] == e_head][\"sim\"].item()\n",
    "            lm_score = d[\"score\"]\n",
    "            extraction_results.append({'head': e_head, 'tail': sead_tail, 'new': 'HEAD',\n",
    "                                       'emb_score': emb_score, 'lm_score': lm_score})\n",
    "        \n",
    "        all_extraction_results.extend(extraction_results)\n",
    "        \n",
    "    return pd.DataFrame(all_extraction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def direct_probing_RE_v3(seed_aligned_concepts_path,\n",
    "                         seed_aligned_relations_path,\n",
    "                         emb_path,\n",
    "                         concept_knn_path,\n",
    "                         templates,\n",
    "                         lm_probe=None,\n",
    "                         emb_dim=768,\n",
    "                         scores_agg_func=None,\n",
    "                         topk=10,\n",
    "                         save_path=None):\n",
    "    '''\n",
    "    For each head / tail, rank candidate tails / heads by overall scores. \n",
    "    Current (default) overall score: 0.1 * ht_sim + 10 * concept_sim + log(lm_prob)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    seed_concepts_df = load_seed_aligned_concepts(seed_aligned_concepts_path)\n",
    "#     seed_relations_df = pd.read_csv(seed_relations_path)\n",
    "#     seed_relations_df = seed_relations_df.iloc[1]\n",
    "    entity_embeddings = load_embeddings(emb_path, emb_dim)\n",
    "    entity_emb_dict = dict(zip(entity_embeddings['entity'].tolist(),\n",
    "                               entity_embeddings['embedding'].tolist()))\n",
    "    concept_knn_results = pd.read_csv(concept_knn_path)\n",
    "\n",
    "    if lm_probe is None:\n",
    "        lm_probe = LMProbe()\n",
    "    if scores_agg_func is None:\n",
    "        scores_agg_func = lambda ht_sim, concept_sim, lm_prob : 0.1 * ht_sim + 10 * concept_sim + np.log10(lm_prob)\n",
    "    \n",
    "#     head_type = seed_relations_df['domain']\n",
    "#     tail_type = seed_relations_df['range']\n",
    "    ## Just for testing\n",
    "    head_type = \"company\"\n",
    "    tail_type = \"dress_code\"\n",
    "    print(head_type, '\\t', tail_type)\n",
    "    seed_heads = seed_concepts_df[seed_concepts_df['alignedCategoryName'] == head_type]['seedInstances'].item()\n",
    "#     seed_heads = eval(list(seed_heads)[0])\n",
    "    seed_tails = seed_concepts_df[seed_concepts_df['alignedCategoryName'] == tail_type]['seedInstances'].item()\n",
    "#     seed_tails = eval(list(seed_tails)[0])\n",
    "    print('seed_heads:', seed_heads)\n",
    "    print('seed_tails:', seed_tails)\n",
    "\n",
    "    # Candidate heads / tails from concept knn \n",
    "    cand_heads_df = concept_knn_results[concept_knn_results['concept'] == head_type]\n",
    "    cand_tails_df = concept_knn_results[concept_knn_results['concept'] == tail_type]\n",
    "    cand_heads_dict = dict(zip(cand_heads_df['neighbor'].tolist(), cand_heads_df['sim'].tolist()))\n",
    "    cand_tails_dict = dict(zip(cand_tails_df['neighbor'].tolist(), cand_tails_df['sim'].tolist()))\n",
    "    for h in seed_heads:\n",
    "        assert h not in cand_heads_dict\n",
    "        cand_heads_dict[h] = 1.0\n",
    "    for t in seed_tails:\n",
    "        assert t not in cand_tails_dict\n",
    "        cand_tails_dict[t] = 1.0\n",
    "    \n",
    "#     print(cand_heads_dict)\n",
    "#     print(cand_tails_dict)\n",
    "    \n",
    "    all_extraction_results = []\n",
    "    \n",
    "    # head -> tail \n",
    "    for seed_head in seed_heads:\n",
    "        print(f'seed_head: {seed_head}')\n",
    "        extraction_results = []\n",
    "\n",
    "        ## For each tail, extract concept sim, head sim, lm score, combine and report\n",
    "        \n",
    "        cand_bins = {1: [], 2: []} ## TODO: allow higher grams; switch to GPT-2 for fair probs \n",
    "        for c in cand_tails_dict.keys():\n",
    "            c_tokenized = lm_probe.tokenizer.tokenize(c)\n",
    "            if len(c_tokenized) in [1, 2]:\n",
    "                cand_bins[len(c_tokenized)].append(c_tokenized)\n",
    "        \n",
    "        cand_scores_per_template = []\n",
    "        for template in templates:\n",
    "            _unigram_template = '[CLS] ' + template.format(seed_head, '[MASK]') + '[SEP]'\n",
    "            _bigram_template = '[CLS] ' + template.format(seed_head, '[MASK] [MASK]') + '[SEP]'\n",
    "\n",
    "            _cand_scores_1 = lm_probe.score_candidates(_unigram_template, cand_bins[1])\n",
    "            _cand_scores_2 = lm_probe.score_candidates(_bigram_template, cand_bins[2])\n",
    "            _cand_scores = sorted(_cand_scores_1 + _cand_scores_2, key=lambda d : d[\"cand\"])\n",
    "            # List[Dict[\"cand\", \"score\"]]\n",
    "            cand_scores_per_template.append(_cand_scores)\n",
    "    \n",
    "        cand_scores = []  # List[Dict[\"cand\", \"score\"]], for each \"cand\" the average score \n",
    "        for _cand_score_lst in zip(*cand_scores_per_template):\n",
    "            # _cand_score_lst: List[Dict[\"cand\", \"score\"]], for the same \"cand\" and different template \n",
    "            _cand = _cand_score_lst[0][\"cand\"]\n",
    "            assert all(d[\"cand\"] == _cand for d in _cand_score_lst), _cand_score_lst\n",
    "            _score = np.mean([d[\"score\"] for d in _cand_score_lst])\n",
    "            cand_scores.append({\"cand\": _cand, \"score\": _score})\n",
    "#         cand_scores.sort(key = lambda d : d[\"score\"], reverse=True)\n",
    "\n",
    "        for d in cand_scores:\n",
    "            e_tail = ' '.join(d[\"cand\"]).replace(' ##', '')\n",
    "            if e_tail not in cand_tails_dict:\n",
    "                continue\n",
    "\n",
    "            lm_score = d[\"score\"]\n",
    "            try:\n",
    "                ht_sim_score = 1 - cosine(entity_emb_dict[seed_head], entity_emb_dict[e_tail])\n",
    "            except KeyError:\n",
    "                print(f'** embedding of {seed_head}: {(seed_head in entity_emb_dict)}')\n",
    "                print(f'** embedding of {e_tail}: {(e_tail in entity_emb_dict)}')\n",
    "                ht_sim_score = float(\"nan\")\n",
    "            concept_sim_score = cand_tails_dict[e_tail]\n",
    "            overall_score = scores_agg_func(ht_sim_score, concept_sim_score, lm_score)\n",
    "\n",
    "            extraction_results.append({'head': seed_head, 'tail': e_tail, 'base': 'HEAD',\n",
    "                                       'ht_sim_score': ht_sim_score,\n",
    "                                       'concept_sim_score': concept_sim_score,\n",
    "                                       'lm_score': lm_score,\n",
    "                                       'overall_score': overall_score})\n",
    "        \n",
    "        extraction_results.sort(key=lambda d : d['overall_score'], reverse=True)\n",
    "        all_extraction_results.extend(extraction_results[:topk])\n",
    "        \n",
    "    # tail -> head \n",
    "    for seed_tail in seed_tails:\n",
    "        print(f'seed_tail: {seed_tail}')\n",
    "        extraction_results = []\n",
    "        \n",
    "        ## For each tail, extract concept sim, head sim, lm score, combine and report\n",
    "        \n",
    "        cand_bins = {1: [], 2: []}\n",
    "        for c in cand_heads_dict.keys():\n",
    "            c_tokenized = lm_probe.tokenizer.tokenize(c)\n",
    "            if len(c_tokenized) in [1, 2]:\n",
    "                cand_bins[len(c_tokenized)].append(c_tokenized)\n",
    "        \n",
    "        cand_scores_per_template = []\n",
    "        for template in templates:\n",
    "            _unigram_template = '[CLS] ' + template.format('[MASK]', seed_tail) + '[SEP]'\n",
    "            _bigram_template = '[CLS] ' + template.format('[MASK] [MASK]', seed_tail) + '[SEP]'\n",
    "\n",
    "            _cand_scores_1 = lm_probe.score_candidates(_unigram_template, cand_bins[1])\n",
    "            _cand_scores_2 = lm_probe.score_candidates(_bigram_template, cand_bins[2])\n",
    "            _cand_scores = sorted(_cand_scores_1 + _cand_scores_2, key=lambda d : d[\"cand\"])\n",
    "            # List[Dict[\"cand\", \"score\"]]\n",
    "            cand_scores_per_template.append(_cand_scores)\n",
    "    \n",
    "        cand_scores = []  # List[Dict[\"cand\", \"score\"]], for each \"cand\" the average score \n",
    "        for _cand_score_lst in zip(*cand_scores_per_template):\n",
    "            # _cand_score_lst: List[Dict[\"cand\", \"score\"]], for the same \"cand\" and different template \n",
    "            _cand = _cand_score_lst[0][\"cand\"]\n",
    "            assert all(d[\"cand\"] == _cand for d in _cand_score_lst), _cand_score_lst\n",
    "            _score = np.mean([d[\"score\"] for d in _cand_score_lst])\n",
    "            cand_scores.append({\"cand\": _cand, \"score\": _score})\n",
    "#         cand_scores.sort(key = lambda d : d[\"score\"], reverse=True)\n",
    "\n",
    "        for d in cand_scores[:topk]:\n",
    "            e_head = ' '.join(d[\"cand\"]).replace(' ##', '')\n",
    "            if e_head not in cand_heads_dict:\n",
    "                continue\n",
    "                \n",
    "            lm_score = d[\"score\"]\n",
    "            try:\n",
    "                ht_sim_score = 1 - cosine(entity_emb_dict[e_head], entity_emb_dict[seed_tail])\n",
    "            except KeyError:\n",
    "                print(f'** embedding of {e_head}: {(e_head in entity_emb_dict)}')\n",
    "                print(f'** embedding of {seed_tail}: {(seed_tail in entity_emb_dict)}')\n",
    "                ht_sim_score = float(\"nan\")\n",
    "            concept_sim_score = cand_heads_dict[e_head]\n",
    "            overall_score = scores_agg_func(ht_sim_score, concept_sim_score, lm_score)\n",
    "        \n",
    "            extraction_results.append({'head': e_head, 'tail': seed_tail, 'base': 'TAIL',\n",
    "                                       'ht_sim_score': ht_sim_score,\n",
    "                                       'concept_sim_score': concept_sim_score,\n",
    "                                       'lm_score': lm_score,\n",
    "                                       'overall_score': overall_score})\n",
    "        \n",
    "        extraction_results.sort(key=lambda d : d['overall_score'], reverse=True)\n",
    "        all_extraction_results.extend(extraction_results[:topk])\n",
    "        \n",
    "    results_df = pd.DataFrame(all_extraction_results)\n",
    "    if save_path is not None:\n",
    "        results_df.to_csv(save_path, index=None)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company \t dress_code\n",
      "seed_heads: ['walmart', 'amazon', 'subway', 'microsoft', 'target']\n",
      "seed_tails: ['business casual', 'uniform', 'hair color', 'tattoos', 'facial hair', 'shoes', 'piercings']\n",
      "seed_head: walmart\n",
      "seed_head: amazon\n",
      "seed_head: subway\n",
      "seed_head: microsoft\n",
      "seed_head: target\n",
      "seed_tail: business casual\n",
      "seed_tail: uniform\n",
      "seed_tail: hair color\n",
      "seed_tail: tattoos\n",
      "seed_tail: facial hair\n",
      "seed_tail: shoes\n",
      "seed_tail: piercings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>base</th>\n",
       "      <th>ht_sim_score</th>\n",
       "      <th>concept_sim_score</th>\n",
       "      <th>lm_score</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walmart</td>\n",
       "      <td>shoes</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.882266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.872273e-07</td>\n",
       "      <td>3.925327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walmart</td>\n",
       "      <td>clothing</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.952076</td>\n",
       "      <td>0.976629</td>\n",
       "      <td>7.555166e-07</td>\n",
       "      <td>3.739739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart</td>\n",
       "      <td>uniform</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.916240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.908441e-07</td>\n",
       "      <td>3.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walmart</td>\n",
       "      <td>public</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.983622</td>\n",
       "      <td>0.952179</td>\n",
       "      <td>5.193604e-07</td>\n",
       "      <td>3.335625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walmart</td>\n",
       "      <td>pink</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.925074</td>\n",
       "      <td>0.947710</td>\n",
       "      <td>4.597000e-07</td>\n",
       "      <td>3.232084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>exact same</td>\n",
       "      <td>piercings</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.917901</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>4.260951e-12</td>\n",
       "      <td>-1.567248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>decent amount</td>\n",
       "      <td>piercings</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.923236</td>\n",
       "      <td>0.971478</td>\n",
       "      <td>1.833157e-12</td>\n",
       "      <td>-1.929699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>exact amount</td>\n",
       "      <td>piercings</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.927078</td>\n",
       "      <td>0.970303</td>\n",
       "      <td>1.432278e-12</td>\n",
       "      <td>-2.048235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>fairly easy</td>\n",
       "      <td>piercings</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.917490</td>\n",
       "      <td>0.967498</td>\n",
       "      <td>9.032076e-13</td>\n",
       "      <td>-2.277485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>drive thru</td>\n",
       "      <td>piercings</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.929021</td>\n",
       "      <td>0.967892</td>\n",
       "      <td>8.321200e-13</td>\n",
       "      <td>-2.307995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               head       tail  base  ht_sim_score  concept_sim_score  \\\n",
       "0           walmart      shoes  HEAD      0.882266           1.000000   \n",
       "1           walmart   clothing  HEAD      0.952076           0.976629   \n",
       "2           walmart    uniform  HEAD      0.916240           1.000000   \n",
       "3           walmart     public  HEAD      0.983622           0.952179   \n",
       "4           walmart       pink  HEAD      0.925074           0.947710   \n",
       "...             ...        ...   ...           ...                ...   \n",
       "3595     exact same  piercings  TAIL      0.917901           0.971146   \n",
       "3596  decent amount  piercings  TAIL      0.923236           0.971478   \n",
       "3597   exact amount  piercings  TAIL      0.927078           0.970303   \n",
       "3598    fairly easy  piercings  TAIL      0.917490           0.967498   \n",
       "3599     drive thru  piercings  TAIL      0.929021           0.967892   \n",
       "\n",
       "          lm_score  overall_score  \n",
       "0     6.872273e-07       3.925327  \n",
       "1     7.555166e-07       3.739739  \n",
       "2     1.908441e-07       3.372303  \n",
       "3     5.193604e-07       3.335625  \n",
       "4     4.597000e-07       3.232084  \n",
       "...            ...            ...  \n",
       "3595  4.260951e-12      -1.567248  \n",
       "3596  1.833157e-12      -1.929699  \n",
       "3597  1.432278e-12      -2.048235  \n",
       "3598  9.032076e-13      -2.277485  \n",
       "3599  8.321200e-13      -2.307995  \n",
       "\n",
       "[3600 rows x 7 columns]"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_concepts.csv')\n",
    "seed_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_relations.csv')\n",
    "seed_aligned_concepts_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_concepts.csv')\n",
    "seed_aligned_relations_path = os.path.join(base_dir, f'data/indeed-benchmark/seed_aligned_relations.csv')\n",
    "# knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/knn_{cluster_size}.csv')\n",
    "concept_knn_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/concept_knn_1000.csv')\n",
    "bert_emb_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/BERTembed+seeds.txt')\n",
    "\n",
    "extraction_save_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "# extraction_save_path = None\n",
    "\n",
    "extraction_results = direct_probing_RE_v3(seed_aligned_concepts_path=seed_aligned_concepts_path,\n",
    "                                          seed_aligned_relations_path=seed_aligned_relations_path,\n",
    "                                          emb_path=bert_emb_path,\n",
    "                                          concept_knn_path=concept_knn_path,\n",
    "                                          templates=has_dress_code_templates,\n",
    "                                          lm_probe=lm_probe,\n",
    "                                          topk=300,\n",
    "                                          save_path=extraction_save_path)\n",
    "\n",
    "extraction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>base</th>\n",
       "      <th>ht_sim_score</th>\n",
       "      <th>concept_sim_score</th>\n",
       "      <th>lm_score</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walmart</td>\n",
       "      <td>shoes</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.882266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.872273e-07</td>\n",
       "      <td>3.925327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walmart</td>\n",
       "      <td>clothing</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.952076</td>\n",
       "      <td>0.976629</td>\n",
       "      <td>7.555166e-07</td>\n",
       "      <td>3.739739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart</td>\n",
       "      <td>uniform</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.916240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.908441e-07</td>\n",
       "      <td>3.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walmart</td>\n",
       "      <td>public</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.983622</td>\n",
       "      <td>0.952179</td>\n",
       "      <td>5.193604e-07</td>\n",
       "      <td>3.335625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walmart</td>\n",
       "      <td>pink</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.925074</td>\n",
       "      <td>0.947710</td>\n",
       "      <td>4.597000e-07</td>\n",
       "      <td>3.232084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>walmart</td>\n",
       "      <td>perfume</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.932586</td>\n",
       "      <td>0.955838</td>\n",
       "      <td>2.919247e-07</td>\n",
       "      <td>3.116911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>walmart</td>\n",
       "      <td>watch</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.964459</td>\n",
       "      <td>0.971305</td>\n",
       "      <td>2.017021e-07</td>\n",
       "      <td>3.114202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>walmart</td>\n",
       "      <td>products</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.970532</td>\n",
       "      <td>0.948523</td>\n",
       "      <td>2.951575e-07</td>\n",
       "      <td>3.052335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>walmart</td>\n",
       "      <td>logo</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.914076</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>3.047899e-07</td>\n",
       "      <td>3.024233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>walmart</td>\n",
       "      <td>short</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.977509</td>\n",
       "      <td>0.966827</td>\n",
       "      <td>1.743601e-07</td>\n",
       "      <td>3.007472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>walmart</td>\n",
       "      <td>gold</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.982562</td>\n",
       "      <td>0.952625</td>\n",
       "      <td>1.582483e-07</td>\n",
       "      <td>2.823849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>walmart</td>\n",
       "      <td>denim</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.902171</td>\n",
       "      <td>0.944942</td>\n",
       "      <td>1.905831e-07</td>\n",
       "      <td>2.819716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>walmart</td>\n",
       "      <td>silver</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.956538</td>\n",
       "      <td>0.959586</td>\n",
       "      <td>1.322819e-07</td>\n",
       "      <td>2.813010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>walmart</td>\n",
       "      <td>single</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.979444</td>\n",
       "      <td>0.945780</td>\n",
       "      <td>1.727841e-07</td>\n",
       "      <td>2.793243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>walmart</td>\n",
       "      <td>fashion</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.957954</td>\n",
       "      <td>1.246385e-07</td>\n",
       "      <td>2.772618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>walmart</td>\n",
       "      <td>design</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.961606</td>\n",
       "      <td>0.943660</td>\n",
       "      <td>1.677633e-07</td>\n",
       "      <td>2.757461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>walmart</td>\n",
       "      <td>cotton</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.960840</td>\n",
       "      <td>0.946649</td>\n",
       "      <td>1.501690e-07</td>\n",
       "      <td>2.739156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>walmart</td>\n",
       "      <td>high</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.979699</td>\n",
       "      <td>0.946725</td>\n",
       "      <td>1.458659e-07</td>\n",
       "      <td>2.729172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>walmart</td>\n",
       "      <td>band</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.963102</td>\n",
       "      <td>0.967848</td>\n",
       "      <td>8.696533e-08</td>\n",
       "      <td>2.714132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>walmart</td>\n",
       "      <td>degree</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.975415</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>1.295199e-07</td>\n",
       "      <td>2.645241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>walmart</td>\n",
       "      <td>music</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.966610</td>\n",
       "      <td>0.952465</td>\n",
       "      <td>1.026114e-07</td>\n",
       "      <td>2.632504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>walmart</td>\n",
       "      <td>daily</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.974759</td>\n",
       "      <td>0.944306</td>\n",
       "      <td>1.184785e-07</td>\n",
       "      <td>2.614176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>walmart</td>\n",
       "      <td>logos</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.918563</td>\n",
       "      <td>0.959955</td>\n",
       "      <td>7.975698e-08</td>\n",
       "      <td>2.593174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>walmart</td>\n",
       "      <td>variety</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>0.946189</td>\n",
       "      <td>1.060422e-07</td>\n",
       "      <td>2.584747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>walmart</td>\n",
       "      <td>safety</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.979643</td>\n",
       "      <td>0.960379</td>\n",
       "      <td>7.067097e-08</td>\n",
       "      <td>2.550995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>walmart</td>\n",
       "      <td>business</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.986019</td>\n",
       "      <td>0.956720</td>\n",
       "      <td>7.546462e-08</td>\n",
       "      <td>2.543544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>walmart</td>\n",
       "      <td>cover</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.972114</td>\n",
       "      <td>0.968168</td>\n",
       "      <td>5.699137e-08</td>\n",
       "      <td>2.534699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>walmart</td>\n",
       "      <td>basic</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.977295</td>\n",
       "      <td>0.956822</td>\n",
       "      <td>6.940284e-08</td>\n",
       "      <td>2.507323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>walmart</td>\n",
       "      <td>art</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.972079</td>\n",
       "      <td>0.953499</td>\n",
       "      <td>7.350524e-08</td>\n",
       "      <td>2.498514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>walmart</td>\n",
       "      <td>action</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.973318</td>\n",
       "      <td>0.938157</td>\n",
       "      <td>9.890259e-08</td>\n",
       "      <td>2.474105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>walmart</td>\n",
       "      <td>sports</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.964498</td>\n",
       "      <td>0.967293</td>\n",
       "      <td>4.984444e-08</td>\n",
       "      <td>2.466999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>walmart</td>\n",
       "      <td>digital</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.979489</td>\n",
       "      <td>0.943876</td>\n",
       "      <td>7.559809e-08</td>\n",
       "      <td>2.415219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>walmart</td>\n",
       "      <td>game</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.971864</td>\n",
       "      <td>0.949442</td>\n",
       "      <td>6.525852e-08</td>\n",
       "      <td>2.406239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>walmart</td>\n",
       "      <td>cosmetics</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.971461</td>\n",
       "      <td>0.949242</td>\n",
       "      <td>6.459376e-08</td>\n",
       "      <td>2.399757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>walmart</td>\n",
       "      <td>food</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.957006</td>\n",
       "      <td>5.274118e-08</td>\n",
       "      <td>2.389669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>walmart</td>\n",
       "      <td>water</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.957693</td>\n",
       "      <td>0.946419</td>\n",
       "      <td>6.239663e-08</td>\n",
       "      <td>2.355116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>walmart</td>\n",
       "      <td>free</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.968379</td>\n",
       "      <td>0.951573</td>\n",
       "      <td>5.515642e-08</td>\n",
       "      <td>2.354164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>walmart</td>\n",
       "      <td>weather</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.976333</td>\n",
       "      <td>0.953590</td>\n",
       "      <td>5.147215e-08</td>\n",
       "      <td>2.345109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>walmart</td>\n",
       "      <td>lipstick</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.954299</td>\n",
       "      <td>0.969757</td>\n",
       "      <td>3.435291e-08</td>\n",
       "      <td>2.328962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>walmart</td>\n",
       "      <td>tank</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.921086</td>\n",
       "      <td>0.949033</td>\n",
       "      <td>5.505935e-08</td>\n",
       "      <td>2.323268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>walmart</td>\n",
       "      <td>conservative</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.941356</td>\n",
       "      <td>0.960297</td>\n",
       "      <td>4.179728e-08</td>\n",
       "      <td>2.318256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>walmart</td>\n",
       "      <td>caps</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.957072</td>\n",
       "      <td>0.970609</td>\n",
       "      <td>3.221776e-08</td>\n",
       "      <td>2.309892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>walmart</td>\n",
       "      <td>production</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.980831</td>\n",
       "      <td>0.948390</td>\n",
       "      <td>5.054877e-08</td>\n",
       "      <td>2.285695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>walmart</td>\n",
       "      <td>skin</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.956635</td>\n",
       "      <td>0.972119</td>\n",
       "      <td>2.928163e-08</td>\n",
       "      <td>2.283447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>walmart</td>\n",
       "      <td>games</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.967288</td>\n",
       "      <td>0.940335</td>\n",
       "      <td>6.011161e-08</td>\n",
       "      <td>2.279034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>walmart</td>\n",
       "      <td>seasonal</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.946803</td>\n",
       "      <td>5.113676e-08</td>\n",
       "      <td>2.269287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>walmart</td>\n",
       "      <td>lace</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.947919</td>\n",
       "      <td>0.951913</td>\n",
       "      <td>4.467961e-08</td>\n",
       "      <td>2.264034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>walmart</td>\n",
       "      <td>school</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.980196</td>\n",
       "      <td>0.944958</td>\n",
       "      <td>5.196411e-08</td>\n",
       "      <td>2.263304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>walmart</td>\n",
       "      <td>private</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.986720</td>\n",
       "      <td>0.946197</td>\n",
       "      <td>4.755734e-08</td>\n",
       "      <td>2.237859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>walmart</td>\n",
       "      <td>professional</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>0.964952</td>\n",
       "      <td>0.970711</td>\n",
       "      <td>2.715714e-08</td>\n",
       "      <td>2.237486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       head          tail  base  ht_sim_score  concept_sim_score  \\\n",
       "0   walmart         shoes  HEAD      0.882266           1.000000   \n",
       "1   walmart      clothing  HEAD      0.952076           0.976629   \n",
       "2   walmart       uniform  HEAD      0.916240           1.000000   \n",
       "3   walmart        public  HEAD      0.983622           0.952179   \n",
       "4   walmart          pink  HEAD      0.925074           0.947710   \n",
       "5   walmart       perfume  HEAD      0.932586           0.955838   \n",
       "6   walmart         watch  HEAD      0.964459           0.971305   \n",
       "7   walmart      products  HEAD      0.970532           0.948523   \n",
       "8   walmart          logo  HEAD      0.914076           0.944882   \n",
       "9   walmart         short  HEAD      0.977509           0.966827   \n",
       "10  walmart          gold  HEAD      0.982562           0.952625   \n",
       "11  walmart         denim  HEAD      0.902171           0.944942   \n",
       "12  walmart        silver  HEAD      0.956538           0.959586   \n",
       "13  walmart        single  HEAD      0.979444           0.945780   \n",
       "14  walmart       fashion  HEAD      0.974304           0.957954   \n",
       "15  walmart        design  HEAD      0.961606           0.943660   \n",
       "16  walmart        cotton  HEAD      0.960840           0.946649   \n",
       "17  walmart          high  HEAD      0.979699           0.946725   \n",
       "18  walmart          band  HEAD      0.963102           0.967848   \n",
       "19  walmart        degree  HEAD      0.975415           0.943536   \n",
       "20  walmart         music  HEAD      0.966610           0.952465   \n",
       "21  walmart         daily  HEAD      0.974759           0.944306   \n",
       "22  walmart         logos  HEAD      0.918563           0.959955   \n",
       "23  walmart       variety  HEAD      0.973825           0.946189   \n",
       "24  walmart        safety  HEAD      0.979643           0.960379   \n",
       "25  walmart      business  HEAD      0.986019           0.956720   \n",
       "26  walmart         cover  HEAD      0.972114           0.968168   \n",
       "27  walmart         basic  HEAD      0.977295           0.956822   \n",
       "28  walmart           art  HEAD      0.972079           0.953499   \n",
       "29  walmart        action  HEAD      0.973318           0.938157   \n",
       "30  walmart        sports  HEAD      0.964498           0.967293   \n",
       "31  walmart       digital  HEAD      0.979489           0.943876   \n",
       "32  walmart          game  HEAD      0.971864           0.949442   \n",
       "33  walmart     cosmetics  HEAD      0.971461           0.949242   \n",
       "34  walmart          food  HEAD      0.974549           0.957006   \n",
       "35  walmart         water  HEAD      0.957693           0.946419   \n",
       "36  walmart          free  HEAD      0.968379           0.951573   \n",
       "37  walmart       weather  HEAD      0.976333           0.953590   \n",
       "38  walmart      lipstick  HEAD      0.954299           0.969757   \n",
       "39  walmart          tank  HEAD      0.921086           0.949033   \n",
       "40  walmart  conservative  HEAD      0.941356           0.960297   \n",
       "41  walmart          caps  HEAD      0.957072           0.970609   \n",
       "42  walmart    production  HEAD      0.980831           0.948390   \n",
       "43  walmart          skin  HEAD      0.956635           0.972119   \n",
       "44  walmart         games  HEAD      0.967288           0.940335   \n",
       "45  walmart      seasonal  HEAD      0.925200           0.946803   \n",
       "46  walmart          lace  HEAD      0.947919           0.951913   \n",
       "47  walmart        school  HEAD      0.980196           0.944958   \n",
       "48  walmart       private  HEAD      0.986720           0.946197   \n",
       "49  walmart  professional  HEAD      0.964952           0.970711   \n",
       "\n",
       "        lm_score  overall_score  \n",
       "0   6.872273e-07       3.925327  \n",
       "1   7.555166e-07       3.739739  \n",
       "2   1.908441e-07       3.372303  \n",
       "3   5.193604e-07       3.335625  \n",
       "4   4.597000e-07       3.232084  \n",
       "5   2.919247e-07       3.116911  \n",
       "6   2.017021e-07       3.114202  \n",
       "7   2.951575e-07       3.052335  \n",
       "8   3.047899e-07       3.024233  \n",
       "9   1.743601e-07       3.007472  \n",
       "10  1.582483e-07       2.823849  \n",
       "11  1.905831e-07       2.819716  \n",
       "12  1.322819e-07       2.813010  \n",
       "13  1.727841e-07       2.793243  \n",
       "14  1.246385e-07       2.772618  \n",
       "15  1.677633e-07       2.757461  \n",
       "16  1.501690e-07       2.739156  \n",
       "17  1.458659e-07       2.729172  \n",
       "18  8.696533e-08       2.714132  \n",
       "19  1.295199e-07       2.645241  \n",
       "20  1.026114e-07       2.632504  \n",
       "21  1.184785e-07       2.614176  \n",
       "22  7.975698e-08       2.593174  \n",
       "23  1.060422e-07       2.584747  \n",
       "24  7.067097e-08       2.550995  \n",
       "25  7.546462e-08       2.543544  \n",
       "26  5.699137e-08       2.534699  \n",
       "27  6.940284e-08       2.507323  \n",
       "28  7.350524e-08       2.498514  \n",
       "29  9.890259e-08       2.474105  \n",
       "30  4.984444e-08       2.466999  \n",
       "31  7.559809e-08       2.415219  \n",
       "32  6.525852e-08       2.406239  \n",
       "33  6.459376e-08       2.399757  \n",
       "34  5.274118e-08       2.389669  \n",
       "35  6.239663e-08       2.355116  \n",
       "36  5.515642e-08       2.354164  \n",
       "37  5.147215e-08       2.345109  \n",
       "38  3.435291e-08       2.328962  \n",
       "39  5.505935e-08       2.323268  \n",
       "40  4.179728e-08       2.318256  \n",
       "41  3.221776e-08       2.309892  \n",
       "42  5.054877e-08       2.285695  \n",
       "43  2.928163e-08       2.283447  \n",
       "44  6.011161e-08       2.279034  \n",
       "45  5.113676e-08       2.269287  \n",
       "46  4.467961e-08       2.264034  \n",
       "47  5.196411e-08       2.263304  \n",
       "48  4.755734e-08       2.237859  \n",
       "49  2.715714e-08       2.237486  "
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extraction_results.copy()\n",
    "df[df['head'] == 'walmart'].head(50)\n",
    "# df['overall_score'] = df['ht_sim_score'] * 0.1 + df['concept_sim_score'] * 10 + np.log10(df['lm_score'])\n",
    "# df.sort_values(by='overall_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>base</th>\n",
       "      <th>ht_sim_score</th>\n",
       "      <th>concept_sim_score</th>\n",
       "      <th>lm_score</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>banks</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.907055</td>\n",
       "      <td>0.978189</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>6.780825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>customs</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.901078</td>\n",
       "      <td>0.968866</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>6.565287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>company</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.921313</td>\n",
       "      <td>0.991990</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>6.078816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>doctors</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.906826</td>\n",
       "      <td>0.968265</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>5.978069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>airports</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.909719</td>\n",
       "      <td>0.974302</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>5.835198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>california</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.884908</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>5.693969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>factory</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.920048</td>\n",
       "      <td>0.989166</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>5.683523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>chicago</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.884863</td>\n",
       "      <td>0.970459</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>5.654504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>ceo</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.908835</td>\n",
       "      <td>0.981629</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5.629846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>contract</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.918390</td>\n",
       "      <td>0.985901</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>5.612891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>cosmetics</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.911079</td>\n",
       "      <td>0.974106</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>5.516807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>city</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.901991</td>\n",
       "      <td>0.983007</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>5.463239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>cards</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.918634</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>5.457802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>employment</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.917788</td>\n",
       "      <td>0.981286</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>5.445527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>ethics</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.919463</td>\n",
       "      <td>0.970663</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>5.392425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>federal</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.909711</td>\n",
       "      <td>0.978361</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>5.385759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>backs</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.910887</td>\n",
       "      <td>0.970075</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>5.332508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>china</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.889609</td>\n",
       "      <td>0.967763</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>5.328929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>construction</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.907274</td>\n",
       "      <td>0.974990</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>5.321739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>friends</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.915214</td>\n",
       "      <td>0.976900</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>5.291193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>dogs</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.922341</td>\n",
       "      <td>0.975788</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>5.268156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>canada</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.892971</td>\n",
       "      <td>0.976935</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5.243275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>advertising</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.912310</td>\n",
       "      <td>0.976360</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5.226710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>cfa</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.911332</td>\n",
       "      <td>0.988112</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>5.216415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>carriers</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.908441</td>\n",
       "      <td>0.976425</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>5.209523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>chinese</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.919584</td>\n",
       "      <td>0.974681</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>5.207626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>computers</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.919998</td>\n",
       "      <td>0.979739</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>5.203145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>australia</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.900233</td>\n",
       "      <td>0.971743</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>5.199394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>department</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.921385</td>\n",
       "      <td>0.984316</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>5.174144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>christian</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.911160</td>\n",
       "      <td>0.976120</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>5.162794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>act</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.912998</td>\n",
       "      <td>0.977226</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>5.158212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>business</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.925640</td>\n",
       "      <td>0.987414</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>5.140643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>e</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.901592</td>\n",
       "      <td>0.983032</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>5.115417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>charter</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.906085</td>\n",
       "      <td>0.986353</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>5.002677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>america</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.901403</td>\n",
       "      <td>0.983181</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4.996491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>education</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.926668</td>\n",
       "      <td>0.978452</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.929875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>english</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.916798</td>\n",
       "      <td>0.972757</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>4.922591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>angel</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.905769</td>\n",
       "      <td>0.971076</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>4.922200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>fashion</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.923988</td>\n",
       "      <td>0.977258</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.911211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>corporation</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.909333</td>\n",
       "      <td>0.985716</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.907988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>burlington</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.908923</td>\n",
       "      <td>0.994165</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>4.892792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>electronics</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.918755</td>\n",
       "      <td>0.982135</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.877856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>firm</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.914005</td>\n",
       "      <td>0.981420</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.867594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>brand</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.921584</td>\n",
       "      <td>0.980557</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.867496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>amazon</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.913941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.852041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>arizona</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.887162</td>\n",
       "      <td>0.971748</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.844838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>cable</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.908715</td>\n",
       "      <td>0.974653</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>4.843309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>district</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.907677</td>\n",
       "      <td>0.979625</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.829037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>candy</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.910970</td>\n",
       "      <td>0.967715</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.815509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>art</td>\n",
       "      <td>hair color</td>\n",
       "      <td>TAIL</td>\n",
       "      <td>0.918635</td>\n",
       "      <td>0.974969</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.814831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              head        tail  base  ht_sim_score  concept_sim_score  \\\n",
       "2100         banks  hair color  TAIL      0.907055           0.978189   \n",
       "2101       customs  hair color  TAIL      0.901078           0.968866   \n",
       "2102       company  hair color  TAIL      0.921313           0.991990   \n",
       "2103       doctors  hair color  TAIL      0.906826           0.968265   \n",
       "2104      airports  hair color  TAIL      0.909719           0.974302   \n",
       "2105    california  hair color  TAIL      0.884908           0.967407   \n",
       "2106       factory  hair color  TAIL      0.920048           0.989166   \n",
       "2107       chicago  hair color  TAIL      0.884863           0.970459   \n",
       "2108           ceo  hair color  TAIL      0.908835           0.981629   \n",
       "2109      contract  hair color  TAIL      0.918390           0.985901   \n",
       "2110     cosmetics  hair color  TAIL      0.911079           0.974106   \n",
       "2111          city  hair color  TAIL      0.901991           0.983007   \n",
       "2112         cards  hair color  TAIL      0.918634           0.979428   \n",
       "2113    employment  hair color  TAIL      0.917788           0.981286   \n",
       "2114        ethics  hair color  TAIL      0.919463           0.970663   \n",
       "2115       federal  hair color  TAIL      0.909711           0.978361   \n",
       "2116         backs  hair color  TAIL      0.910887           0.970075   \n",
       "2117         china  hair color  TAIL      0.889609           0.967763   \n",
       "2118  construction  hair color  TAIL      0.907274           0.974990   \n",
       "2119       friends  hair color  TAIL      0.915214           0.976900   \n",
       "2120          dogs  hair color  TAIL      0.922341           0.975788   \n",
       "2121        canada  hair color  TAIL      0.892971           0.976935   \n",
       "2122   advertising  hair color  TAIL      0.912310           0.976360   \n",
       "2123           cfa  hair color  TAIL      0.911332           0.988112   \n",
       "2124      carriers  hair color  TAIL      0.908441           0.976425   \n",
       "2125       chinese  hair color  TAIL      0.919584           0.974681   \n",
       "2126     computers  hair color  TAIL      0.919998           0.979739   \n",
       "2127     australia  hair color  TAIL      0.900233           0.971743   \n",
       "2128    department  hair color  TAIL      0.921385           0.984316   \n",
       "2129     christian  hair color  TAIL      0.911160           0.976120   \n",
       "2130           act  hair color  TAIL      0.912998           0.977226   \n",
       "2131      business  hair color  TAIL      0.925640           0.987414   \n",
       "2132             e  hair color  TAIL      0.901592           0.983032   \n",
       "2133       charter  hair color  TAIL      0.906085           0.986353   \n",
       "2134       america  hair color  TAIL      0.901403           0.983181   \n",
       "2135     education  hair color  TAIL      0.926668           0.978452   \n",
       "2136       english  hair color  TAIL      0.916798           0.972757   \n",
       "2137         angel  hair color  TAIL      0.905769           0.971076   \n",
       "2138       fashion  hair color  TAIL      0.923988           0.977258   \n",
       "2139   corporation  hair color  TAIL      0.909333           0.985716   \n",
       "2140    burlington  hair color  TAIL      0.908923           0.994165   \n",
       "2141   electronics  hair color  TAIL      0.918755           0.982135   \n",
       "2142          firm  hair color  TAIL      0.914005           0.981420   \n",
       "2143         brand  hair color  TAIL      0.921584           0.980557   \n",
       "2144        amazon  hair color  TAIL      0.913941           1.000000   \n",
       "2145       arizona  hair color  TAIL      0.887162           0.971748   \n",
       "2146         cable  hair color  TAIL      0.908715           0.974653   \n",
       "2147      district  hair color  TAIL      0.907677           0.979625   \n",
       "2148         candy  hair color  TAIL      0.910970           0.967715   \n",
       "2149           art  hair color  TAIL      0.918635           0.974969   \n",
       "\n",
       "      lm_score  overall_score  \n",
       "2100  0.000810       6.780825  \n",
       "2101  0.000612       6.565287  \n",
       "2102  0.000117       6.078816  \n",
       "2103  0.000160       5.978069  \n",
       "2104  0.000100       5.835198  \n",
       "2105  0.000085       5.693969  \n",
       "2106  0.000050       5.683523  \n",
       "2107  0.000073       5.654504  \n",
       "2108  0.000053       5.629846  \n",
       "2109  0.000046       5.612891  \n",
       "2110  0.000048       5.516807  \n",
       "2111  0.000035       5.463239  \n",
       "2112  0.000037       5.457802  \n",
       "2113  0.000035       5.445527  \n",
       "2114  0.000039       5.392425  \n",
       "2115  0.000032       5.385759  \n",
       "2116  0.000035       5.332508  \n",
       "2117  0.000037       5.328929  \n",
       "2118  0.000030       5.321739  \n",
       "2119  0.000027       5.291193  \n",
       "2120  0.000026       5.268156  \n",
       "2121  0.000024       5.243275  \n",
       "2122  0.000024       5.226710  \n",
       "2123  0.000018       5.216415  \n",
       "2124  0.000023       5.209523  \n",
       "2125  0.000023       5.207626  \n",
       "2126  0.000021       5.203145  \n",
       "2127  0.000025       5.199394  \n",
       "2128  0.000017       5.174144  \n",
       "2129  0.000020       5.162794  \n",
       "2130  0.000020       5.158212  \n",
       "2131  0.000015       5.140643  \n",
       "2132  0.000016       5.115417  \n",
       "2133  0.000011       5.002677  \n",
       "2134  0.000012       4.996491  \n",
       "2135  0.000011       4.929875  \n",
       "2136  0.000013       4.922591  \n",
       "2137  0.000013       4.922200  \n",
       "2138  0.000011       4.911211  \n",
       "2139  0.000009       4.907988  \n",
       "2140  0.000007       4.892792  \n",
       "2141  0.000009       4.877856  \n",
       "2142  0.000009       4.867594  \n",
       "2143  0.000009       4.867496  \n",
       "2144  0.000006       4.852041  \n",
       "2145  0.000011       4.844838  \n",
       "2146  0.000010       4.843309  \n",
       "2147  0.000009       4.829037  \n",
       "2148  0.000011       4.815509  \n",
       "2149  0.000009       4.814831  "
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extraction_results.copy()\n",
    "df[df['tail'] == 'hair color'].head(50)\n",
    "# df['overall_score'] = df['ht_sim_score'] * 0.1 + df['concept_sim_score'] * 10 + np.log10(df['lm_score'])\n",
    "# df.sort_values(by='overall_score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation \n",
    "\n",
    "def load_benchmark_relations(benchmark_path):\n",
    "    '''Currently only has_dress_code.'''\n",
    "    benchmark = pd.read_csv(benchmark_path)\n",
    "    \n",
    "    # List[Dict[head, tail]]\n",
    "    rel_pairs = []\n",
    "    \n",
    "    for i, row in benchmark.iterrows():\n",
    "        if row['relation_name'] != 'has_dress_code':\n",
    "            continue\n",
    "        if row['n_head_category'] != 'company' or row['n_tail_category'] != 'dress_code':\n",
    "            continue\n",
    "        if row['type'] != 'fact':\n",
    "            continue\n",
    "        \n",
    "        if row['n_head'] != 'company':\n",
    "            # already instance \n",
    "            rel_pairs.append({'head': str(row['n_head']).lower(),\n",
    "                              'tail': str(row['n_tail']).lower()})\n",
    "            continue\n",
    "        \n",
    "        evidence_sents = eval(str(row['sentences']))\n",
    "        head_instances = eval(str(row['Evidence']))\n",
    "        assert len(evidence_sents) == len(head_instances), f'Line {i} length mismatch'\n",
    "        \n",
    "        for inst in head_instances:\n",
    "            if len(inst) > 0:\n",
    "                rel_pairs.append({'head': inst.lower(),\n",
    "                                  'tail': str(row['n_tail']).lower()})\n",
    "        \n",
    "    return rel_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark_path = os.path.join(base_dir, f'data/indeed-benchmark/benchmark_evidence.csv')\n",
    "\n",
    "benchmark_relations_list = load_benchmark_relations(benchmark_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_extraction_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "\n",
    "rel_extraction = pd.read_csv(rel_extraction_path)\n",
    "rel_extraction_list = rel_extraction[['head', 'tail']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 3597, 3)"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_relations_set = set([tuple(d.values()) for d in benchmark_relations_list])\n",
    "rel_extraction_set = set([tuple(d.values()) for d in rel_extraction_list])\n",
    "\n",
    "intersection = benchmark_relations_set & rel_extraction_set\n",
    "\n",
    "len(benchmark_relations_set), len(rel_extraction_set), len(intersection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Verification baseline\n",
    "(finding co-occurrences of head / tail from corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307122, 10), 100)"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_extraction_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/rel_extraction.csv')\n",
    "# corpus_path = os.path.join(base_dir, f'data/{data_ac}/intermediate/sentences.json')\n",
    "\n",
    "indeed_dataset_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/question_answers.csv'\n",
    "company_path = '/home/ubuntu/users/nikita/data/indeed/indeedQA/fccid-companyName.csv'\n",
    "\n",
    "# with open(corpus_path, 'r') as f:\n",
    "#     sent_dicts = [json.loads(l) for l in tqdm(f.readlines())]\n",
    "\n",
    "indeed_dataset = pd.read_csv(indeed_dataset_path)\n",
    "indeed_dataset = indeed_dataset[indeed_dataset['answerContent'].notna()]\n",
    "company_df = pd.read_csv(company_path)\n",
    "company_dict = dict(zip(company_df[\"fccompanyId\"].to_list(), company_df[\"companyName\"].to_list()))\n",
    "\n",
    "indeed_dataset.shape, len(company_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent_dict in sent_dicts:\n",
    "#     _s = ' ' + ' '.join(sent_dict['tokens']).lower() + ' '\n",
    "#     if (' walmart ' in _s) and (' red ' in _s):\n",
    "#         print(_s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487a3214b2114c8aad58be538458b929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-826-0b96dcb7874c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindeed_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0m_company_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fccompanyId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0m_company\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompany_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_company_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answerContent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminiters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminiters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Try to detect if there was an error or KeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# in manual mode: if n < total, things probably got wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;31m# decrement instance pos and remove from internal set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decr_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;31m# GUI mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m_decr_instances\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instances\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m                     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/site-packages/tqdm/_monitor.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_killed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meg_nikita/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, row in tqdm(indeed_dataset.iterrows()):\n",
    "    _company_id = row['fccompanyId']\n",
    "    _company = company_dict[_company_id]\n",
    "    \n",
    "    _answer = row['answerContent']\n",
    "    _tokens = [str(t) for t in spacy_tokenizer(_answer)]\n",
    "    _s = f\" {_company} : {' '.join(_tokens).lower()} \"\n",
    "    if (' walmart ' in _s) and (' perfume ' in _s):\n",
    "        print(_s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore various techniques\n",
    "# Get prompts \"between\" entities\n",
    "# Get prompts by syntactic parsing\n",
    "# Get prompts by paraphrasing\n",
    "# Get prompts uisng AutoPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit here: /meg-kb/src/analysis/pattern_mining.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Prompt Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit here: /meg-kb/src/analysis/lm_probing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggest Quality Prompts"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_meg_nikita)",
   "language": "python",
   "name": "conda_meg_nikita"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
