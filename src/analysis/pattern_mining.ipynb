{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe of Entity Pairs and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(sentences_json_filename, sampled_dest=None):\n",
    "    df = pd.read_json(sentences_json_filename, lines=True)\n",
    "    mapping = {}\n",
    "    for _, row in df.iterrows():\n",
    "        mentions = [x['text'] for x in row['entityMentions']]\n",
    "        mentions = list(set(mentions))  # there can be multiple mentions of the same entity in the sentence\n",
    "        if len(mentions) < 2:\n",
    "            continue\n",
    "        pairs = list(itertools.combinations(mentions, 2))\n",
    "        sent = ' '.join(row['tokens'])\n",
    "        for pair in pairs:\n",
    "            pair_key = str(set(pair))\n",
    "            sents = mapping.get(pair_key, [])\n",
    "            sents.append(sent)\n",
    "            mapping[pair_key] = sents\n",
    "\n",
    "    print(len(mapping))\n",
    "    lst = []\n",
    "    for key, val in mapping.items():\n",
    "        lst.append({\"pair\": key, \"mentions\": val, \"mention_ct\": len(val)})\n",
    "    df = pd.DataFrame(lst)\n",
    "    df = df.sort_values(by='mention_ct', ascending=False)\n",
    "    if sampled_dest is not None:\n",
    "        df.head(200).to_csv(sampled_dest, index=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe of sentences only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(sentences_filename):\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['ner'])\n",
    "    with open(sentences_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "        lines = [l for l in lines if len(l) > 0]\n",
    "        print(len(lines))\n",
    "        sents = []\n",
    "        for line in tqdm(lines):\n",
    "            doc = nlp(line)\n",
    "            sentences = [sent.text for sent in doc.sents]\n",
    "            sents += sentences\n",
    "    df = pd.DataFrame(sents, columns=['mentions'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn patterns from matched rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_patterns(all_mentions, src, tgt, nlp=None):\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    src_matcher = Matcher(nlp.vocab)\n",
    "    src_pattern = [{\"LOWER\": t} for t in src.split(' ')]\n",
    "    src_matcher.add(\"src\", [src_pattern])\n",
    "\n",
    "    tgt_matcher = Matcher(nlp.vocab)\n",
    "    tgt_pattern = [{\"LOWER\": t} for t in tgt.split(' ')]\n",
    "    tgt_matcher.add(\"tgt\", [tgt_pattern])\n",
    "    \n",
    "    patterns = {}\n",
    "    for mention in all_mentions:\n",
    "        doc = nlp(mention)\n",
    "        src_matches = src_matcher(doc)\n",
    "        if len(src_matches) == 0:\n",
    "            # print('src not matched')\n",
    "            continue\n",
    "        tgt_matches = tgt_matcher(doc)\n",
    "        if len(tgt_matches) == 0:\n",
    "            # print('tgt not matched')\n",
    "            continue\n",
    "        src_match = src_matches[0]\n",
    "        tgt_match = tgt_matches[0]\n",
    "        src_span = doc[src_match[1]: src_match[2]]\n",
    "        tgt_span = doc[tgt_match[1]: tgt_match[2]]\n",
    "        \n",
    "        if len(spacy.util.filter_spans([src_span, tgt_span])) != 2: # distinct_spans\n",
    "            print('overlapping spans')\n",
    "            continue\n",
    "        \n",
    "        src_root = src_span.root\n",
    "        tgt_root = tgt_span.root\n",
    "        \n",
    "        #  print(mention)\n",
    "        edges = []\n",
    "        for token in doc:\n",
    "            for child in token.children:\n",
    "                edges.append(('{}-{}'.format(token.lower_,token.i), '{}-{}'.format(child.lower_,child.i))) \n",
    "        \n",
    "        graph = nx.Graph(edges) \n",
    "        path = None\n",
    "        source = '{}-{}'.format(src_root.lower_, src_root.i)\n",
    "        target = '{}-{}'.format(tgt_root.lower_, tgt_root.i)\n",
    "        if nx.has_path(graph, source=source, target=target):\n",
    "            path = nx.shortest_path(graph, source=source, target=target)\n",
    "        #  print(path)\n",
    "        if path is not None:\n",
    "            for t in src_span:\n",
    "                n = '{}-{}'.format(t.lower_, t.i)  \n",
    "                if n not in path:\n",
    "                    path.append(n)\n",
    "            for t in tgt_span:\n",
    "                n = '{}-{}'.format(t.lower_, t.i)\n",
    "                if n not in path:\n",
    "                    path.append(n)\n",
    "            path_nodes = {}\n",
    "            for p in path:\n",
    "                t, i = p.rsplit('-', 1)\n",
    "                i = int(i)\n",
    "                if i in range(src_match[1], src_match[2]):\n",
    "                    t = '<src>'\n",
    "                elif i in range(tgt_match[1], tgt_match[2]):\n",
    "                    t = '<tgt>'\n",
    "                path_nodes[i] = t\n",
    "            path_nodes = sorted(path_nodes.items(), key=lambda x: x[0])\n",
    "            pattern = ' '.join([p[1] for p in path_nodes])\n",
    "            patterns[pattern] = patterns.get(pattern, 0) + 1\n",
    "    patterns = {k:v for k,v in patterns.items() if v > 1}\n",
    "    patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find patterns from Entity Pair Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns_extracted_mentions(df, src, tgt, nlp=None):\n",
    "    relevant_rows = df[df.apply(lambda x: src in eval(x['pair']) and tgt in eval(x['pair']), axis=1)]\n",
    "    if len(relevant_rows) == 0:\n",
    "        print('No mentions found')\n",
    "        return []\n",
    "    \n",
    "    mentions = relevant_rows['mentions'].tolist()\n",
    "    all_mentions = list(itertools.chain(*mentions))\n",
    "    all_mentions = list(set(all_mentions))\n",
    "    print('mentions found: {}'.format(len(all_mentions)))\n",
    "    return learn_patterns(all_mentions, src, tgt, nlp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find patterns from raw sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns_raw(df, src, tgt, nlp=None):\n",
    "    relevant_rows = df[df.apply(lambda x: src in x['mentions'] and tgt in x['mentions'], axis=1)]\n",
    "    if len(relevant_rows) == 0:\n",
    "        print('No mentions found')\n",
    "        return []\n",
    "        \n",
    "    all_mentions = relevant_rows['mentions'].tolist()\n",
    "    all_mentions = list(set(all_mentions))\n",
    "    print('mentions found: {}'.format(len(all_mentions)))\n",
    "    return learn_patterns(all_mentions, src, tgt, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Ans dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48908\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/ubuntu/users/nikita/src/HiExpan/data/indeeda/intermediate/sentences.json'\n",
    "ans_df = analyze(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/318786 [00:00<44:54, 118.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318786/318786 [31:17<00:00, 169.78it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = '/home/ubuntu/users/nikita/src/HiExpan/data/indeeda/source/corpus.txt'\n",
    "ans_sent_df = load(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypernym-Hyponym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 111\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'part-time', 'full-time', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'part-time', 'full-time', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 61\n",
      "[('<tgt> <src>', 10), ('<tgt> <src> test', 9), ('<src> <tgt>', 8), ('<src> test <tgt>', 2), ('<src> test <tgt> blood test', 2), ('<src> <tgt> drug test', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'urine', 'swab', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'urine', 'swab', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "mentions found: 13\n",
      "[('<src> <src> <tgt> <tgt>', 7)]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 16\n",
      "[('<src> <src> <tgt> <tgt>', 5)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'morning shift', 'night shift', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'morning shift', 'night shift', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 19\n",
      "[('<src> <tgt> <tgt>', 4), ('<tgt> <tgt> <src>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'dental', 'health insurance', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'dental', 'health insurance', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 9\n",
      "[('<src> <tgt> <tgt>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, '401k', 'paid vacation', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, '401k', 'paid vacation', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Part-of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 201\n",
      "[('<src> <tgt>', 33), ('<tgt> <src>', 3), ('after <src> do <tgt>', 2), ('had <src> <tgt>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'interview', 'orientation', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'interview', 'orientation', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 231\n",
      "[('<src> was <tgt>', 8), ('<tgt> <src>', 7), ('<src> process is <tgt>', 7), ('<tgt> take call for <src>', 3), ('<tgt> take for <src>', 3), ('<src> process was <tgt>', 3), ('<tgt> <src> process', 2), ('<tgt> take get <src>', 2), ('<src> is <tgt>', 2), ('<src> <tgt>', 2), ('<tgt> <src> hiring process', 2), ('<tgt> for <src>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'interview', 'long', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'interview', 'long', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 131\n",
      "[('<src> <tgt>', 15), ('<src> 6+hours of <tgt>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'interview', 'training', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'interview', 'training', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "mentions found: 39\n",
      "[('<tgt> <src> <src>', 23), ('<src> <src> is <tgt> test', 2), ('<src> <src> is <tgt> swab', 2)]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 41\n",
      "[('<tgt> <src> <src>', 19), ('<src> <src> is <tgt> test', 2), ('<src> <src> is <tgt> swab', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'drug test', 'saliva', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'drug test', 'saliva', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 299\n",
      "[('<src> <src> for <tgt>', 32), ('<tgt> require <src> <src>', 3), ('<tgt> <src> <src>', 3), ('<src> <src> required for <tgt>', 3), ('<src> <src> applying for <tgt>', 2), ('<tgt> have take <src> <src>', 2), ('<src> <src> needed for <tgt>', 2), ('<tgt> have <src> <src>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'drug test', 'position', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'drug test', 'position', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 135\n",
      "[('<src> <src> at <tgt>', 9), ('<tgt> <src> <src>', 5), ('<src> <src> <tgt>', 3), ('<tgt> did <src> <src>', 2), ('do <src> <src> at <tgt>', 2), ('<tgt> do <src> <src>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'drug test', 'location', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'drug test', 'location', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 52\n",
      "[('<src> <tgt>', 11), ('<tgt> <src>', 8), ('<src> of <tgt>', 3), ('<tgt> plan <src>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'benefits', '401k', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'benefits', '401k', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 58\n",
      "[('<tgt> with <src>', 4), ('<tgt> had <src>', 2), ('<tgt> with pay <src>', 2), ('<tgt> <src>', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'benefits', 'position', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'benefits', 'position', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 169\n",
      "[('<src> <tgt>', 61), ('<tgt> <src>', 4)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'pay', 'schedule', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'pay', 'schedule', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others (YS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 39\n",
      "[('<src> offer <tgt>', 8), ('<src> provide <tgt>', 4), ('<src> have <tgt>', 3), ('<src> provide <tgt> benefits', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'they', '401k', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'they', '401k', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns from extracted mentions\n",
      "No mentions found\n",
      "[]\n",
      "\n",
      "\n",
      "Patterns from raw text\n",
      "mentions found: 245\n",
      "[('<src> pay <tgt>', 66), ('<src> paid <tgt>', 41), ('<src> get <tgt>', 4), ('<src> are <tgt>', 3), ('<src> do <tgt>', 2), ('<src> paid <tgt> weekly', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Patterns from extracted mentions')\n",
    "extracted_patterns = get_patterns_extracted_mentions(ans_df, 'they', 'weekly', nlp)\n",
    "print(extracted_patterns)\n",
    "print()\n",
    "print()\n",
    "print('Patterns from raw text')\n",
    "raw_patterns = get_patterns_raw(ans_sent_df, 'they', 'weekly', nlp)\n",
    "print(raw_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
