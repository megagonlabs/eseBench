# Unsupervised entity set expansion 

Given a corpus and a seed set of entities of user-defined categories, the goal is to find more entities that have the same categories.

## Run the code

## Step 1: Corpus pre-processing

You need to first create a `data` folder in project root if it already does not exist. Then create a new folder with dataset name `$DATA` in the `data` folder of the project. Then create a `$DATA/intermediate/` and put the intermediate files generated by AutoPhrase into this folder. 

```
data/$DATA
└── intermediate
    └── AutoPhrase_single-word.txt: the sub-ranked list for single-word phrases only.
    └── AutoPhrase_multi-words.txt: the sub-ranked list for multi-word phrases only.
    └── sent_segmentation.txt: sentence-wise the highlighted phrases will be enclosed by the phrase tags (e.g., <phrase>data mining</phrase>).
    └── sentences.json: documents with entity phrases and noun-chunks
    └── BERTembed+seeds.txt: embeddings for the keyphrases
``` 

The intermediate files corresponding to the datasets in the benchmark can be found [here](https://drive.google.com/drive/folders/1qXg-UHxJffjHpbHumxo4UJtt1Rl_ecd6)

For running AutoPhrase on a new dataset, please follow instructions provided [here](https://github.com/shangjingbo1226/AutoPhrase)

## Step 2: Seed Definition

You next need to create a (comma-separated) csv file with the name `seed_aligned_concepts.csv` under the `$DATA` folder to specify the the seed entities. 

```
data/$DATA
└── seed_aligned_concepts.csv
```

Here is what it should look like:

```
=======================================================================================================================================================
| alignedCategoryName              |   unalignedCategoryName |      generalizations |   seedInstances                                                 |
|----------------------------------+-------------------------+---------------------+----------------------------------------------------------------- |
| technology                       |   technology            |                      |"['python', 'sql', 'java', 'html', 'perl', 'javascript', 'php']" |
| programming_language             |   programming language  |                      |"['distributed systems', 'load balancing', 'network monitoring']"|
=======================================================================================================================================================
```

`alignedCategoryName` is a canonical name of the category. `unalignedCategoryName` is the common name of the category used by the pre-trained language model. `generalizations` if any for the category. this field is optional. `seedInstances` is a comman-separated list of seed entities. typically 5-10 entities are sufficient per category. 

## Step 3: Run the tool

First create a conda environment from the provided `conda_entity_expan.yml` file. The run the following to create keywords as entity candidates from the corpus, learn their embeddings and create a ranked list of entities for each category.
```
cd src
source activate conda_entity_expan
./expand_taxonomy.sh $DATASET_NAME
```

This creates the following output files at: 

```
data/$DATA
└── intermediate
    └── ee_mrr_combine_bert_k=200.csv: top-200 predictions based on MRR over corpus embeddings and PLM rankings
    └── ee_concept_knn_k=None.csv: ranked list of entities based on corpus embeddings
    └── ee_LM_bert_k=None.csv: ranked list of entities based on PLM
```

# Benchmark Details

We have made four benchmark datasets public: apr, tripadvisor, wiki, and yelp, which can be accessed [here](https://drive.google.com/drive/folders/1qXg-UHxJffjHpbHumxo4UJtt1Rl_ecd6). The directory structure of each dataset is as follows:
```
Release-dataset/$DATA
└── corpus_docs.txt.zip: the input corpus which is essentially a collection of documents
└── entity_candidates.txt: frequency distribution of entity candidates
└── final_benchmark.csv: the ground truth labels of entity candidates
└── entity_properties.csv: the characteristics of is positive entity candidates (multifcated=`y/n`, vague=`y/n`,non-named=`y/n`)
└── intermediate: the intermediate files generated by applying AutoPhrase on corpus_docs.txt
```

# Technical Details
For more details on the benchmark and experiments read our technical paper at [NAACL 2022](https://openreview.net/forum?id=r3GMppibH-c). Cite our work as follows: 
```
@article{rahman2021noah,
  title={Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text},
  author={Shao, Yutong and Bhutani, Nikita and Rahman, Sajjadur and Hruschka, Estevam},
  journal={Findings of NAACL 2022},
  year={2022}
}
```